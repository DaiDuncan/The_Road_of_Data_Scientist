优化方法：梯度下降

两个或两个以上自变量 => 多元线性回归

非线性关系 => 多项式回归



# #技巧

正则化：模型参数

- 训练误差小
- 测试误差小(泛化好)





# #多种方式实现线性回归

使用 python 做线性回归分析有好几种方式，常要的是 scipy 包，statsmodels 包，以及 sklearn 包。但是， **这些包目前都不能处理共线性**，即自动剔除部分共线性的变量，需要自己去编函数，这一点不如 spss 或 r 语言。



数据源：

| 不良贷款 | 各项贷款余额 | 本年累计应收贷款 | 贷款项目个数 | 本年固定资产投资额 |
| -------- | ------------ | ---------------- | ------------ | ------------------ |
| 0.9      | 67.3         | 6.8              | 5            | 51.9               |
| 1.1      | 111.3        | 19.8             | 16           | 90.9               |
| 4.8      | 173.0        | 7.7              | 17           | 73.7               |
| 3.2      | 80.8         | 7.2              | 10           | 14.5               |
| 7.8      | 199.7        | 16.5             | 19           | 63.2               |
| 2.7      | 16.2         | 2.2              | 1            | 2.2                |
| 1.6      | 107.4        | 10.7             | 17           | 20.2               |
| 12.5     | 185.4        | 27.1             | 18           | 43.8               |
| 1.0      | 96.1         | 1.7              | 10           | 55.9               |
| 2.6      | 72.8         | 9.1              | 14           | 64.3               |
| 0.3      | 64.2         | 2.1              | 11           | 42.7               |
| 4.0      | 132.2        | 11.2             | 23           | 76.7               |
| 0.8      | 58.6         | 6.0              | 14           | 22.8               |
| 3.5      | 174.6        | 12.7             | 26           | 117.1              |
| 10.2     | 263.5        | 15.6             | 34           | 146.7              |
| 3.0      | 79.3         | 8.9              | 15           | 29.9               |
| 0.2      | 14.8         | 0.6              | 2            | 42.1               |
| 0.4      | 73.5         | 5.9              | 11           | 25.3               |
| 1.0      | 24.7         | 5.0              | 4            | 13.4               |
| 6.8      | 139.4        | 7.2              | 28           | 64.3               |
| 11.6     | 368.2        | 16.8             | 32           | 163.9              |
| 1.6      | 95.7         | 3.8              | 10           | 44.5               |
| 1.2      | 109.6        | 10.3             | 14           | 67.9               |
| 7.2      | 196.2        | 15.8             | 16           | 39.7               |
| 3.2      | 102.2        | 12.0             | 10           | 97.1               |



## scipy

```python
'''
scipy.stats 中的 linregress 函数可以做一元线性回归
目前只能做一元线性回归，也不能用来做预测
'''
import scipy.stats as st
import pandas as pd

datas = pd.read_excel(r'D:\Users\chen_\git\Statistics-book\datas\linear_regression.xlsx') # 读取 excel 数据，引号里面是 excel 文件的位置
y = datas.iloc[:, 1] # 因变量为第 2 列数据
x = datas.iloc[:, 2] # 自变量为第 3 列数据

# 线性拟合，可以返回斜率，截距，r 值，p 值，标准误差
slope, intercept, r_value, p_value, std_err = st.linregress(x, y)

print(slope)# 输出斜率
print(intercept) # 输出截距
print(r_value**2) # 输出 r^2
```





## statsmodels

```python
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt

datas = pd.read_excel(r'D:\Users\chen_\git\Statistics-book\datas\linear_regression.xlsx') # 读取 excel 数据，引号里面是 excel 文件的位置
y = datas.iloc[:, 1] # 因变量为第 2 列数据
x = datas.iloc[:, 2] # 自变量为第 3 列数据 => 多元回归：x = datas.iloc[:, 2:6],其余不变

x = sm.add_constant(x) # 若模型中有截距，必须有这一步

model = sm.OLS(y, x).fit() # 构建最小二乘模型并拟合
print(model.summary()) # 输出回归结果

# 画图
# 这两行代码在画图时添加中文必须用
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

predicts = model.predict() # 模型的预测值
x = datas.iloc[:, 2] # 自变量为第 3 列数据
plt.scatter(x, y, label='实际值') # 散点图
plt.plot(x, predicts, color = 'red', label='预测值')
plt.legend() # 显示图例，即每条线对应 label 中的内容
plt.show() # 显示图形
```

注意：若导入包时使用命令 import statsmodels.formula.api as sm, 则在回归分析时不用添加截距 add_constant，但是必须使用统计语言给出模型信息，代码如下：

```python
import pandas as pd
import statsmodels.formula.api as sm
import matplotlib.pyplot as plt

datas = pd.read_excel(r'D:\Users\chen_\git\Statistics-book\datas\linear_regression.xlsx') # 读取 excel 数据，引号里面是 excel 文件的位置
model = sm.ols('不良贷款~各项贷款余额', datas).fit() # 构建最小二乘模型并拟合，
                               #此时不用单独输入 x，y了，而是将自变量与因变量用统计语言公式表示，将全部数据导入
print(model.summary()) # 输出回归结果


# 画图
# 这两行代码在画图时添加中文必须用
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

predicts = model.predict() # 模型的预测值
y = datas.iloc[:, 1] # 因变量为第 2 列数据
x = datas.iloc[:, 2] # 自变量为第 3 列数据
plt.scatter(x, y, label='实际值')
plt.plot(x, predicts, color = 'red', label='预测值')
plt.legend() # 显示图例，即每条线对应 label 中的内容
plt.show() # 显示图形
```



## sklearn⭐

sklearn 包机器学习中常见的 python 包，做统计分析时，它并不能像 statsmodels 那样生成非常详细的统计分析结果。一元回归时，自变量与因变量都需要处理下，对于上面同样的例子：

```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression

datas = pd.read_excel(r'D:\Users\chen_\git\Statistics-book\datas\linear_regression.xlsx') # 读取 excel 数据，引号里面是 excel 文件的位置
y = datas.iloc[:, 1] # 因变量为第 2 列数据
x = datas.iloc[:, 2] # 自变量为第 3 列数据 => 多元线性回归：x = datas.iloc[:, 2:6]

# 将 x，y 分别增加一个轴，以满足 sklearn 中回归模型认可的数据
x = x[:, np.newaxis]
y = y[:, np.newaxis]

model = LinearRegression() # 构建线性模型
model.fit(x, y) # 自变量在前，因变量在后
predicts = model.predict(x) # 预测值
R2 = model.score(x, y) # 拟合程度 R2
print('R2 = %.2f' % R2) # 输出 R2
coef = model.coef_ # 斜率

intercept = model.intercept_ # 截距
print(model.coef_, model.intercept_) # 输出斜率和截距

# 画图
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

y = datas.iloc[:, 1] # 因变量为第 2 列数据
x = datas.iloc[:, 2] # 自变量为第 3 列数据
plt.scatter(x, y, label='实际值') # 散点图
plt.plot(x, predicts, color = 'red', label='预测值')
plt.legend() # 显示图例，即每条线对应 label 中的内容
plt.show() # 显示图形
```



# 实例|多元线性回归

## 1 sklearn

自动添加 截距





## 2 特征工程？

在数据集split==拆分后==进行

- 如果之前进行：只作用于测试集(训练集没有效果)





## 3 模型|线性回归

```python
sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)
'''
- fit_intercept：是否对训练数据中心化
- normalize：是否对数据进行标准化处理
- copy_X：如果是false，那么直接对原数据进行覆盖(即中心化，标准化之后，直接覆盖原数据)
- n_jobs：设置任务个数，1代表使用所有的CPU => 对于目标个数>1(n_target>1)且足够大规模的问题有加速作用
'''
```



模型方法：

![image-20210222091035049](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222091035.png)





## 4 模型拟合

1）实例化 sm.OLS()

2）拟合 .fit()

3）预测：基于拟合得到的模型

4）评分-模型评价

```python
X = df[['area', 'bedrooms', 'bathrooms']]
y = df['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

lm = LinearRegression()
lm.fit(X_train, y_train)
lm.score(X_test, y_test)	//结果是0.6759
```



注意：`.split()`只是**简单划分**了 训练集和测试集







# 案例|多元线性回归(统计学内容)

## 库&数据

[Link: 数据集说明](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)

![image-20210222091912493](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222091912.png)



## 1 探索变量

有必要去观察可视化界面、**变量间的关系**以及各变量的汇总统计

```python
df.head()
df.describe()
df.info()	#根据统计观察是否有NaN

sb.heatmap(df.corr())	#注意：负相关性的存在
```

![MedianHomePrice -  -09  06  INDUS  CHAS  NOX  PTRATIO  LSAT ](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222092020.png)

| 描述                                          | 对象  |
| --------------------------------------------- | ----- |
| 与房价中位数(MedianHomePrice)的线性相关性最强 | LSTAT |
| 与TAX的相关性最强                             | RAD   |
| 多重共线性问题？                              | 是的  |
| 与NOX的相关性最强                             | DIS   |







## 2 变量处理

1）分割数据集

```python
X_data = df.drop('MedianHomePrice', axis=1, inplace=False)
X_train, X_test, y_train, y_test = train_test_split(X_data, df['MedianHomePrice'], test_size=0.2, random_state=0)
```

2）特征工程：数据转化

注意：转换后的索引问题——直接合并表格：出现NaN值

![image-20210222092628241](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222092628.png)



## 3 构建模型(线性模型)

```python
training_data['intercept'] = 1
mod_full = sm.OLS(training_data['y_train'], training_data.drop('y_train', axis=1, inplace=False))

res = mod.full.fit()
print(res.summary())
```

![image-20210222092948538](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222092948.png)



结果解释：

- 每增加一个单位的CHAS，预测的房价就会增加0.5949
- 77.3%(右上角$r^2$)的价格浮动可以用这些因素来解释
- 数据集里一共有404条记录





### 重点：计算VIFs

剔除相关性比较强的数据@多元线性回归

```python
def vif_calc(df, response):
    '''
    INPUT:
    	df - a dataframe holding the x and y
    	response - the column name of the response as a string

	OUTPUT:
		vif - a dataframe of the vifs
    '''
	df2 = df.drop(response, axis=1, inplace=False)
    features = "+".join(df2.columns)
    
    y, X = dmatrices(response + '~' + features, df, return_type='dataframe')
    
    vif = pd.DataFrame()
    vif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape)]
    vif['features'] = X.columns
    vif = vif.round()
    
    return vif
```

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222095634.png" alt="10  11  12  13  VIE Factor  616.4  1.8  2.3  4.0  1.1  4.7  2.0  3.3  7.4  8.9  3.1  features  Intercept  CRIM  INDUS  CHAS  NOX  AGE  DIS  TAX  PTRATIO  LSTAT " style="zoom:80%;" />

结合之前的.corr()结论(线性相关强度)

- indus 4.0

- nox 4.7 & dis 4.0

- rad 7.4 & tax 8.9

=> 需要删除TAX和NOX，删掉后另外两个VIF会变小





### 剔除后

迭代建模

![image-20210222095827660](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222095827.png)

删掉RAD后，模型效果反而差一些



## 4 评价(模型迭代后)

在这个问题里，决定系数高的模型==可能只是因为变量较多==，而不意味着这个模型真的能在新 （测试）数据集上有更好的效果。



**决定系数高**意味着模型拟合得更好

==在**测试集中**，拥有所有变量的模型表现最佳==。

不过，我们可以用交叉验证 （即在多个训练和测试集里重复这一操作）来确定模型效果是否有**稳定性**







# 补充|注意事项

## [Link: 截距](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model)

- 几乎所有回归都需要截距。只有极少数情况不需要在线性模型里使用截距intercept



## 对系数进行假设检验

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222170640.png" alt="area is statistically significant for predicting price  342.403  348.4664  112.662  9587 .8878  coef  area  interce t  std  err  3.093  7637.479  1.255  P style="zoom:80%;" >ltl  0.000  0.209  (0.025  -5384.303  0.9751  354.530  2.46e+04 " />

可以用 Python (或其它软件) 来对线性模型的系数进行假设检验。

- 这些测试能帮我们判断**某个变量**与其因变量是否具有具**统计显著性的线性关系**
- 不过对截距进行假设检验通常没什么作用。





## 决定系数$r^2$

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222170804.png" alt="R-Squared  1  Between 1 and 0  Closer to 1 is a better fit  The square of the  correlation coefficient " style="zoom:50%;" />

即相关系数的平方

- 表示模型拟合后：模型中能以 x 变量解释的因变量的变化范围。 

- 上述$r^2$=0.678，表示还有0.322的部分可能取决于房屋面积的其他特征



决定系数不能体现正、负相关性

很多人觉得决定系数不是个很好的衡量标准 (他们可能是对的)，不过我想说，我们所使用的==任何衡量模型数据拟合度的方法，都可借助交叉验证来判断其准确性==

[Link：文章：用**交叉验证**探讨了觉得决定系数不好的原因](http://data.library.virginia.edu/is-r-squared-useless/
)