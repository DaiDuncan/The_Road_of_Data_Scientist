[博客|ML评价指标汇总](https://zhaokv.com/machine_learning/2016/03/ml-metric.html)

[博客|ML评价指标](http://charleshm.github.io/2016/03/Model-Performance/)

![image-20210316100651854](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210316100652.png)



## 背景|分类问题

分类结果1）类输出

SVM和KNN等算法创建类输出

例如，在二进制分类问题中，输出值将为0或1



分类结果2）概率输出

逻辑回归( Logistic Regression )，随机森林( Random Forest )，梯度递增( Gradient Boosting )，Adaboost等算法会产生概率输出

将概率输出转换为类输出只是创建一个阈值概率的问题



备注：回归问题中，输出时不会出现这种不一致性。输出本来就是连续的，不需要进一步处理。

# 一、二分类

## 0 准确度

### 思考|准确度有什么缺陷？

虽然准确率可以判断总的正确率，但是在样本不平衡的情况下，并不能作为很好的指标来衡量结果。

举个简单的例子，比如在一个总样本中，正样本占 90%，负样本占 10%，样本是严重不平衡的。

对于这种情况，我们只需要将全部样本预测为正样本即可得到 90% 的高准确率，但实际上我们并没有很用心的分类，只是随便无脑一分而已。

这就说明了：由于样本不平衡的问题，导致了得到的高准确率结果含有很大的水分。

即==如果样本不平衡，准确率就会失效==。





## 1.1 精确率(查准率)

- 宁放过一万，不错拿一个

- 分类阈值较高 "疑罪从无"

查准率precision：$\frac{TP}{TP+FP}$



## 1.2 召回率(查全率)

- 宁错拿一万，不放过一个

- 分类阈值较低

查全率recall(召回率): $\frac{TP}{TP+FN}$

| 真实情况↓模型预测→ | False  |  True  |
| :----------------: | :----: | :----: |
|       False        | ==TN== |   FP   |
|        True        |   FN   | ==TP== |





<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210316101036.png" alt="image-20210316093137814" style="zoom:80%;" />





精确率与召回率多用于二分类问题

- 查准率precision|**宁愿漏掉，不可错杀**：图中圆形右半部分越小越好
  - 预测股票：预测升的那些股票里，真的升了有多少
- 查全率recall(召回率)|**宁愿错杀，不可漏掉/放过一个**：图中圆形左半部分越大越好
  - 网贷违约率：相对好用户，我们更关心坏用户，不能错放过任何一个坏用户
  - 预测病患：真的患病的那些人里，我们预测错了情况应该越少越好
  - 场景：识别垃圾邮件

在推荐系统中，我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容(**减少图中圆形右半部分**)，这样就漏掉了一些用户感兴趣的内容，召回率就低了；

如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上(增加图中圆形左半部分)，宁可错杀一千，不可放过一个，这样准确率就很低了。



### 思考|案例：

某一家互联网金融公司风控部门的主要工作是利用机器模型抓取坏客户。互联网金融公司要扩大业务量，尽量多的吸引好客户，此时风控部门该怎样调整Recall和Precision？如果公司坏账扩大，公司缩紧业务，尽可能抓住更多的坏客户，此时风控部门该怎样调整Recall和Precision？



如果互联网公司要扩大业务量，为了减少好客户的误抓率，保证吸引更多的好客户，风控部门就会==提高阈值==，从而提高模型的查准率Precision，同时，导致查全率Recall下降。

如果公司要缩紧业务，尽可能抓住更多的坏客户，风控部门就会==降低阈值==，从而提高模型的查全率Recall，但是这样会导致一部分好客户误抓，从而降低模型的查准率 Precision。



### 实例|理解

主成分分析PCA——>基于特征对应的姓名(面部图)，识别人脸

主成分分析PCA：提取面部图特征，再输入到SVM，结果如下：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317143634.png" alt="image-20210222104105928" style="zoom:80%;" />

1 有多少Chavez的图片(数据集中)？——True行

2 Chavez预测的准确性？除了对角线，都被分类错误 ——关注行：基于产品质量合格的前提

3 根据预测结果，确实是Chavez的概率？——关注列：针对预测的结果

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222104143.png" alt="img" style="zoom: 67%;" />



### 思考|如何权衡Recall和Precision 的矛盾？

可以用一个指标来统一Recall和Precision的矛盾，即利用Recall和Precision的加权调和平均值作为衡量标准



即Fβ分数：==加权==调和平均，是F1度量的一般形式

- 当$\beta=1$时，称为$F_1$分数(调和平均harmonic mean)

![image-20210316095005617](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210316095005.png)

- 若认为 Precision 更重要，则减小 β，若认为 Recall 更重要，则增大 β



有时候我们需要在精确率与召回率间进行权衡，一种选择是画出精确率-召回率PR曲线（Precision-Recall Curve），曲线下的面积被称为AP分数（Average precision score）；

评估方法：

- 若一个学习器B的 P-R 曲线被另一个学习器A的 P-R 曲线完全包住，则称：A的性能优于 B

  若 A 和 B 的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的 => 比较平衡点

-  “平衡点”（Break-Event Point，简称 BEP）

![image-20210316101909955](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210316101910.png)



备注：由于Fβ Score 无法直观反映数据的情况，同时==业务含义相对较弱，实际工作用到的不多。==



## 2 混淆矩阵Confusion Matrix

混淆矩阵通常仅用于类输出模型



[link: wiki](https://en.wikipedia.org/wiki/Confusion_matrix)，通过它可以直观地观察到算法的效果

混淆矩阵是一个N×N矩阵，N是预测的类的数量

- 它的每一列是样本的预测分类
- 每一行是样本的真实分类（反过来也可以）



顾名思义，它反映了分类结果的混淆程度。

混淆矩阵 i 行 j 列的原始是原本是类别 i 却被分为类别 j 的样本个数，计算完之后还可以对之进行可视化：

![image-20210316161932231](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316161932.png)

@实例及其分析

![image-20210316162022106](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316162022.png)

目前案例的准确率达到88％。

从以上两个表中可以看出，阳性预测值很高，但阴性预测值很低，而敏感度和特异度一样。这主要由选择的阈值所造成，如果降低阈值，两对截然不同的数字将更接近。

通常，大家关注上面定义的指标中的一项

- 例如，一家制药公司，更关心的是最小错误阳性诊断 => 会更关注高特异度
- 另一方面，消耗模型会更注重敏感度



下图中的AUC针对Science以及其他 => 转化为二分类的背景

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316160158.png" alt="image-20210316160158572" style="zoom:80%;" />











## 3 ROC 和 AUC⭐

⭐理解的关键突破|注意理解下图的样本分布：(之后的gif动图均基于此图 来理解)

- 红色是正类样本
- 蓝色是负类样本

实线表示阈值

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316154308.png" alt="image-20210316154308381" style="zoom:80%;" />

ROC 曲线即 Receiver Operating Characteristic，叫做受试者工作特性曲线，ROC 不依赖模型的阈值，可以综合评价模型的泛化能力。

特性1）当测试集中的正负样本的分布变化的时候，ROC 曲线能够保持不变

在实际的数据集中经常会出现类别不平衡（Class Imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化，ROC 以及 AUC 可以很好的消除样本类别不平衡对指标结果产生的影响。



特性2）ROC 和上面的 P-R 曲线一样，是一种不依赖于阈值（Threshold）的评价指标

在输出为概率分布的分类模型中，如果仅使用准确率、精确率、召回率作为评价指标进行模型对比时，都必须时基于某一个给定阈值的，对于不同的阈值，各模型的 Metrics 结果也会有所不同，这样就很难得出一个很置信的结果。



### 指标

以下**两个指标的选择使得 ROC 可以无视样本的不平衡**

#### 指标1：灵敏度sensitivity/TPR(正类中的左半圆-准确预测)

1 真正率 (True Positive Rate , TPR)，又称灵敏度：$TPR=\frac{TP}{TP+FN}$

可以发现灵敏度和召回率是一模一样的，只是名字换了而已



2 假负率 (False Negative Rate , FNR) ：$FNR=\frac{FN}{TP+FN}$

#### 指标2：特异度specificity/TNR(负类中的右半圆外-准确预测)

3 假正率 (False Positive Rate , FPR) ：$FPR=\frac{FP}{TN+FP}$

4 真负率 (True Negative Rate , TNR)，又称特异度：$TNR=\frac{TN}{TN+FP}$

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316155732.png" alt="image-20210316155732485" style="zoom:80%;" />

#### 指标内容小结

- 灵敏度（真正率）TPR 是正样本的召回率
- 特异度（真负率）TNR 是负样本的召回率
- 而假负率 FNR=1-TPR、假正率 FPR=1-TNR

上述四个量都是==针对单一类别==的预测结果而言的，所以对整体样本是否均衡并不敏感



举个例子：假设总样本中，90% 是正样本，10% 是负样本。

在这种情况下我们如果使用准确率进行评价是不科学的，但是用 TPR 和 TNR 却是可以的，因为 TPR 只关注 90% 正样本中有多少是被预测正确的，而与那 10% 负样本毫无关系，同理，FPR 只关注 10% 负样本中有多少是被预测错误的，也与那 90% 正样本毫无关系。这样就避免了样本不平衡的问题。





### ROC（Receiver Operating Characteristic）曲线

注意：对于某个二分类分类器来说，输出结果标签（0还是1）往往取决于输出的概率以及预定的概率阈值，比如常见的阈值就是0.5，大于0.5的认为是正样本，小于0.5的认为是负样本

- 如果增大这个阈值，预测错误的概率就会降低但是随之而来的就是预测正确的概率也降低
- 如果减小这个阈值，那么预测正确的概率会升高但是同时预测错误的概率也会升高

=> 这种阈值的选取也一定程度上反映了分类器的分类能力。我们当然希望无论选取多大的阈值，分类都能尽可能地正确，也就是希望该分类器的分类能力越强越好，一定程度上可以理解成一种鲁棒能力吧。



[link: 博客|参考动图解释](https://www.guoyaohua.com/classification-metrics.html#%E5%88%A4%E6%96%AD%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD)

该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。

后来人们将其用于评价模型的预测能力。

ROC 曲线中的主要两个指标就是真正率 TPR 和 假正率 FPR：

- 其中横坐标为假正率（FPR），纵坐标为真正率（TPR）
  - $FPR=\frac{FP}{TN+FP}$
  - $TPR=\frac{TP}{TP+FN}$

<img src="C:%5CUsers%5CStein_2%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210316110919896.png" alt="image-20210316110919896" style="zoom:67%;" />

#### 判断模型性能

如何判断一个模型的 ROC 曲线是好的呢？

这个还是要回到我们的目的：

- FPR 表示模型对于负样本误判的程度
- 而 TPR 表示模型对正样本召回的程度。

我们所希望的当然是：负样本误判的越少越好，正样本召回的越多越好。

所以总结一下就是 TPR 越高，同时 FPR 越低（即 ROC 曲线越陡），那么模型的性能就越好。

​	

图解：controls表示负类样本 => FPR保持不变

- 随着正类样本分布往右(大于阈值)：TPR持续增大

![ROC Curve Change](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210316112053.gif)

即：进行模型的性能比较时，与 PR 曲线类似，若一个模型 A 的 ROC 曲线被另一个模型 B 的 ROC 曲线完全包住，则称 B 的性能优于 A。若 A 和 B 的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。





#### 特性|阈值问题

与前面的 P-R 曲线类似，ROC 曲线也是通过遍历所有阈值来绘制整条曲线的。

如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在 ROC 曲线图中也会沿着曲线滑动。

![ROC-Threshold](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210316111125.gif)

我们看到改变阈值只是不断地改变预测的正负样本数，即 TPR 和 FPR，但是曲线本身并没有改变。这是有道理的，阈值并不会改变模型的性能。



me|直观理解：所谓阈值对应上图中预测为正类的圆圈的半径大小

- 阈值小：圆圈半径大，FPR(右半圆), TPR(左半圆)都是1
- 阈值大：FPR、TPR都是0





#### 特性|无视样本不平衡

前面已经对 ROC 曲线为什么可以无视样本不平衡做了解释，下面我们用动态图的形式再次展示一下它是如何工作的。我们发现：无论红蓝色样本比例如何改变，ROC 曲线都没有影响。

![img](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210316112252.gif)

me|理解：样本不平衡 => 相当于中间分界线往左、右移动

- FRP和TPR不受影响



### AUC

AUC (Area Under Curve) 又称为曲线下面积，是处于 ROC Curve 下方的那部分面积的大小。

上文已经提到，对于 ROC 曲线下方面积越大表明模型性能越好，于是 AUC 就是由此产生的评价指标。

通常，AUC 的值介于 0.5 到 1.0 之间，较大的 AUC 代表了较好的 Performance。

如果模型是完美的，那么它的 AUC = 1，证明所有正例排在了负例的前面(圆圈左半部分就是左边的正类)

如果模型是个简单的二类随机猜测模型，那么它的 AUC = 0.5

如果一个模型好于另一个，则它的曲线下方面积相对较大，对应的 AUC 值也会较大。

- AUC = 1，是完美分类器。
- 0.5 < AUC < 1，优于随机猜测。有预测价值。
- AUC = 0.5，跟随机猜测一样（例：丢铜板），没有预测价值。
- AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。



注：对于AUC小于 0.5 的模型，我们可以考虑取反（模型预测为positive，那我们就取negtive），这样就可以保证模型的性能不可能比随机猜测差。

![image-20210316145204641](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img220210316145204.png)



#### 物理意义：⭐

- ROC某个点表示圆圈左半部分比例与右半部分比例的关系
- AUC综合了ROC所有点的信息

AUC曲线下面积对所有可能的分类阈值的效果进行综合衡量。

首先 AUC 值是一个概率值，可以理解为随机挑选一个正样本（P）和负样本（N），模型对这两个样本进行预测得到每个样本属于正类的概率值(负样本属于正类的概率低)，根据概率值对样本进行排序后，正样本排在负样本前面的概率就是AUC值@下图：正样本(成年人)>负样本的概率是100%

- 所以AUC反应的是分类器对样本的排序能力





#### 案例

小明一家四口，小明5岁，姐姐10岁，爸爸35岁，妈妈33岁，建立一个逻辑回归分类器，来预测小明家人为成年人概率。



以下为三种模型的输出结果，求三种模型的 AUC ：

![image-20210316215409053](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316215409.png)

1 AUC更多的是关注对计算概率的排序，关注的是概率值的相对大小，与阈值和概率值的绝对大小没有关系 

例子中并不关注小明是不是成人，而关注的是，预测为成人的概率的排序。



2 AUC只关注正负样本之间的排序，并不关心正样本内部，或者负样本内部的排序。

这也体现了AUC的本质： 任意个正样本的概率都大于负样本的概率的能力



例子中AUC只需要保证（小明和姐姐）（爸爸和妈妈），小明和姐姐在前2个排序，爸爸和妈妈在后2个排序，而不会考虑小明和姐姐谁在前，或者爸爸和妈妈谁在前 。

AUC只与概率的相对大小（概率排序）有关，和绝对大小没关系。

由于三个模型概率排序的前两位都是未成年人（小明，姐姐），后两位都是成年人（妈妈，爸爸），因此三个模型的AUC都等于1 => 完美地区分开



##### 案例进一步思考

以下已经对分类器输出概率从小到大进行了排列，哪些情况的AUC等于1， 情况的AUC为0

（其中背景色表示True value，红色表示成年人，蓝色表示未成年人）

![image-20210316215838515](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316215838.png)

D 模型, E模型和F模型的AUC值为1

C 模型的AUC值为0（爸妈为成年人的概率小于小明和姐姐，显然这个模型预测反了）





### 实践|如何求解AUC？

ROC曲线是由一系列 (FPR, TPR)点构成的，但一个特定的模型，只得到一个分类结果，即只有一组 (FPR, TPR)，对应ROC曲线上的一个点，如何得到多个呢？



方法1）计算面积

1. 得到结果数据，数据结构为：（输出概率，标签真值）
2. 将所有样本的预测值（属于正类的概率值）降序排列
3. 从大到小，把每一个输出概率作为分类阈值，==统计该分类阈值下的TPR和FPR==，依次生成多组 (FPR, TPR) 值
   - 很明显，阈值设置的次数越多，就会生成更多的 (FPR, TPR) 值，画出的ROC曲线也就越光滑
4. 微元法计算ROC曲线面积、绘制ROC曲线



方法2）基于AUC的物理意义：计算正样本预测结果大于负样本预测结果的概率

取n1* n0(n1为正样本数，n0为负样本数)个二元组，每个二元组比较正样本和负样本的预测结果，正样本预测结果高于负样本预测结果则为预测正确，预测正确的二元组占总二元组的比率就是最后得到的AUC

- 时间复杂度为O(N* M)



方法3）计算Rank

首先把所有样本==按照score排序==，依次用rank表示他们

- 如最大score的样本，rank=n (n=n0+n1，其中n0为负样本个数，n1为正样本个数)，其次为n-1。



那么对于正样本中rank最大的样本，rank_max

- 有n1-1个其他正样本比他score小
- 那么就有(rank_max-1)-(n1-1)个负样本比他score小。

其次为(rank_second-1)-(n1-2)。

最后我们得到正样本大于负样本的概率为 :

![image-20210316220255476](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316220255.png)

=> 计算复杂度为O(N+M)

注意：相等概率得分的样本，无论正负，谁在前，谁在后无所谓



下面有一个简单的例子：真实标签为 (**1**, 0, 0, 1, 0) 预测结果1（0.9, 0.3, 0.2, 0.7, 0.5） 预测结果2（0.9, 0.3, 0.2, 0.7, 0.8）)

- 分别对两个预测结果进行排序，并提取他们的序号 
  - 结果1 (**5**, 2, 1, 4, 3) 
  - 结果2 (5, 2, 1, 3, 4)
- 对正分类序号累加@注意真实标签的对应
  - 结果1: SUM正样本（rank(score))=5+4=9 
  - 结果2: SUM正样本（rank(score))=5+3=8
- 计算两个结果的AUC
  - 结果1：AUC= (9-2*3/2)/6=1 
  - 结果2：AUC= (8-2*3/2)/6=0.833





### 思考|为什么说 ROC 和AUC都能应用于非均衡的分类问题？

ROC曲线只与横坐标 (FPR) 和 纵坐标 (TPR) 有关系 。我们可以发现TPR只是正样本中预测正确的概率，而FPR只是负样本中预测错误的概率，和正负样本的比例没有关系。

因此 ROC 的值与实际的正负样本比例无关，因此既可以用于均衡问题，也可以用于非均衡问题。而 AUC 的几何意义为ROC曲线下的面积，因此也和实际的正负样本比例无关。



### ROC曲线与PR曲线

正负样本不平衡的时候，使用ROC曲线更好

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317141215.png" alt="image-20210317141215095" style="zoom:80%;" />





### 小结|ROC方法的评价

1 作为类输出的模型，将在ROC图中用单个点表示

2 这些模型无法相互比较，因为需要在==单个指标==基础上进行判断而不是多个指标

- 例如，具有参数（0.2,0.8）的模型和具有参数（0.8,0.2）的模型可以来自相同的模型，因此不应直接比较这些指标



#### ROC方法的优点

为什么要使用ROC而不是升力曲线等指标？

升力取决于人口的总响应率。因此，如果人口的响应率发生变化，同一模型将带来不同的升力图。

解决这种问题的方案可以是真正的升力图（在每个十分位数处找到升力值和完美模型升力值的比率）。但这种比例很少对企业有价值



另一方面，ROC曲线几乎与响应率无关。这是因为它有两个来自混淆矩阵柱状计算中的轴。在响应率变化的情况下，x轴和y轴的分子和分母也会有相应的改变。



### 小结|工业应用场景⭐

在机器学习的使用场景当中，我们**往往更加看重正例**。

比如广告的点击率预测、搜索排序、推荐等等这些场景下，我们更加关注用户点击行为的发生和预测准确情况，而不太关心没有点击是否预测准确。

在这些场景当中，我们衡量精确度或者是召回其实不是特别重要，尤其这种涉及排序、摆放位置调整的场景，我们**更加在意模型是否能够把高质量的内容给出一个高的预测分**，让它能够排在前面，让用户优先看到。

这个时候往往AUC更加能够说明模型的能力。



也因此，相比于精确度、准确度和召回率，在实际的工业应用场景当中，我们可能**使用AUC更多一些**。

当然这并非是说其他概念不重要，这主要还是应用场景决定的。既然应用场景决定了使用AUC的范围很广，那么当我们去应聘岗位的时候，问到AUC的可能性就很高，尤其是考察候选人基础能力的时候。

如果被问到，**光理解它是什么意思是不够的**，我们还需要掌握它的应用场景，它的前因后果，甚至能够进行发散思考一些之前没有想过的问题。





#### AUC应用|点击率预估CTR

AUC最大的应用应该就是点击率预估（CTR）的离线评估，CTR的离线评估在公司的技术流程中占有很重要的地位，一般来说，ABTest和转全观察的资源成本比较大，所以，一个合适的离线评价可以节省很多时间、人力、资源成本。

那么，为什么AUC可以用来评价CTR呢？

1 CTR是把分类器输出的概率当做是点击率的预估值，如业界常用的LR模型，利用sigmoid函数将特征输入与概率输出联系起来，这个输出的概率就是点击率的预估值。内容的召回往往是根据CTR的排序而决定的

2 AUC量化了ROC曲线表达的分类能力。这种分类能力是与概率、阈值紧密相关的，分类能力越好（AUC越大），那么输出概率越合理，排序的结果越合理



我们不仅希望分类器给出是否点击的分类信息，更需要分类器给出准确的概率值，作为排序的依据。所以，这里的AUC就直观地反映了CTR的准确性（也就是CTR的排序能力）





# 二、多分类

有时候会有多组混淆矩阵(每两两类别的组合都对应一个二元的混淆矩阵)

例如：多次训练或者在多个数据集上训练的结果，那么估算全局性能的方法有两种，分为宏平均（macro-average）和微平均（micro-average）。



## 宏平均

- 先算出每个混淆矩阵的 P 值和 R 值
- 然后取得平均 P 值 macro-P 和平均 R 值 macro-R
- 再算出$Fβ$ 或$F1$

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316160455.png" alt="image-20210316160455574" style="zoom:80%;" />![image-20210316160503685](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316160503.png)

## 微平均

- 计算出混淆矩阵的平均 TP、FP、TN、FN
- 接着进行计算 P、R
- 进而求出$Fβ$ 或$F1$

![image-20210316160503685](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316160503.png)

其它分类指标同理，均可以通过宏平均/微平均计算得出。



需要注意的是，在多分类任务场景中，如果非要用一个综合考量的 Metric 的话，宏平均会比微平均更好一些，因为宏平均受稀有类别影响更大。

宏平均平等对待==每一个类别==，所以它的值主要受到稀有类别的影响

而微平均平等考虑数据集中的==每一个样本==，所以它的值受到常见类别的影响比较大。



# 补充



## 补充|增益图和提升图

[link: 11个重要的机器学习模型评估指标](https://zhuanlan.zhihu.com/p/84665209)

增益图和提升图主要用于检查概率的顺序

1. 计算每个样本的概率。

2. ==按降序排列==这些概率。

3. 每组构建十分位数时都有近10%的样本。

4. 计算每个十分位数的响应率，分为Good( Responders )、Bad( Non-responders )和总数。



提升图或增益图表广泛应用于目标定位问题。

这告诉我们，在特定的活动中，可以锁定客户在哪个十分位数上。

此外，它会告诉你对新目标数据期望的响应量。





## 补充|K-S图

K-S或Kolmogorov-Smirnov图表衡量分类模型的性能

更准确地说，KS值是在模型中用于区分预测==正负样本分隔程度==的评价指标，一般应用于金融风控领域。

- ROC是以FPR作为横坐标，TPR作为纵坐标，通过改变不同阈值，从而得到ROC曲线
- 以阈值作为横坐标，以FPR和TPR作为纵坐标

ks曲线则为TPR-FPR，ks曲线的最大值通常为ks值



如果分数将人数划分为单独两组，其中一组含所有正例，另一组含所有负例，则K-S值为100



另一方面，如果模型不能区分正例和负例，那么就如同模型从总体中随机选择案例一样，K-S为0。

在大多数分类模型中，K-S值将从0和100之间产生，并且值越高，模型对正例和负例的区分越好。



还可以绘制 %Cumulative Good和Bad来查看最大分离。

下面是示例图：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316162545.png" alt="image-20210316162545804" style="zoom:80%;" />

### 思考|为什么这样求KS值呢？

我们知道，当阈值减小时，TPR和FPR会同时减小，当阈值增大时，TPR和FPR会同时增大@圆的大小同时增大，或减小



在实际工程中，我们希望TPR更大一些，FPR更小一些，即`TPR-FPR`越大越好，即ks值越大越好。

- 可以理解TPR是收益，FPR是代价，ks值是收益最大。





## 补充|对数损失Log loss

对预测概率的==似然估计==：$Logloss=-log \ P(Y|X)$

对数损失最小化本质是上利用样本中的已知分布，求解导致这种分布的==最佳模型参数，使这种分布出现概率最大==



确定模型性能时AUC-ROC会考虑预测概率

然而，AUC ROC存在一个问题，就是只考虑概率的顺序，因此忽略了模型对更可能是正样本预测更高概率的能力

这种情况下，可以采取对数损失，它只是每个案例修正预测概率的对数的负平均值

对数损失也被称为被称为逻辑回归损失（Logistic regression loss）或交叉熵损失（Cross-entropy loss）



### 二分类问题

设$y \in \{0, 1\}, p=Pr(y=1)$，则对每个样本的对数损失为：

![image-20210316163249076](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316163249.png)

随机计算几个值的对数损失`Logloss(y, Pr(y=1))`，得出上述数学函数的要点：

Logloss（1,0.1）= 2.303

Logloss（1,0.5）= 0.693

Logloss（1,0.9）= 0.105

如果绘制这种关系，曲线图如下：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316172921.png" alt="image-20210316172920950" style="zoom:80%;" />

从斜率向右下方逐渐平缓可以明显看出，随着预测概率Pr(y=1)的提高，对数损失值逐渐下降。

不过反方向移动时，对数损失快速增加而预测概率趋近于0



logloss衡量的是预测概率分布和真实概率分布的差异性，取值越小越好。

因此，降低对数损失，对模型更好。

但是，对于好的对数损失没有绝对的衡量标准，它取决于用例或者应用程序。



虽然AUC是根据具有不同决策阈值的二进制分类计算的，但对数损失实际上考虑了分类的“确定性”。



### 多分类问题

设Y是指示矩阵，当样本i的分类为k时$y_{i,k}=1$

设P为估计的概率矩阵，$p_{i,k}=Pr(t_{i,k}=1)$

则对每个样本的对数损失为：

![image-20210316163433839](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316163434.png)



## 补充|铰链损失Hinge loss

一般用来使“边缘最大化”（maximal margin）

### 二分类问题

假设正样本被标记为1，负样本被标记为-1，y是真实值，w是预测值，则铰链损失定义为：

![image-20210316163523220](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316163523.png)

### 多分类问题

假设$y_w$是对真实分类的预测值，$y_t$是对非真实分类预测中的最大值，则铰链损失定义为：

![image-20210316163555102](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316163555.png)

注意，二分类情况下的定义并不是多分类情况下定义的特例。



## 补充|基尼系数

有时用于分类问题。基尼系数可由AUC ROC数直接导出

基尼只是ROC曲线和diagnol线之间的面积与上述三角形的面积之比：Gini = 2*AUC – 1

基尼系数高于60％，模型就很好。





## 补充|kappa系数(基于混淆矩阵)

用于一致性检验的指标，也可以用于衡量分类的效果

- kappa系数的计算是基于混淆矩阵的，取值为-1到1之间,通常大于0



为什么要使用kappa？

分类问题中，最常见的评价指标是acc，它能够直接反映分正确的比例，同时计算非常简单。但是实际的分类问题种，各个类别的样本数量往往不太平衡

这时需要一种能够惩罚模型的“偏向性”的指标来代替acc

而根据kappa的计算公式，越不平衡的混淆矩阵，$p_e$越高，kappa值就越低，正好能够给“偏向性”强的模型打低分。



[Cohen’s kappa](https://en.wikipedia.org/wiki/Cohen's_kappa)

用来衡量两种标注结果的吻合程度，标注指的是把N个样本标注为C个互斥类别。计算公式为：

![image-20210316163655476](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316163655.png)

其中$p_o$是观察到的符合比例(也就是acc)，$p_e$是由于随机性产生的符合比例。

当两种标注结果完全相符时，$\kappa=1$，越不相符其值越小，甚至是负的。

示例：基于下述混淆矩阵

| 成绩评级 | 好   | 中   | 差   |
| -------- | ---- | ---- | ---- |
| 好       | a    | b    | c    |
| 中       | d    | e    | f    |
| 差       | g    | h    | i    |

- 预测与真实值相符共有20+15个

![image-20210316163954639](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316163954.png)



## 补充|配对Concordant – Discordant ratio？







## 补充|海明距离

Hamming Distance：用于需要对样本多个标签进行分类的场景

![image-20210316164232569](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316164232.png)

- 当预测结果与实际情况完全相符时，距离为0；
- 当预测结果与实际情况完全不符时，距离为1；
- 当预测结果是实际情况的真子集或真超集时，距离介于0到1之间





## 补充|杰卡德相似系数

[Jaccard similarity coefficients](http://en.wikipedia.org/wiki/Jaccard_index)：也是用于需要对样本多个标签进行分类的场景

![image-20210316164319948](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210316164320.png)

它与海明距离的不同之处在于分母。

- 当预测结果与实际情况完全相符时，系数为1；
- 当预测结果与实际情况完全不符时，系数为0；
- 当预测结果是实际情况的真子集或真超集时，距离介于0到1之间。





## 补充|多标签排序？

### 1 涵盖误差

Coverage error：计算的是预测结果中平均包含多少真实标签，适用于二分类问题



### 2 标签排序平均精度

Label ranking average precision简称LRAP，它比涵盖误差更精细



### 3 排序误差

Ranking loss进一步精细考虑排序情况



# #小结-思考|问题

1. 为什么说ROC曲线的光滑程度与样本数量没有绝对的关系呢？

   只取决于阈值选择的密度

2. 如果一个模型的AUC小于0.5，可能是因为什么原因造成的呢？

   正、负类选择与模型预测恰好相反

3. 在一个预测流量的场景中，尝试了多种回归模型，但是得到的 RMSE 指标都非常高，考虑下可能是因为什么原因造成的呢？

   @评价指标-回归 RMSE

   - ==用到了均值，受到异常值的影响很大==
   - 假设是否成立？即误差是无偏的并遵循正态分布

4. 在一个二分类问题中，15个样本的真实结果为[0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0]，模型的预测结果为[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1]，计算准确率、精确率、召回率以及F1值。

5. 在一个二分类问题中，7个样本[A, B, C, D, E, F, G]的真实结果为[1, 1, 0, 0, 1, 1, 0]，模型的预测概率为[0.8, 0.7, 0.5, 0.5, 0.5, 0.5, 0.3]，计算AUC值。





# #参考文献

[link: 知乎专栏|阈值划分的软分类器：LR GBDT](https://zhuanlan.zhihu.com/p/137305152)

[link: 11个重要的机器学习模型评估指标](https://zhuanlan.zhihu.com/p/84665209)