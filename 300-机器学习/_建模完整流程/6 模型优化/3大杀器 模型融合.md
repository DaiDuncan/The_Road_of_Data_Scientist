## 是什么|model ensemble

Ensemble Learning是一组individual learner的组合

- 如果individual learner（基学习器）是同质，即都是决策树或都是SVM，成为base learner
- 如果individual learner异质，称为component learner



简单来说，模型融合有以下几条原则：

1、群众的力量是伟大的，集体的智慧是惊人的

- Voting投票器
- Bagging
- 随机森林/Random forest



2、站在巨人的肩膀上，能看得更远

- stacking
- blending



3、一万小时定律：针对残差值，多联系几次

- Boosting



# 1 Voting投票器

1 模型效果不好的原因？

- 欠拟合/过拟合

> 认为模型与模型之间有差异，而单个模型很难控制过拟合。
> 所以采用“使用多种模型，结果多数表决”方法



[sklearn.ensemble.VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)

- Soft Voting/Majority Rule classifier for **unfitted** estimators.

```python
class sklearn.ensemble.VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)


>>> import numpy as np
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.naive_bayes import GaussianNB
>>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier
>>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)
>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
>>> clf3 = GaussianNB()
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> eclf1 = VotingClassifier(estimators=[
...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
>>> eclf1 = eclf1.fit(X, y)
>>> print(eclf1.predict(X))
[1 1 1 2 2 2]
>>> np.array_equal(eclf1.named_estimators_.lr.predict(X),
...                eclf1.named_estimators_['lr'].predict(X))
True
>>> eclf2 = VotingClassifier(estimators=[
...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...         voting='soft')
>>> eclf2 = eclf2.fit(X, y)
>>> print(eclf2.predict(X))
[1 1 1 2 2 2]
>>> eclf3 = VotingClassifier(estimators=[
...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...        voting='soft', weights=[2,1,1],
...        flatten_transform=True)
>>> eclf3 = eclf3.fit(X, y)
>>> print(eclf3.predict(X))
[1 1 1 2 2 2]
>>> print(eclf3.transform(X).shape)
(6, 6)
```





# 1 Bagging

1 模型效果不好的原因？

- 欠拟合/过拟合

2 如何缓解过拟合？
少给点题，别让它直接把所有题目答案背下来
多找几个同学做题，综合一下他们的答案

[sklearn.ensemble.BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html?highlight=bagging#sklearn.ensemble.BaggingClassifier)

```python
class sklearn.ensemble.BaggingClassifier(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)


>>> from sklearn.svm import SVC
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=100, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = BaggingClassifier(base_estimator=SVC(),
...                         n_estimators=10, random_state=0).fit(X, y)
>>> clf.predict([[0, 0, 0, 0]])
array([1])
```







# 1 RandomForest

随机森林是一种基于树模型的Bagging的优化版本。

对每棵树构建的时候，特征也会做采样处理



[sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforest#sklearn.ensemble.RandomForestClassifier)

```python
class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)



>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.datasets import make_classification
>>> X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
>>> clf = RandomForestClassifier(max_depth=2, random_state=0)
>>> clf.fit(X, y)
RandomForestClassifier(...)
>>> print(clf.predict([[0, 0, 0, 0]]))
[1]
```



# 2 Stacking

用上层estimator结果作为下一层的特征。

有点像是在公司里信息先传到各个部门经理那里，各个部门经理做出决策，再汇总到大boss。

大boss根据不同的部门经理决策再做出最终的判断。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319195342.jpeg" alt="img" style="zoom:80%;" />



[sklearn.ensemble.StackingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html?highlight=stacking#sklearn.ensemble.StackingClassifier)

```python
class sklearn.ensemble.StackingClassifier(estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)



>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.svm import LinearSVC
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.ensemble import StackingClassifier
>>> X, y = load_iris(return_X_y=True)
>>> estimators = [
...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
...     ('svr', make_pipeline(StandardScaler(),
...                           LinearSVC(random_state=42)))
... ]
>>> clf = StackingClassifier(
...     estimators=estimators, final_estimator=LogisticRegression()
... )
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, stratify=y, random_state=42
... )
>>> clf.fit(X_train, y_train).score(X_test, y_test)
0.9...
```





# 2 Blending

弱化版的stacking，相当于对结果做线性加权。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319195504.jpeg" alt="img" style="zoom: 67%;" />



# 3 Boosting⭐

考得不好的原因是什么？

1. 还不够努力，练习题需要多次学习：重复迭代和训练
2. 时间分配要合理，要多练习之前做错的题：每次==分配给分错的样本更高的权重==
3. 我不聪明，但是脚踏实地，用最简单的知识不断积累，成为专家：最简单的分类器叠加



Adaboost：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319195742.jpeg" alt="img" style="zoom:80%;" />



假如现在是个很简单的一个分类器，只会“一刀切”。

在第一次分类的时候（D1），检测出有三个样本分类错误，那么这三个样本在下一次会着重考虑，在第二次分类的时候会将这几个样本正确划分。

而在第二次划分的时候又有样本划分错误，那么在第三次分类的时候会着重考虑这些样本。。。。。。

最后，根据每个分类的结果，进行加权相加。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319195855.jpeg" alt="img" style="zoom:80%;" />



可以发现最后的分类边界由原先的“一刀切”变成一个更接近实际的分类边界。

在上图中两个类别里面不同块颜色有深浅，代表着不同区域的数据分到这个类别的概率不同。

[sklearn.ensemble.GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=boosting#sklearn.ensemble.GradientBoostingClassifier)

```python
class sklearn.ensemble.GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)



>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier

>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]

>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.913...
```



对于回归问题，也可以这样考虑：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319195923.jpeg" alt="img" style="zoom:67%;" />

上面左图是一个很“简单”的回归模型，右图是这个模型与实际样本之间的残差值模拟的回归线。

我们可以把这个残差加在左图训练出来的回归模型上，得到新的回归模型，再做出其残差图，然后再相加。。。。。。

这样迭代多次，最终得到的回归模型就与原数据实际分布很近似。

<img src="https://pic3.zhimg.com/80/v2-1a1a7e646ca85dfb471b106b624d3f32_720w.jpg" alt="img" style="zoom:80%;" />

