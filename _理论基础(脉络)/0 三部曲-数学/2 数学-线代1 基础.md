# 区分|线性空间：向量空间，矩阵空间

有些书上直接用向量空间指代线性空间，但我觉得应该摒弃这种用法。

这样很容易让人产生误解，应该明确的区分向量空间和线性空间

- 向量空间是狭义的，他的元素是向量。

- 线性空间是广义的，他的元素可以任何东西，可以是向量，矩阵，多项式，函数……还可以是映射，橘子香蕉苹果，桌椅板凳等.....
  - 线性空间可以是有限维的，也可以是无限维的
  - 当其为有限维时，可以用向量组组成的空间表示





# 向量

==波(声波、光波)都可以看做向量==

- 特征波作为基底
- 波由特征波叠加组成：即基底向量的线性组合
  - 幅值 <=> 数乘
  - $\vec{0}$表示直流
- 波的叠加
  - 干涉
    - 当两个音调结合在一起时，它们会发出拍子
    - 当光波在油膜中混合在一起时，它们会干涉并形成彩虹图案



实际意义：用来==高度抽象、概括==某些研究对象的本质特征，这样可以利用相关的性质和结论



## 向量空间

遇到问题：先要考虑是不是向量空间 [Link: 向量空间的基本性质⭐](https://brilliant.org/practice/waves-abstract-vectors/?p=3)

- 符号表示 |u>



<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223222920.png" alt="Rules for vectors addition and scaling  3. There is a vector O such that O +  4. For every vector there is a vector  5. a (IR)  6.1 v v  for all  — so that + ( —G) —  Axioms&quot; " style="zoom:80%;" />

1 定义：满足向量空间的几条性质

- 加法封闭：
  - 加法交换律
  - 加法结合律
  - 零向量(零元)
  - 负向量

- 数乘封闭：$c \cdot x_1$
  - 数对元素的分配律
  - 元素对数的分配律
  - 数因子结合律
  - 单位向量(单位元)



比如：Ax = 0

- 对加法封闭：$x_1+x_2$
- 对数乘封闭：$c \cdot x_1$



2 性质：

- 零元素唯一
- 任一元素的负元素唯一



3 线性空间的维数

线性空间V中**线性无关向量组**所含向量最大个数n，称为V的维数，记作 dimV=n



4 线性空间的基

n维线性空间中，任意n个线性无关的向量(不共线/共面/共体等 分别对应1维~3维空间)构成该空间的一组基

- 这n个线性无关的向量称作基向量。
- 空间中任意一个向量 x 可由这组基唯一表示



5 基变换与坐标变换

对线性空间作变换，也就是对线性空间的基做变换。（这是因为，线性空间中的任一向量都能由该空间的一组基线性表示，即一组基可决定一个空间。但是，一个空间可对应不同的多组基）

线性空间中的一个向量本身是不变的，但对基作变换后，基改变，从而基下的坐标改变，称为坐标变换，即，同一向量在不同基下的表示是不同的





### 子空间subspace

子空间V‘中所有的元素都属于空间V

比如：三维空间中的一个过原点的二维平面，或一条过原点的直线，都是该三维空间中的线性子空间。这两个子空间也满足数乘封闭和加法封闭



子空间的运算

| 符号描述 |                                    |
| -------- | ---------------------------------- |
| V ∩ W    | 交集：向量组同时属于这两个向量空间 |
| V ⊕ W    | 当v∈V and w∈W：v + w ∈ V ⊕ W       |
| V ∪ W    |                                    |

假如$V_1, V_2$都是V的子空间，那么前两种运算后的空间也都是V的子空间(并集不一定)



性质：

- 线性子空间也是线性空间。（定义中满足数乘、加法封闭，即线性子空间首先要是线性的）

- 一个线性空间的子空间，其维数小于等于线性空间的维数（显然）





### 4个向量空间

| 向量空间       |                          |
| -------------- | ------------------------ |
| column space   | 矩阵A的列生成的空间      |
| row space      | 矩阵A的行生成的空间      |
| nullspace      | $Av=0$                   |
| left nullspace | $A^Tv=0$，或者说$v^TA=0$ |



示例|矩阵A：

![image-20210213185206781](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213185206.png)

A的列向量生成的空间：

![image-20210213185221175](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213185221.png)

维度：注意后面两个列向量是前面两个列向量的线性组合(2个列向量冗余)，所以最终维度是2



基于上述关系：定义rank

1 列向量空间的维度/秩等于行向量空间的维度/秩 $rank(A) = rank(A^T)$

2 nullspace的维度

- ker(A)表示nullspace => kernel of A
- 具体计算过程：基于Av=0，代入(v1, v2, ... )，求解vi的自由度 [Link: Brilliant](https://brilliant.org/practice/four-fundamental-subspaces/?p=5)

![image-20210213201732115](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213201732.png)

![image-20210213202318650](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213202318.png)

#### rank-nullity theorem

nullspace的维度也被称为nullity：rank(A) + nullity(A) = 矩阵的==列数==

证明过程：

- 首先使用高斯消除将矩阵转换为其高斯-约旦形式(只涉及行初等变换，不改变rank) => Gauss-Jordan matrix：最简行形式





## 矩阵$A_{m \times n}$的基本向量空间

| Subspace                                                     | Subspace of    | Symbol            | Dimension                                    |
| ------------------------------------------------------------ | -------------- | ----------------- | -------------------------------------------- |
| [column space](https://brilliant.org/wiki/row-and-column-spaces/) | $\mathbb{R}^m$ | $\text{im}(A)$    | r = [rank](https://brilliant.org/wiki/rank/) |
| [nullspace (kernel)](https://brilliant.org/wiki/kernel/)     | $\mathbb{R}^n$ | $\text{ker}(A)$   | n - r                                        |
| [row space](https://brilliant.org/wiki/row-and-column-spaces/) | $\mathbb{R}^n$ | $\text{im}(A^T)$  | r                                            |
| [left nullspace (kernel)](https://brilliant.org/wiki/kernel/) | $\mathbb{R}^m$ | $\text{ker}(A^T)$ | m - r                                        |

![image-20210213203225087](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213203225.png)





针对一般矩阵(非方阵) $B∈C^{m×n}$

![image-20210223212444930](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223212445.png)



 

### 概念|向量张成的空间

span()



### 矩阵的值域空间

矩阵的值域是 矩阵中的所有**列向量**所张成的空间



若把 A看作一种线性变换，那么矩阵的值域 y=Ax，为线性空间中的原向量x经线性变换后所得到的象

![矩 阵 € Cmxn 的 “ 个 0 为 al, a2, ．  ， 则 走 阵 A 的 值 域 为 ](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223213126.png)



### 矩阵的列空间column space

![a 11  」 = d21  矩 阵 的 列 空 间 ， 具 实 就 是 矩 阵 的 列 所 组 成 的 空 间 。 比 如 我 们 考 虑 一 个 （ 3 * 2 ） 的 矩 阵  031  a 12  d22 ： @ 的  032  ， 他 的 列 空 间 就 是  曱 向 量 和 ． 42 向 量 所 能 绢 成 的 空 间 。 在 汶 里 ， 我 们 有 两 个 向 量 ， 所 以 矩 阵 的 列 秩 为 2 〔 在 两 向 量 线 注 不 想 关 的 情 况 下 ， 表 现 在 条 中 即 两 个 向  量 不 共 线 ） 。 如 果 共 线 ， 那 么 向 量 ． 40 以 写 成 ． 4 的 线 性 表 示 ， 汶 个 时 候 ， 汶 两 个 向 量 所 张 成 的 空 间 只 能 是 一 条 直 线 所 以 秩 变 成 了 1 。  一 个 矩 阵 ． 中 的 m 和 n 不 能 等 价 于 矩 阵 的 秩 。 i ： 严  矩 阵 的 秩 的 意 义 是 列 向  量 所 能 张 成 的 空 间 的 形 状 的 一 种 描 述 ， 虽 然 在 三 维 空 间 中 ， 列 向 畢 张 成 的 空 间 中 的 任 一 个 向 畢 要 用 三 维 坐 标 来 表 示 ， 但 是 并 不 意 味 看 汶 个  空 间 是 一 个 三 维 的 体 ， 而 是 一 个 面 ， 只 不 过 汶 个 是 带 有 角 的 。  从 线 性 变 换 的 角 度 理 解 的 佰 ， 具 实 就 是 从 空 间 角 度 理 解 的 矩 阵 的 列 空 间 。 ](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223213206.png)



### 矩阵的核空间$K(\cdot)$/零空间$N(\cdot)$

核空间也叫零空间，零空间的维数为零度，记作 n(A)

- 定义：使 Ax=0成立的x



若把 A看作一种线性变换， 那么**矩阵的核**是经过线性变换后变为零向量的向量（原象）——线性变换后降维压缩为零向量的特殊向量

![image-20210223213316330](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223213316.png)

![零 空 间 是 ． 4 ： 〔 所 张 成 的 空 间 。 如 果 说 除 去 ： 〔 空 间 还 存 在 ， 那 么 就 一 定 意 味 看 空 间 是 被 压 缩 了 的 ， 因 为 只 有 压 缩 之 后  才 能 把 一 条 直 线 压 缩 到 零 点 上 。  凸 外 之 意 ， 矩 阵 A 的 列 秩 不 满 ， 矩 阵 A 的 列 向 量 具 有 线 性 相 ](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223213322.png)



### 维度：核与值域空间

$rank() + dim(K()) = number \ of \ columns$



V的维度也就是V的基的数目。

这些基分为两部分，一部分在核中，一部分是值域中==非零象的原象==（肯定可以分，因为核和值域都是独立的子空间）。

如果把V中的任意向量用基的形式写出来，那么这个向量必然也是一部分在核中，另一部分在值域中非零象的原象里。

现在对这个向量作变换，核的那部分当然为零了，另一部分的维度刚好等于值域的维度。







### 矩阵秩的结论

![image-20210222180554269](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222180554.png)





# 矩阵

## 矩阵的初等变换|高斯消元法

行初等变换：每一行是一个方程



### #pivot

优先行/列，比如：

![image-20210213190243489](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213190243.png)

## 矩阵的特性

结合律

不满足交换律

矩阵转置

矩阵的迹(只适用于方阵) => @最优化的指标





## 矩阵的运算

### 基本运算及其性质

@Roam 数学-线性代数

- 数乘
- 加法
- 乘法
- $$A^T, A^{-1}, A^*$$三者之间的关系
- 有关$$A^*$$的结论
- 有关$$A^{-1}$$的结论

### 逆矩阵

几何意义@3B1B

| 情况                        |                                                              |
| --------------------------- | ------------------------------------------------------------ |
| 满秩\|存在逆矩阵            | 不压缩到更低维度的空间(3D——>二维，直线，点)                  |
|                             | 唯一对应变换后零点的就是零向量本身                           |
| 非满秩\|不存在逆矩阵(det=0) | 压缩到更低维度的空间——>没有逆变换                            |
|                             | 因为降维：一系列向量在变换后变为零向量<br />变换前<br /><img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223222231.png" alt="img" style="zoom:67%;" /> |
|                             | 变换后<br /><img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223222242.png" alt="img" style="zoom:67%;" /> |
|                             | 3D——>2D 有一条直线映射到原点<br/>3D——>1D直线 有一个平面映射到原点 |

原因：不可能将压缩后的空间 解压缩为高维空间(不能将“线”解压缩为平面——空间变不回去)——>至少一个线性变换的函数做不到

原因：逆变换等价于将一个向量点——>拓展成一条线(如此才有可能将“线”解压缩为平面)——>**不符合函数一输入对一输出**的特性



#### 零空间|映射到零点⭐

零空间Null space，核Kernel

零空间：==变换后映射到零点的向量组成的空间==(如下图直线)

![img](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223222337.png)







### 向量的运算@MSV

点积dot product：针对实数向量(不包括矩阵)

- 意义：度量向量的“接近”程度 => 刻画相似性@信号三部曲，@机器学习-LLS最小二乘回归(尽可能地接近/误差小)

范数

柯西施瓦茨不等式

![image-20210213195028966](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213195029.png)

内积inner product：点积的广义拓展 <v, w>

- 内积的运算性质@SVNT，MSV

- 拓广到非数值的向量空间 => 连续函数

  ![image-20210213195516872](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213195516.png)

- 拓广到随机变量：$<X, Y> = E[XY]$



正交向量orthogonal => 构造基底

- 对于实数向量：正交就等价于垂直

- 正交化方法：Gram-Schmidt正交化



### 汇总|矩阵的乘积

1 矩阵乘法

2 Hadamard乘积@VES-最大 加算法==邻接图

- 矩阵元素的乘积

![image-20210223165558864](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223165559.png)

3 Kronecker Product 直积 => 张量积

![image-20210223165628276](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223165628.png)





## 矩阵的高级运算⭐

@Roam 数学 线性代数-矩阵论及AI应用

求导：向量，矩阵，转置等相互关系



## 内积的应用|最小二乘法

$min \ ||Ax-b||$@上述矩阵的高级运算





# 概念|最小生成集 基底 维度⭐

| 基本概念                    |                                  |
| --------------------------- | -------------------------------- |
| vector space                |                                  |
| span                        | minimal spanning sets            |
| degrees of freedom          | 基于线性方程：讨论自由变量的个数 |
| Basis/minimal spanning sets | standard basis                   |
| dimension: size of basis    |                                  |

## span：

向量空间的==线性组合== => “张成”的空间



## minimal spanning sets

最小生成集 => 线性独立，没有冗余redundant

- 线性独立：线性方程只有在系数全为0的时候才成立





## degrees of freedom

- `x+y+z=6`的自由度为2

- 下式(A|b)秩为2，独立度为1

![image-20210213191329315](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213191329.png)

在系数矩阵中添加一个方程，那么自由度一般就会减少1

- 如果该方程是冗余的(线性相关)，那么自由度不变



## Basis

向量空间V的任一最小生成集minimal spanning set

- 正交基：向量空间一组基中的向量两两正交
- 规范正交基standard basis：正交基中每个向量都是单位向量





## dimension

意义：可以用来分析子空间、间接定义的向量空间



向量空间基底basis的维度

- 理论依据1：每一个向量空间都有基底Basis
- 理论依据2：向量空间所有有限的基底维度相同

拓展：向量空间-3*3矩阵的维度是9

![image-20210213192037804](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213192038.png)

![image-20210213192044913](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213192045.png)

- 3*3对角阵的维度是6

- 多项式$x^3+x$也是向量空间：
  - 基底是$\{x^3, x^2, x, 1\}$
  - 维度是4(基底的size是4)



### 示例|[Link: Brilliant](https://brilliant.org/practice/basis-and-dimension/?p=8)

矩阵A：

- A diagonal：$A^T=A$
- A skew symmetric：$A^T=-A$ => 对角线必定为0

V是对称阵向量空间，W是反对称阵向量空间：

- dim(V) = 6
- dim(W) = 3
- dim(V + W) =9
- dim(V ∩ W)=0



分析过程：

1 dim(V + W) =9

![image-20210213194017083](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213194017.png)

![image-20210213194023198](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213194023.png)

$(A^T)^T=((A_+ + A_-)^T)^T=...=A_+^T-A_-^T=A_+ + A_-=A$

- 说明任意任意一个普通的矩阵A都可以写成v+w的形式(v∈V and w∈W)

- 两者等价：维度相同，都是9



2 dim(V ∩ W)=0

从定义出发：

![image-20210213194634315](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213194634.png)

零向量的维度是0



注意到：

![image-20210213194659307](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213194659.png)







# 线性变换|矩阵描述

特征：线性变换可以用矩阵来描述



加法 + 数乘

下图从左到右：

- 尺度变换
- 旋转
- 水平方向的分量拉伸

![image-20210213214644112](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213214644.png)



- 微分：符合线性运算的性质



## 线性变换/矩阵的几何意义

变换=函数=映射

- 尺度变换：拉伸，压缩，翻转向量的行为，统称为scaling，而这些数值本身，称之为scalars

- 对称矩阵=>正交变换

- 旋转和伸缩=>保形映射



| 看操作对象的位置                        |                                                              |
| --------------------------------------- | ------------------------------------------------------------ |
| 操作对象在右边(列向量)  $y=M \cdot x$   | <img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223221812.png" alt="Shear  —1  Rotation  —1  Composition " style="zoom:50%;" /><br />先做的Rotation，再做的Shear，但是Rotation需要写在右边，右边的总是比左边的变换矩阵先操作 |
| 操作对象在左边(行向量)  $y=x^T \cdot M$ | 左边矩阵先操作(但此时 同一个矩阵：左边操作和右边操作的意义不同？？) |
|                                         |                                                              |
|                                         |                                                              |



$new = M \cdot old$

$$M=\begin{pmatrix}
newAxis_{1,x}  & newAxis_{2,x}  \\
newAxis_{1,y}  & newAxis_{2,y} 
\end{pmatrix}$$

- 以原先x轴单位向量$(1, 0)^T$ => 得到经过M变换后的新向量是：$(newAxis_{1,x}, newAxis_{1,y})^T$
- 同理，原先y轴的单位向量$(0, 1)^T$ => $(newAxis_{2,x}, newAxis_{2,y})^T$
- 上述两者分别是：新坐标轴的基向量

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223205521.png" alt="image-20210223205520701" style="zoom:80%;" />







## 变换的象与原象

线性变换T(V)=W|P⊂V，S⊂W：

- P是S的原象preimage
- ==线性变换/矩阵的rank就是象S的维度==：rank(T)=dim im(T)
  - 比如T(x)=2x，线性变换的秩为1
- kernel/零空间T(x)=0：求解kernel就等于求解齐次的线性方程组(可以用高斯消去法)
  - 比如T(x)=2x，线性变换的kernel为0
  - nullity(T)=dim ker(T)



示例|T(x,y)=(x+y,2(x+y))

- 象(右边的结果)可以写成：c(1, 2)，所以维度为1
  - 正式来说：(1, 2)是象的基底basis
  - 因此：线性变换的rank=1
- kernel：也就是T(x, y)=0 => x+y=0
  - 对应结果是(x, -x)，基底是(1, -1)
  - 因此：nullity =1 



理解：为什么成为kernel？

- 因为T(x)=0，代表了线性变换的本质/结构属性



### injective transformation 单射

本质：不存在多射一



也就是T(x)=b，有唯一的结果b

- 此时，对于kernel T(x)=0
  - 只有T(x)={0}，变换T(x)才是单射的
  - 反证法：如果T(x)在多种情况下都为0，那么对应的，在这些情况下，T(x)=0+b均为0，违背了单射的原则
  - [Link: 具体证明](https://brilliant.org/practice/properties-of-linear-transformations/?p=5) 在T(x)={0}的前提下进行推导



从Ax=b这个角度来看：单射说明Ax=0只有零解(也就是变换的kernel T(x)=0)



### surjectivity 满射

象包含整个空间 => 线性变换/矩阵满秩



### bijectivity 一一映射 <=> isomorphism同构

重要结论：

> If \{v\} is a ==linearly independent set== of vectors in a vector space V
>
> and T is an injective transformation单射 from V to W
>
> then T(\{v\}) = \{w\} is a ==linearly independent set== of vectors in the space W.



结论的反面：

> If \{w\} is a ==linearly independent set== of vectors in a vector space W
>
> and T is an injective transformation单射 from V to W
>
> then T(\{v\}) = \{w\} is a ==linearly independent set== of vectors in the space V.

证明思路：

![image-20210213222206909](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213222207.png)



## #小结

1 线性变换 T: V(domain) -> W(image) | T(v)=w

- T、W可以在同一个域：T(x)=2x
- 也可以不在同一个域：比如多项式 => 矩阵



2 秩与维度

- rank(M) = dim(image)
- nullity(M) = dim(kernel)



3 映射关系

- injective：不同的V转换为不同的W

- surjective满射：每一个象都有原象
- bijective：一一映射

 injective transformations保持线性独立性



0 重点：线性变换可以看做与矩阵相乘





# 特征值与矩阵对角化

特征向量：在变换过程中不改变方向的向量

- 图像压缩
- 协同预测：用户相似性
- PageRank 



验证是否为特征向量|正向验证：代入v => 找到标量$\lambda$使得向量v只发生尺度变换(下式中从(1, 1)到(3, 3))

![image-20210214084614297](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214084614.png)

- 注意：如果是复数域，$\lambda$可以是复数



结论：

> $n \times n$矩阵有n个复数的特征向量
>
> - 如果矩阵有两个特征值都是1，那么就计算为2个特征值

这与代数基本定理很类似：n阶多项式有n个复数的根





意义|对角化

- 求特征值很方便
- 求矩阵的幂更加方便
  - $D=P^{-1}AP$ => $A=PDP^{-1}$
  - ![image-20210214092312531](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214092312.png)
  - 最终结果是$A^k=PD^kP^{-1}$





性质：[Link: 多项式的根之和、积公式](https://brilliant.org/wiki/vietas-formula/#vietas-formula-higher-degrees)

![image-20210214090007870](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214090008.png)

- 特征值的和：有一项的系数是特征值的和($x^{n-1}$)@待定系数法 => $\sum r_i=-\frac{a_{n-1}}{a_n}$
- 特征值的积：令x为0 => $\Pi r_i=(-1)^n \frac{a_0}{a_n}$

应用|基于特征多项式：求特征值的和或积

![image-20210214090633979](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214090634.png)

![image-20210214090752759](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214090753.png)

注意观察：$x^{n-1}$项只能由对角线元素组成 => 而且恰巧是对角线元素的和 => 也就是迹的意义

- $tr(A)=\sum \lambda_i$



## 对角化

除非另有说明，否则本测验中的所有矩阵均为实数



对角化定义|存在P，是的$P^{-1}AP$是对角阵

性质：对角化 <=> 相似

- 相似矩阵具有相同的特征多项式
- 相似矩阵具有相同的特征值
- 相似矩阵具有相同的行列式

![image-20210214091255881](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214091256.png)

![image-20210214091310296](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214091310.png)



### 如何对角化？

思想：$D=P^{-1}AP$ => $PD=AP$

- D是对角阵 => 把P拆成列向量(c1, c2, ..., cn)来看待

- 即$(\lambda_1 c_1, \lambda_2 c_2, ..., \lambda_n c_n) = (Ac_1, Ac_2,..., Ac_n)$ 

- => ==惊奇发现==：$c_i$就是矩阵A的特征向量



由此：可对角化的条件 => A有n个线性独立的特征向量



1 特征值不同，那么一定能对角化

2 即使特征值相同，也可能能够对角化





## 应用

斐波那契数列

![image-20210214094027192](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214094027.png)

![image-20210214094038255](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214094038.png)



# 实例|PageRank

背景：搜索网页，加载数十亿的结果，并针对查询进行检查，找到相关项

- 细节：如何排名？ => 如何保证页面P的重要性？
  - 许多链接指向P：页面P可能对该主题具有权威性
  - 链接到它的站点已经很重要，比如wikipedia



建立评分过程：

初始状态都均等

![image-20210214094658359](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214094658.png)

迭代得到稳态：

- 根据链接数量，平均分配初始值的“给予”
  - 比如A给B、C每一个0.1
  - D给E是0.2
  - E给A也是0.2

- 一次更新过后A=0.3,B=0.1,C=0.2,D=0.1,E=0.3.

- 多次更新...(计算量巨大)





## 分析|状态转移矩阵

![image-20210214095558248](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214095558.png)

令M表示转移矩阵：$M_{ij}$表示从节点j到i，所以第一行是从其他节点到节点A

- $M^n=PD^{n}P^{-1}$
  - $D^n=\{\lambda_i^n\}$
- $M^nv$|v可以看做初始状态
  - ![image-20210214100916829](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214100917.png)



## 随机矩阵

定义：方阵，其元素为==非负实数==，且行和或列和为1

- 如果行和为1，则称为行随机矩阵
- 如果列和为1，则称为列随机矩阵
- 如果行和和列和都为1，则称为双随机矩阵

[谷歌矩阵](http://bitjoy.net/2016/08/04/googles-pagerank-and-beyond/)和[HMM中的转移矩阵](http://bitjoy.net/2016/08/20/introduction-to-hmm-1/)都属于随机矩阵，所以随机矩阵也称为概率矩阵、转移矩阵、或马尔可夫矩阵。



### 最大特征值为1|稳态

随机矩阵有一个性质，就是其所有特征值的绝对值==小于等于1，且其最大特征值为1==。



首先，随机矩阵A肯定有特征值1：当v是向量$(\frac{1}{n}, \frac{1}{n},\frac{1}{n}, ...)^T$时，由于$Av=1v$，所以至少有一个特征值是1@VES

其次：

- 证明方法|反证法
  - ![image-20210214143426626](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214143426.png)
- 证明方法|常规证法
  - $Ax=\lambda x$  => $x^TAx=\lambda x^Tx$
  - ![image-20210214143527823](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214143528.png)



由于最大特征值为1，而$M^nv_i=\lambda^n v_i, v=\alpha_1 v_1+ \alpha_2 v_2+...$

- 除了特征值为1的情况外，其余情况由于$\lambda<1$，n很大的时候变为0
- 所以$M^nv=M^n(\alpha_1 v_1+ \alpha_2 v_2+...) \approx  \alpha_i v_i$



最大特征值对应的特征向量==也就是稳态==

- 上述PageRank图形的稳态是(0.3077, 0.1538, 0.2308, 0.1154, 0.1923)
- 即A > C > E > B > D






​	

### 第二大特征值$\lambda(A)$

矩阵A的谱间隔（spectral gap）$1- \lambda(A)$表示：最大特征值和第二大特征值之间的差值

$\lambda(A)$在马尔可夫随机游走领域有重要作用

![image-20210214143732111](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214143732.png)









# #参考文献

[Link: 3B1B|线性代数的本质](https://www.bilibili.com/video/BV1ys411472E?p=10)

[Link: 随机矩阵及其特征值](https://bitjoy.net/2016/08/23/the-eigenvalue-of-stochastic-matrix/)

[Link: wiki|凸几何-凸组合](https://zh.wikipedia.org/wiki/%E5%87%B8%E7%BB%84%E5%90%88)

- 点的线性组合，要求==所有系数都非负且和为 1==。此处的“点”可以是仿射空间中的任何点，包括向量和标量。
- 任意两个点的凸组合都在它们之间的线段上
- 三个点以上的凸组合：在几何体内部 => 这个几何体被包含在最长边围成的球体当中
- 点集的凸包等价于该点集的所有凸组合

