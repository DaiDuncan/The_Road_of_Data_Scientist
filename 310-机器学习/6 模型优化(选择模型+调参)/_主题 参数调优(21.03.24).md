调参即超参数优化，是指从超参数空间中选择一组合适的超参数，以权衡好模型的偏差(bias)和方差(variance)，从而提高模型效果及性能。

常用的调参方法有：

- 人工手动调参
- 网格/随机搜索(Grid / Random Search)
- 贝叶斯优化(Bayesian Optimization)

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324085151.webp)



注：超参数 vs 模型参数差异 

- 超参数是控制模型**学习过程**的(如网络层数、学习率)
- 模型参数是**通过模型训练学习后得到的**（如网络最终学习到的权重值）





# 人工调参|理解参数的作用

结合数据情况及算法的理解，选择合适调参的优先顺序及参数的经验值

不同模型手动调参思路会有差异：

- 如随机森林是一种**bagging集成**的方法，参数主要有n_estimators（子树的数量）、max_depth（树的最大生长深度）、max_leaf_nodes（最大叶节点数）等。
  - 对于n_estimators：通常越大效果越好。参数越大，则参与决策的子树越多，可以消除子树间的随机误差且增加预测的准度，以此降低方差与偏差。
  - 对于max_depth或max_leaf_nodes：通常对效果是先增后减的。取值越大则子树复杂度越高，偏差越低但方差越大。

<img src="https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTv6bgRCTOgJxZ4gen6JnxOyqgJNiaqntySdxuhOFgYPuaWAMaHVmKlVQNYVj3RncVicrUktEjPPyA0Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:80%;" />



Dr.Mukesh Rao的超参数样本清单：

![img](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210318160237.jpeg)

```python
#importing required libraries
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold , cross_val_score
from sklearn.datasets import load_wine

wine = load_wine()
X = wine.data
y = wine.target

#splitting the data into train and test set
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 14)

#declaring parameters grid
k_value = list(range(2,11))
algorithm = ['auto','ball_tree','kd_tree','brute']
scores = []
best_comb = []
kfold = KFold(n_splits=5)

#hyperparameter tunning
for algo in algorithm:
  for k in k_value:
    knn = KNeighborsClassifier(n_neighbors=k,algorithm=algo)
    results = cross_val_score(knn,X_train,y_train,cv = kfold)	#k折交叉验证

    print(f'Score:{round(results.mean(),4)} with algo = {algo} , K = {k}')
    scores.append(results.mean())
    best_comb.append((k,algo))

best_param = best_comb[scores.index(max(scores))]
print(f'\nThe Best Score : {max(scores)}')
print(f"['algorithm': {best_param[1]} ,'n_neighbors': {best_param[0]}]")
```



## 缺陷

1. 不能保证得到最佳的参数组合。

2. 这是一种反复试验的方法，因此会消耗更多的时间。



# 网格搜索Grid Search

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324090140.png" alt="image-20210324090140553" style="zoom:80%;" />

类似于手动调优，是超参数优化的传统方法，为网格中指定的所有给定超参数值的每个排列建立模型，并评估和选择最佳模型

```python
from sklearn.model_selection import GridSearchCV

knn = KNeighborsClassifier()
grid_param = { 'n_neighbors' : list(range(2,11)) , 
              'algorithm' : ['auto','ball_tree','kd_tree','brute'] }

grid = GridSearchCV(knn,grid_param,cv = 5)
grid.fit(X_train,y_train)

#best parameter combination
grid.best_params_

#Score achieved with best parameter combination
grid.best_score_

#all combinations of hyperparameters
grid.cv_results_['params']

#average scores of cross-validation
grid.cv_results_['mean_test_score']
```



## 缺陷

尝试每一种超参数组合，并根据交叉验证分数选择最佳组合，这使得 GridsearchCV 极其缓慢





# 随机搜索

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324090217.png" alt="image-20210324090217722" style="zoom:80%;" />

是对超参数组合的子集简单地做固定次数的随机搜索，找到表现最佳的超参数子集。

对于规模较大的参数空间，采用随机搜索往往效率更高。



使用随机搜索代替网格搜索的动机是，在许多情况下，所有的超参数可能并非同等重要。

随机搜索从超参数空间中随机选择参数组合，参数按 n_iter 给定的迭代次数进行选择。

==随机搜索已经被实践证明比网格搜索得到的结果更好==。

```python
from sklearn.model_selection import RandomizedSearchCV

knn = KNeighborsClassifier()

grid_param = { 'n_neighbors' : list(range(2,11)) , 
              'algorithm' : ['auto','ball_tree','kd_tree','brute'] }

rand_ser = RandomizedSearchCV(knn,grid_param,n_iter=10)
rand_ser.fit(X_train,y_train)

#best parameter combination
rand_ser.best_params_

#score achieved with best parameter combination
rand_ser.best_score_

#all combinations of hyperparameters
rand_ser.cv_results_['params']

#average scores of cross-validation
rand_ser.cv_results_['mean_test_score']
```



## 缺陷

随机搜索的问题是它不能保证给出最佳的参数组合。





# 贝叶斯优化

贝叶斯优化(Bayesian Optimization) 与网格/随机搜索最大的不同，在于考虑了历史调参的信息，使得调参更有效率

- 但在高维参数空间下，贝叶斯优化复杂度较高，效果会近似随机搜索

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324090408.webp" alt="图片" style="zoom:80%;" />



## 算法简介

贝叶斯优化思想简单可归纳为两部分:

- 高斯过程(GP)：以历史的调参信息（Observation）去学习**目标函数的后验分布**（Target）的过程。
- 采集函数(AC)：由学习的目标函数进行采样评估，分为两种过程：
  - 1、开采过程：在**最可能出现全局最优解的参数**区域进行采样评估。
  - 2、勘探过程：兼顾**不确定性大的参数区域的采样评估，避免陷入局部最优**。



贝叶斯优化属于一类被称为sequential model-based optimization(SMBO)的优化算法

这些算法使用先前对损失 f 的观测，来确定下一个(最佳)点来取样 f

1. 使用先前计算过的点 X1: Xn，计算损失 f 的后验期望值。

2. 在一个新的点 Xnew取样损失 f ，它最大化了 f 的期望的某些效用函数。该函数指定 f 域的哪些区域是最适合采样的。

重复这些步骤，==直到达到某种收敛准则==。



贝叶斯优化算法通过对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。



具体来说，它学习目标函数形状的万法是：

- 首先根据先验分布，假设一个搜集函数
- 然后，每一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布；
- 最后，算法测试由后验分布给出的全局最值最可能出现的位置的点。



## 算法流程

for循环n次迭代：
    采集函数依据学习的目标函数(或初始化)给出下个开采==极值点== Xn+1;
    评估超参数Xn+1得到表现Yn+1;
    加入新的Xn+1、Yn+1数据样本，并==更新高斯过程模型==；

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324090549.webp)



```python
#pip install scikit-optimize
#另一个库：pip install bayesian-optimization
from skopt import BayesSearchCV

import warnings
warnings.filterwarnings("ignore")

# parameter ranges are specified by one of below
from skopt.space import Real, Categorical, Integer

knn = KNeighborsClassifier()
#defining hyper-parameter grid
grid_param = { 'n_neighbors' : list(range(2,11)) , 
              'algorithm' : ['auto','ball_tree','kd_tree','brute'] }

#initializing Bayesian Search
Bayes = BayesSearchCV(knn , grid_param , n_iter=30 , random_state=14)
Bayes.fit(X_train,y_train)

#best parameter combination
Bayes.best_params_

#score achieved with best parameter combination
Bayes.best_score_

#all combinations of hyperparameters
Bayes.cv_results_['params']

#average scores of cross-validation
Bayes.cv_results_['mean_test_score']
```



```python
"""
随机森林分类Iris使用贝叶斯优化调参
"""
import numpy as np
from hyperopt import hp, tpe, Trials, STATUS_OK, Trials, anneal
from functools import partial
from hyperopt.fmin import fmin
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestClassifier

def model_metrics(model, x, y):
    """ 评估指标 """
    yhat = model.predict(x)
    return  f1_score(y, yhat,average='micro')

def bayes_fmin(train_x, test_x, train_y, test_y, eval_iters=50):
    """
    bayes优化超参数
    eval_iters：迭代次数
    """
    
    def factory(params):
        """
        定义优化的目标函数
        """
        fit_params = {
            'max_depth':int(params['max_depth']),
            'n_estimators':int(params['n_estimators']),
            'max_leaf_nodes': int(params['max_leaf_nodes'])
            }
        # 选择模型
        model = RandomForestClassifier(**fit_params)
        model.fit(train_x, train_y)
        # 最小化测试集（- f1score）为目标
        train_metric = model_metrics(model, train_x, train_y)
        test_metric = model_metrics(model, test_x, test_y)
        loss = - test_metric
        return {"loss": loss, "status":STATUS_OK}

    # 参数空间
    space = {
        'max_depth': hp.quniform('max_depth', 1, 20, 1),
        'n_estimators': hp.quniform('n_estimators', 2, 50, 1), 
        'max_leaf_nodes': hp.quniform('max_leaf_nodes', 2, 100, 1)
            }
    # bayes优化搜索参数
    best_params = fmin(factory, space, algo=partial(anneal.suggest,), max_evals=eval_iters, trials=Trials(),return_argmin=True)
    # 参数转为整型
    best_params["max_depth"] = int(best_params["max_depth"])
    best_params["max_leaf_nodes"] = int(best_params["max_leaf_nodes"])
    best_params["n_estimators"] = int(best_params["n_estimators"])
    return best_params

#  搜索最优参数
best_params = bayes_fmin(train_x, test_x, train_y, test_y, 100)
print(best_params)
```



## 缺陷

对于贝叶斯优化算法，有一个需要注意的地方，一旦找到了一个局部最优值，它会在该区域不断采样，==所以很容易陷入局部最优值==。

为了弥补这个缺陷，贝叶斯优化算法会在探索和利用之间找到一个平衡点：

- 探索就是在==还未取样的区域==获取采样点
- 而利用则是根据后验分布在==最可能出现全局最值的区域==进行采样。



在2维或3维搜索空间中，需要十几个样本才能得到一个良好的替代曲面（surrogate surface）; 

增加搜索空间的维数需要更多的样本。



# 总结

在找到参数的最佳组合的保证和计算时间之间总是有一个权衡。

如果你的超参数空间(超参数个数)非常大：

1. 使用随机搜索==找到超参数的潜在组合==
2. 然后使用该局部的网格搜索(超参数的潜在组合)来选择最优特征。



[作为一个一般性的经验法则，任何时候想要优化调整超参数，优先考虑网格化寻优方法和随机寻优方法！][id2]



## 总结|针对应用⭐

超参数个数少的时候：使用贝叶斯优化

超参数个数多的时候：

- 先使用随机搜索==找到超参数的潜在组合==
- 再使用局部的网格搜索(超参数的潜在组合)来选择最优特征



# #参考文献

[link: 公众号我是算法工程师|4种主流超参数调优技术](https://my.oschina.net/u/4579551/blog/4675964)

[id2]: https://zhuanlan.zhihu.com/p/138005319 "link: deephub|机器学习模型的超参数优化"

- 超参数优化方法：上述4种调参方法
- 基于梯度的优化方法（Gradient-based Optimization）
- 进化寻优（Evolutionary Optimization）

[link: 算法进阶](https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&mid=2247494040&idx=2&sn=d49e4a061267ca814de82817c7268842&chksm=97076224a070eb327cc960deaa27ad8f01135a26753baf64bd81c7387810305d7d750dbf2257&mpshare=1&scene=24&srcid=0307RlGAtdDO12eJne1r3kz9&sharer_sharetime=1615097841375&sharer_shareid=04169dcd804a29a99e43aea044294546&key=33d2b61dc88184174744d458e3171161c1f9960635776ae4c32900ebdb6fdd1230a326ce11ca73d88c064e4019b017f59044234e6d4115841d8518390d8a5e5fc4375e0dd9a55210306b0f9e7a2cf90960f83dc015b62222ca8335e1b4489a0ca8f581327e6be7c4676bcb42fa843b39c9d69f03eb5c2997a3ae14f46dafe3d7&ascene=14&uin=MTUwNzc5NjY4MA%3D%3D&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=ARfrh3%2FkUmhbv7X4aBEGXKE%3D&pass_ticket=pMoOtH6gQFB9VKmDMKAk5gIlEvds0axeBHuSLq7o6c%2FCK0TKtCX2%2Fu4Nz845ap6F&wx_header=0)