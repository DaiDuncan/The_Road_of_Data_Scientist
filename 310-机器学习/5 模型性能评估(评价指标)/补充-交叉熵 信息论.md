交叉熵的概念起源于信息论，可以理解为==刻画的其实是两个概率分布之间的距离==

交叉熵越小，说明模型预测的概率分布和真实情况的概率分布越接近，也就是我们的预测值接近了答案。



对交叉熵做深层的理解需要了解下面几个概念：



## 信息量

信息奠基人香农（Shannon）认为“==信息是用来消除随机不确定性的东西==”，也就是说衡量信息量的大小就是看这个信息消除不确定性的程度。

- “太阳从东边升起”，这条信息没有减少不确定性，因为太阳肯定是从东边升起的，这是一句废话，信息量为0
- ”2022年中国队成功进入世界杯“，从直觉上来看，这句话具有很大的信息量。因为中国队进入世界杯的不确定性因素很大，而这句话消除了进入世界杯的不确定性，所以按照定义，这句话的信息量很大。



归纳关系：信息量的大小与==内容/事件发生的概率==成反比

- 事件发生的概率越大，信息量越小。
- 事件发生的概率越小，信息量越大。



设某一事件发生的概率为P(x)，其信息量表示为：$I(x)=-log_e(P(x))$

- 这里的log是以e为底的自然对数





## 信息熵Entropy

信息熵用来表示所有信息量的期望，在信息论里面， 熵是用于描述==对**不确定性**的度量==

> 传输1比特的信息意味着将接收者的不确定性降低2倍 —— 香浓
>
> - **不确定性就是事件概率的倒数** 
>   - 概率为1，确定事件 => 熵就是0，没有不确定性 => 信息量小

所以信息量的熵可以表示为：（这里的X是一个离散型随机变量）

![image-20210319145533348](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319145533.png)

对于 0-1 分布问题：

![image-20210319145558339](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319145558.png)



## 相对熵（KL散度）

如果对于同一个随机变量X有两个单独的概率分布P(x)和Q(x)，则我们可以==用KL散度来衡量这两个概率之间的差异== => 优化目标：差异最小化

![image-20210319145632643](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319145632.png)



在机器学习中，使用P(x)来表示样本的真实分布，Q(x)来表示模型所预测的分布

比如在一个三分类任务中(例如，猫狗马分类器)，x1，x2，x3分别代表猫，狗，马

例如一张猫的图片真实分布P(X) = [1,0,0]，预测分布Q(X) = [0.7,0.2,0.1]，则KL散度为：

![image-20210319145705492](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319145705.png)





## 交叉熵⭐

如果我们的预测是完美的，那就是**预测分布等于真实分布**，此时**交叉熵就等于熵**

但是，如果分布不同，则**交叉熵将比熵大一些位数**。**交叉熵超过熵的量称为相对熵**，或更普遍地称为**库尔贝克-莱布里埃发散度（KL Divergence**）



将KL散度公式拆开：KL散度 = 交叉熵 - 信息熵

- p是真实分布
- q是预测分布

![image-20210319145740071](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319145740.png)

- H(p(x))表示信息熵

- 交叉熵

![image-20210319145822820](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319145822.png)

在机器学习训练网络时，输入数据与标签常常已经确定，那么真实概率分布P(x)也就确定下来了，所以信息熵就是一个常量。

那么KL散度的值表示真实概率分布P(x)与预测概率分布Q(x)之间的差异，值越小表示预测的结果越好，==最小化KL散度的本质其实等价于最小化交叉熵==，所以就有了交叉熵损失函数。





# 应用|交叉熵损失

## 二分类|配合sigmoid激活层

- sigmoid作为最后一层输出：将最后一层的每个神经元看作一个二项分布，对应的 target 属于二项分布

第 i 个神经元交叉熵为：

![image-20210319150006334](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319150006.png)

神经元的输出是$a=\sigma(z)$，n是训练数据的总数

交叉熵是非负的，在神经元达到很好的正确率的时候会接近0，符合代价函数的特性。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319150136.png" alt="image-20210319150135962" style="zoom:80%;" />





## 多分类|配合softmax激活层

softmax作为激活层：

- 激活层上的数值和为1

![image-20210319150210095](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319150210.png)

交叉熵函数为(y是label值，a是预测值)：

![image-20210319150234949](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319150235.png)



示例：

对于一个互斥的三分类问题，如果某一个样例数据的正确答案是(1,0,0)。经过softmax回归后的预测答案是(0.5,0.4,0.1)，那么这个预测和正确答案之间的交叉熵为：

![image-20210319150439745](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319150439.png)



## 小结|分类器：softmax vs. logistic回归

对于一个k分类问题，选用softmax分类器还是用logistic回归算法建立k个独立的二元分类器取决于分类类别之间是否互斥。



例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归

如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。





# 交叉熵损失函数的优化

## 单任务

相对于多任务学习而言，我们常见的模型大部分都是单任务学习，只有一个学习目标(loss)



### 样本不平衡

对于样本不平衡，除了对小类的数据样本进行过采样，对大类的数据进行欠采样来减少大类的数据样本。

也可以对损失函数进行修正，==减少大类的数据对损失的影响程度，增加小类的影响程度==。



对于普通的交叉熵损失函数，通过一个α参数来调节，缓解正样本少的问题

![image-20210319150853786](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319150853.png)



这种方法其实比较常见，在SSD物体识别模型中，由于大部分priors产生的predictions中，大部分是不包含物体的negative样本。

在设计confidence loss 的时候，用了Hard Negative Mining难例挖掘。

选择hard negative的样本，并限定数量。

![image-20210319150925445](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319150925.png)

> 在目标检测中我们会事先标记好ground_truth，接下来在图片中随机提取一系列sample，与ground_truth重叠率IoU超过一定阈值的(比如0.5)，则认为它是positive sample，否则为negative sample
>
> 考虑到实际负样本数>>正样本数，我们为了避免network的预测值少数服从多数而向负样本靠拢，取正样本数：负样本数大约为1:3
>
> 显而易见，用来训练网络的负样本为提取的负样本的子集，那么，我们当然选择负样本中容易被分错类的困难负样本来进行网络训练。
>
> 那么负样本中哪些是困难负样本(hard negative)呢？困难负样本是指哪些容易被网络预测为正样本的proposal，即假阳性(false positive)，如roi里有二分之一个目标时，虽然它仍是负样本，却容易被判断为正样本，这块roi即为hard negative，训练hard negative对提升网络的分类性能具有极大帮助，因为它相当于一个错题集。
>
> 
>
> 如何判断它为困难负样本呢？
>
> 也很简单，我们先用初始样本集(即第一帧随机选择的正负样本)去训练网络，再用训练好的网络去预测负样本集中剩余的负样本，选择其中得分最高，即最容易被判断为正样本的负样本为困难样本，加入负样本集中，重新训练网络，循环往复。





### 难易不平衡

上面我们解决了正负样本不平衡的问题，但是实际情况下样本可以分为下面四类：

|      | 难   | 易   |
| :--- | :--- | :--- |
| 正   | 正难 | 正易 |
| 负   | 负难 | 负易 |

虽然α平衡了正负样本，但是对难易样本的不平衡没有解决。

实际上，必然存在一些样本容易学习(特征鲜明)，一些样本较难学习。

以目标检测来说，大量的候选目标都是易分样本（例如大部分候选框其实是背景，或者明显包含一个种类的物体，比较难分的候选框是包含两种或者多种物体的框，这些候选框比较少）。

虽然易分样本的损失很低，但是由于数量上==易分样本的数量实在太多，最终还是会主导损失函数==，导致模型对难分样本的学习能力较弱，从而影响模型的效果。



#### loss优化|Focal loss

假设：易分样本（置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本。

（这个假设是有一定问题的，后面GHM的改进目标）



一个简单的方法，通过增加指数，把高置信度样本的损失再降低一些，那么高置信度样本的p也会特别大。

一个三分类的问题，真实值是（1，0，0）,属于第一类。如果预测值是（0.98，0.02，0）那么这个其实就是个易分样本。

最终的Focal Loss的形式：

![image-20210319151156338](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319151156.png)



==实验表明γ取2, α取0.25的时候效果最佳==

|      | 难         | 易            |
| :--- | :--------- | :------------ |
| 正   | 正难       | 正易 γ衰减    |
| 负   | 负难 α衰减 | 负易 α、γ衰减 |

==训练过程关注对象的排序==为正难>负难>正易>负易



#### loss优化|GHM(gradient harmonizing mechanism)

首先Focal Loss存在很多问题：

1. 关注难分样本：如果样本中存在离群点，==Focal Loss过分关注离群点==，那么模型就跑偏了。
2. λ和α是==超参数，全靠经验==，而且两个超参数还会互相影响。



Focal Loss是从置信度p的角度入手衰减loss，而GHM是一定范围置信度p的样本数量的角度衰减loss，根据（一定梯度内）的样本数量进行衰减，也就是谁的样本数量多，就衰减谁。



首先定义梯度模长g：

- 为什么叫做梯度模长，因为g是从交叉熵损失求梯度得来的。

![image-20210319151321421](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319151321.png)

其中p是模型预测的概率，$p^*$是 ground-truth的标签，$p^*$的取值为0或1。

g正比于检测的难易程度，g越大则检测难度越大。

这个其实看公式也很好理解，毕竟预测偏差越大说明检测难度大。

![image-20210319151428687](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319151428.png)



梯度模长接近于0的样本数量最多，随着梯度模长的增长，样本数量迅速减少，但是在梯度模长接近于1时，样本数量也挺多。



GHM的想法是，我们确实==不应该过多关注易分样本==，但是特别难分的样本（outliers，离群点）也不该关注。

这些离群点的梯度模长d要比一般的样本大很多，如果模型被迫去关注这些样本，有可能降低模型的准确度，并且这些样本的数量也很多。



那怎么同时衰减易分样本和特别难分的样本呢?太简单了，谁的数量多衰减谁呗！

那怎么衰减数量多的呢？定义一个变量，让这个变量能衡量出一定梯度范围内的样本数量——这就是物理上密度的概念：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210319151541.png" alt="image-20210319151541313" style="zoom:80%;" />





## 多任务

多任务损失函数的确定和融合是个难点。



简单的融合方法：

- 简单求和(求和)
- 加权求和(权重难以确定，基本需要多次实验)





# #参考文献

[link: 信息论-交叉熵及其优化](https://breezepqf.cn/2020/06/02/loss-function/)





