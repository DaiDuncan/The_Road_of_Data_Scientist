[link: 原文链接|Datawhale 2019.12.07](https://mp.weixin.qq.com/s/q4R-TAG4PZAdWLb41oov8g)

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414094704.png" alt="image-20210414094704755" style="zoom:80%;" />

XGBoost 是**大规模并行** boosting tree 的工具，它是目前最快最好的开源 boosting tree 工具包，比常见的工具包快 10 倍以上。

Xgboost 和 GBDT 两者都是 boosting 方法，除了工程实现、解决问题上的一些差异外，最大的不同就是目标函数的定义。



# 1 数学原理

## 1.1 目标函数

我们知道 XGBoost 是由 k 个基模型组成的一个加法运算式：

- $f_k$是第 k 个==基模型==
- $\hat{y}_i$是==第 i 个样本的预测值==

![image-20210414094950384](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414094950.png)

损失函数可由预测值$\hat{y}_i$与真实值$y_i$进行表示：

![image-20210414101729434](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414101729.png)

其中 n 为样本数量。

我们知道模型的预测精度由模型的偏差和方差共同决定：

- 损失函数代表了模型的偏差
- 想要方差小则需要简单的模型

所以目标函数由模型的损失函数 L 与抑制模型复杂度的正则项$\Omega$组成，所以我们有：

![image-20210414101807832](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414101808.png)

$\Omega$为模型的正则项，由于 XGBoost 支持决策树==也支持线性模型==，所以这里不再展开描述。

我们知道 boosting 模型是==前向加法==，以第 t 步的模型为例，模型对第 i 个样本$x_i$的预测为：

- 前相加法 => @GBDT：累加减少残差

![image-20210414101900475](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414101900.png)



其中$\hat{y}_i^{t-1}$由第 t-1 步的模型给出的预测值，是已知常数， $f_t(x_i)$是我们这次需要加入的新模型的预测值，此时，目标函数就可以写成：

![image-20210414101937919](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414101938.png)

求此时最优化目标函数，就相当于求解$f_t(x_i)$。

![image-20210414102056013](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414102056.png)

![image-20210414102212596](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414102212.png)





## 1.2 基于决策树的目标函数

Xgboost 的基模型不仅支持决策树，还支持线性模型，这里我们主要介绍基于决策树的目标函数。

我们可以将决策树定义为$f_t(x)=w_{q(x)}$，x 为某一样本，这里的 q(x) 代表了该样本在哪个叶子结点上，而 ==w_q 则代表了叶子结点取值 w==，所以 就代表了**每个样本的取值 w** （即预测值）。

决策树的复杂度可由叶子数 T 组成，==叶子节点越少模型越简单==(一阶正则化)，此外叶子节点也不应该含有过高的权重 w(二阶正则化) （类比 LR 的每个变量的权重），所以目标函数的正则项可以定义为：

![image-20210414102444310](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414102444.png)

@me|深刻理解：正则化避免过拟合 => 减少模型复杂度

即决策树模型的复杂度由生成的所有决策树的叶子节点数量，和所有节点权重所组成的向量的$L_2$范式共同决定

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/vI9nYe94fsFms0HmibFPHjZR48CSy3I8Dq6WvBn4tNRbE7fwjhPG4QbhkNYWAUFnZic9r7FjoOz1LvrWb2sZuicfA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



![image-20210414102834117](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414102834.png)

![image-20210414102908729](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414102908.png)

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414102919.webp)

上图给出目标函数计算的例子，

- 求每个节点每个样本的一阶导数$g_i$和二阶导数$h_i$，
- 然后针对每个节点对所含样本求和得到的$G_j$和$H_j$，
- 最后遍历决策树的节点即可得到目标函数。





## 1.3 最优切分点的划分算法

在决策树的生长过程中，一个非常关键的问题是如何找到叶子的节点的最优切分点，Xgboost 支持两种分裂节点的方法——贪心算法和近似算法。

### **1）贪心算法**

1. 从深度为 0 的树开始，对每个叶节点枚举所有的可用特征；
2. 针对每个特征，把属于该节点的训练样本==根据该特征值进行升序排列==，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；
3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集
4. 回到第 1 步，递归执行到满足特定条件为止



那么如何计算每个特征的分裂收益呢？

假设我们在某一节点完成特征分裂，则分裂前的目标函数可以写为：

![image-20210414103332363](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414103332.png)



注意该特征收益也可作为特征重要性输出的重要依据。

对于每次分裂，我们都需要枚举所有特征可能的分割方案，如何高效地枚举所有的分割呢？

我假设我们要枚举所有 x < a 这样的条件，对于某个特定的分割点 a 我们要计算 a 左边和右边的导数和。

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414103345.webp)

我们可以发现对于所有的分裂点 a，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和$G_L$和$G_R$。

然后用上面的公式计算每个分割方案的分数就可以了。



### 2）近似算法|候选点: 分箱⭐

贪婪算法可以的到最优解，但==当数据量太大时则无法读入内存进行计算==，近似算法主要针对贪婪算法这一缺点给出了近似最优解。

对于每个特征，只**考察分位点**可以减少计算复杂度。

该算法会首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。



在提出候选切分点时有两种策略：

- Global：学习每棵树前就**提出候选切分点**，并在每次分裂时都采用这种分割；
- Local：每次分裂前将**重新提出候选切分点**。

直观上来看，Local 策略需要更多的计算步骤，而 Global 策略因为节点没有划分**所以需要更多的候选点**。

下图给出不同种分裂策略的 AUC 变换曲线，

- 横坐标为迭代次数，纵坐标为测试集 AUC，eps 为近似算法的精度，其倒数为桶的数量。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414103537.webp" alt="图片" style="zoom: 80%;" />



我们可以看到 Global 策略在候选点数多时（eps 小）可以和 Local 策略在候选点少时（eps 大）具有**相似的精度**。

此外我们还发现，在 eps 取值合理的情况下，**分位数策略可以获得与贪婪算法相同的精度**。

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414103617.webp)

- 第一个 for 循环：对特征 k 根据该特征分布的分位数找到切割点的候选集合$S_k=\{s_{k1}, s_{k2}, ..., s_{kl} \}$。XGBoost 支持 Global 策略和 Local 策略。
- 第二个 for 循环：针对每个特征的候选集合，将样本映射到由该特征对应的候选点集构成的分桶区间中，即$s_{k, v} \ge x_{jk} > s_{k, v-1}$，对每个桶统计 G,H 值，最后在这些统计量上寻找最佳分裂点。



下图给出近似算法的具体例子，以三分位为例：

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414103750.webp)

- 根据**样本特征进行排序**，
- 然后**基于分位数**进行划分，
- 并统计三个桶内的 G,H 值，
- 最终求解节点划分的增益。





## 1.4 候选点原理|基于样本二阶导$h_i$ (加权分位数 缩略图)

事实上， XGBoost 不是简单地按照样本个数进行分位，而是以二阶导数值$h_i$作为样本的权重进行划分，如下：

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414103943.webp)

那么问题来了：为什么要用$h_i$进行样本加权？

![image-20210414104011921](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414104012.png)

对于样本权值相同的数据集来说，找到候选分位点已经有了解决方案（GK 算法），但是当样本权值不一样时，该如何找到候选分位点呢？（作者给出了一个 Weighted Quantile Sketch 算法，这里将不做介绍。）



## 1.5 稀疏感知算法⭐

在决策树的第一篇文章中我们介绍 CART 树在==应对数据缺失==时的分裂策略，XGBoost 也给出了其解决方案。

XGBoost 在构建树的节点过程中只考虑**非缺失值的数据遍历**，而为每个节点增加了一个缺省方向，**当样本相应的特征值缺失时，可以被归类到缺省方向上**，最优的缺省方向可以从数据中学到。

- @me-理解|样本有多个特征值：有部分特征值缺失

至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，==选择增益最大的枚举项即为最优缺省方向==。

在构建树的过程中需要枚举特征缺失的样本，乍一看该算法的计算量增加了一倍，但其实该算法在构建树的过程中只考虑了**特征未缺失的样本遍历**，

而特征值缺失的样本无需遍历只需直接分配到左右节点，故算法所需遍历的样本量减少，下图可以看到**稀疏感知算法**比 basic 算法速度块了超过 50 倍。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414104219.webp" alt="图片" style="zoom:67%;" />





# 2 工程实现

## 2.1 块结构设计

我们知道，决策树的学习最耗时的一个步骤就是在每次寻找最佳分裂点是==都需要对特征的值进行排序==。

而 XGBoost 在训练之前对**根据特征对数据进行了排序**，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式（Compressed Sparse Columns Format，CSC）进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。

- 每一个块结构包括一个或多个已经排序好的特征；
- **缺失特征值将不进行排序**；
- 每个特征会存储指向样本梯度统计值的索引，方便计算一阶导和二阶导数值；

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210414104243.webp)

这种块结构存储的特征之间相互独立，方便计算机进行并行计算。

在对节点进行分裂时需要选择增益最大的特征作为分裂，这时各个特征的增益计算可以同时进行，这也是 Xgboost 能够实现**分布式或者多线程计算**的原因。



## 2.2 缓存访问优化算法

块结构的设计可以减少节点分裂时的计算量，但特征值通过索引访问样本梯度统计值的设计会导致访问操作的**内存空间不连续**，这样会造成缓存命中率低，从而影响到算法的效率。

为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：**为==每个线程==分配一个连续的缓存区**，将需要的梯度信息存放在缓冲区中，这样就是实现了**非连续空间到连续空间的转换**，提高了算法效率。

此外适当调整块大小，也可以有助于缓存优化。



## 2.3 “核外”块计算

当数据量过大时无法将数据全部加载到内存中，只能先将无法加载到内存中的数据**暂存到硬盘**中，直到需要时再进行加载计算，而这种操作必然涉及到因内存与硬盘速度不同而造成的资源浪费和性能瓶颈。

为了解决这个问题，XGBoost **==独立一个线程专门用于从硬盘读入数据==**，以实现处理数据和读入数据同时进行。

此外，XGBoost 还用了两种方法来降低硬盘读写的开销：

- 块压缩：对 Block 进行按列压缩，并在读取时进行解压；
- 块拆分：将每个块存储到不同的磁盘中，从多个磁盘读取可以增加吞吐量。





# 3 小结|优缺点

## 3.1 优点

1. 精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost  对损失函数进行了二阶泰勒展开。
   - XGBoost 引入二阶导一方面是为了增加精度，
   - 另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；
   
2. 灵活性更强：

   - GBDT 以 CART 作为基分类器，
   - XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的**逻辑斯蒂回归（分类问题）或者线性回归（回归问题）**）。

   - 此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；

3. 正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；

4. Shrinkage（缩减）：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；

5. 列抽样：XGBoost 借鉴了**随机森林的做法，支持列抽样**，不仅**能降低过拟合，还能减少计算**；

6. 缺失值处理：XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；

7. 可以并行化操作：块结构可以很好的支持并行计算。



## 3.2 缺点

1. 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但**在节点分裂过程中仍需要遍历数据集**；
2. **预排序过程的空间复杂度过高，不仅需要存储特征值**，还需要存储特征对应样本的梯度统计值的**索引**，相当于消耗了两倍的内存。

