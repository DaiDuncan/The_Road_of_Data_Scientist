以决策树为基础，诞生了一大批集成算法，包括Random Forest、Adaboost、**GBDT**、**xgboost**，lightgbm

![img](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210417163027.jpeg)

大部分集成模型都通过一个基础学习算法来生成一个同质的基础学习器，即同类型的学习器，也叫同质集成。

有同质集成就有异质集成，为了集成后的结果表现最好，异质基础学习器需要尽可能准确并且差异性够大。



# Bagging|民主 => 减少方差

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210420090111.png" alt="image-20210420090110986" style="zoom:80%;" />

所有基础模型都一致对待，每个基础模型手里都只有一票。

然后使用民主投票的方式得到最终的结果。

- 根据具体问题采用不同的分类或回归方法，如决策树、感知器等
- 大部分情况下，**经过 bagging 得到的结果方差（variance）更小**。



通过应用Iris数据集的分类问题来距离说明bagging。我们可以使用两种基础模型：决策树和KNN。

图像1展示了基础模型与集成模型学习得到的决策边界。

- Accuracy: 0.63 (+/- 0.02) [Decision Tree]
- Accuracy: 0.70 (+/- 0.02) [K-NN]
- Accuracy: 0.64 (+/- 0.01) [Bagging Tree]
- Accuracy: 0.59 (+/- 0.07) [Bagging K-NN]

![img](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210420091618.jpeg)

决策树学到的是轴平行边界，然而k=1最近邻对数据拟合的最好。

bagging通过训练10个基础模型以及**随机选择80%的数据作为训练集，同样随机选择80%的特征进行训练。**

- K-NN对于训练样本的扰动并不敏感，这也是为什么K-NN成为稳定学习器的原因。

- **整合稳定学习器对于提升泛化性能没有帮助**
  - 图像结果同样展示了通过**增加集成模型的个数**带来的测试准确率变化。基于交叉验证的结果，我们可以看到整合基础模型个数大于10个之后性能就基本不再提升了，只是带来了计算复杂度的增加。

- 最后一张图绘制的是集成学习模型的学习曲线，注意训练集数据的平均误差为0.3，在对训练集做80%采样的时候训练集和验证集误差最小。



## Bagging与bootstrap(有放回抽样)

**Bagging**是对数据做行抽样, 使用bootstrap sampling采样m次, 可并行获得m个模型, 再将m个模型进行组合. 每个模型的权重相同.



## 随机森林

- 每个树模型都是bagging采样训练的
- 特征也是随机选择的
- 对于训练好的树也是随机选择的

这种处理的结果是随机森林的偏差增加的很少，而由于弱相关树模型的平均，方差也得以降低，最终得到一个方差小，偏差也小的模型。



在一个极端的随机树算法中，随机应用的更为彻底：**训练集分割的阈值**也是随机的，即每次划分得到的训练集是不一样的。这样通常能够进一步减少方差，但是会带来偏差的轻微增加。



# Boosting|挑精英(改变数据分布) => 减少偏差

![image-20210420090222537](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210420090222.png)

对基础模型不是一致对待的，而是经过不停的考验和筛选来挑选出「精英」，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。

- 每一轮训练都提升那些错误率小的基础模型权重，同时减小错误率高的模型权重
- 在每一轮改变训练数据的权值或概率分布，通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果
- 大部分情况下，**经过 boosting 得到的结果偏差（bias）更小**。



Boosting指的是通过算法集合将弱学习器转换为强学习器。boosting的主要原则是训练一系列的弱学习器，所谓弱学习器是指仅比随机猜测好一点点的模型，例如较小的决策树，训练的方式是**利用加权的数据**。



## Adaboost

- $\epsilon$表示单个分类器的加权错误率
- $\alpha$是分类器的权重

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210420092051.jpeg" alt="img" style="zoom:80%;" />

![img](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210420092137.jpeg)

每个基础模型包含一个深度为1的决策树，这种决策树依靠线性划分进行分类，决策平面跟其中一个轴平行。





## GBDT

boosting算法在损失函数上的泛化

Gradient Boosting采用串行方式构建模型：

- $F_m(x)=F_{m-1}(x)+\gamma_m \cdot h_m(x)$
  - $h_m(x)=argmin_h \sum_{i=1}^n L(y_i, F_{m-1}(x_i) + h(x_i))$



# 小结|Bagging vs. Boosting

1 样本选择

- Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的
- Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的**权重**发生变化
  - 权值是根据上一轮的分类结果进行调整



2 样例权重

- Bagging：使用均匀取样，每个样例的权重相等
- Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大



3 预测函数

- Bagging：所有预测函数的权重相等。
- Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。



4 并行计算

- Bagging：各个预测函数可以并行生成
- Boosting：各个预测函数**只能顺序生成**，因为后一个模型参数需要前一轮模型的结果。



# Stacking => 提升预测结果

通过一个元分类器或者元回归器来整合多个分类模型或回归模型的集成学习技术

- 基础模型利用整个训练集做训练
- 元模型==将基础模型的特征==作为特征进行训练



基础模型通常包含不同的学习算法，因此stacking通常是异质集成

![img](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210420092517.jpeg)

![img](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210420092554.jpeg)

各基础模型的预测结果如下：

Accuracy: 0.91 (+/- 0.01) [KNN]

Accuracy: 0.91 (+/- 0.06) [Random Forest]

Accuracy: 0.92 (+/- 0.03) [Naive Bayes]

Accuracy: 0.95 (+/- 0.03) [Stacking Classifier]



分别在K-NN，Random Forest,Naive Bayes做训练和预测，然后将其输出结果作为特征，利用逻辑回归作为元模型进一步训练

- 如图所示，stacking集成的结果由于每个基础模型，并且没有过拟合。



Stacking被Kaggle竞赛获奖者广泛使用。

例如，[Otto Group Product分类挑战赛的第一名](https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335)通过对30个模型做stacking赢得了冠军。

- 他==将30个模型的输出作为特征==，继续在三个模型中训练，这三个模型XGBoost，Neural Network和Adaboost，

- 最后再**加权**平均。

![img](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210420092739.jpeg)



# 总结

## 1 拓展

集成学习还被广泛应用于**利用多种分类器做训练**的深度学习模型中。

深度学习模型中的分类器可能在**架构、超参数以及训练技巧**上存在差异，都可以进行集成。

集成学习已经被证明在数据比赛中能够获得较好的成绩。





# #参考文献

1 上述图像结果的[代码|github](https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/ensemble_methods.ipynb)



[link: 主要|知乎专栏 2018.06.06](https://zhuanlan.zhihu.com/p/36161812)

- Zhi-Hua Zhou, “Ensemble Methods: Foundations and Algorithms”, CRC Press, 2012
- L. Kuncheva, “Combining Pattern Classifiers: Methods and Algorithms”, Wiley, 2004
- [Kaggle Ensembling Guide](https://link.zhihu.com/?target=https%3A//mlwave.com/kaggle-ensembling-guide/)
- [Scikit Learn Ensemble Guide](https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/ensemble.html)
- [S. Rachka, MLxtend library](https://link.zhihu.com/?target=http%3A//rasbt.github.io/mlxtend/)
- [Kaggle Winning Ensemble](https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335)