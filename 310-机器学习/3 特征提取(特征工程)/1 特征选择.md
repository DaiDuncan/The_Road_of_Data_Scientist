## 导读：特征工程

特征选择（Feature Selection,FS）和特征抽取（Feature Extraction, FE）是特征工程Feature Engineering的两个重要的方面。  

他们之间最大的区别就是是否生成新的属性。



FS仅仅对特征进行排序（Ranking）和选择， FE更为复杂，需要重新认识事物， 挖掘新的角度， 创新性的创立新的属性， 而目前深度学习这么火， 一个很重要的原因是缩减了特征提取的任务。 

不过， 目前特征工程依然是各种机器学习应用领域的重要组成部分。

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092054.webp)

# 1. 为什么要特征选择？

## 1.1 特征选择与分类器性能的关系

一般说来，当固定一个分类器的话， 所选择的==特征数量和分类器的效果==之间会有一个曲线， 在某个x（1<=x<=n）的地方， 会达到最优。

那么， 该怎么选这x个特征呢？ 这是一个比较难的问题。  

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092209.jpeg" alt="图片" style="zoom:67%;" />

## 1.2 为什么特征少了不行？

这个比较直观， 特征少了会导致无法区分的情况发生。

如下图所示，仅仅依赖x1或者x2特征， 都无法区分这两类数据， 所以当特征数量过小， 很可能导致数据重叠。进而， 所有分类器都会失效。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092242.webp" alt="图片" style="zoom:67%;" />



## 1.3 为什么特征多了也不行？

那为什么特征多了也不行呢？ 

如下图所示，明明根据纵轴来判断就可以容易的区分两类， 但是因为引入了横轴的特征， 使得同类数据在空间中距离变远，变稀疏了。

进而使得很多分类器失效。 所以， 特征不是越多越好！

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092307.webp" alt="图片" style="zoom:80%;" />





# 2. 特征选择的一般流程

根据前面如何得到一个最优的子集， 那么特征选择的的一般流程就是： 

- 找一个集合，然后针对某个学习算法， 测试效果如何
- 一直循环直到找到最优集合为止。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092507.jpeg" alt="图片" style="zoom:80%;" />

如果把 Evaluation 再突出表示， 那么整个流程就会是如下图这样一个过程

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092515.webp" alt="图片" style="zoom:80%;" />



那么按照这个流程， 是不是特征选择就解决呢？  

固定一个算法， 然后搜索（Search）一个测试子集， 然后进行 Cross-Validation 进行评价，直到找到最优目标。 那么这个搜索空间有多大呢？假设有n个特征， 每个特征可以选择或者不选择， 那么就是2的n次方的搜索空间，n为特征数量。

所以基本上n=10的时候，就是一个1024次的尝试。

如果每次尝试1分钟， 光这个就需要1天时间。  尤其当n>10的情况， 是非常常见的。



所以==当n<10的时候==， 看上去一些暴力搜索算法还是可以尝试一下的。

其他一些常见的搜索算法也可以试试：

- 穷举法（Exhaustive）： 暴力穷尽
- 贪心法（Greedy Selection）： 线性时间
- 模拟退火（Simulated Annealing）： ==随机尝试找最优==
- 基因算法（Genetic Algorithm）： 组合深度优先尝试
- 邻居搜索（Variable Neighbor Search）： 利用相近关系搜索



我们把基于Cross-Validation和基因算法（GA）的一般流程列在如下：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092538.webp" alt="图片" style="zoom:80%;" />

但是， 始终当n很大的时候， 这个搜索空间会很大，如何找最优值还是需要一些**经验结论**。



# 3. 机器学习特征选择的经典三刀？

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092739.jpeg" alt="图片" style="zoom:80%;" />

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092723.jpeg" alt="图片" style="zoom:80%;" />

## Filter 过滤|小李飞刀

快速无比， 但是能不能打的准，还得看各自功力



顾名思义，就是要基于贪心的思想， 把需要的特征筛/滤出来。  

一般说来， 基于贪心就需要对特征进行打分。  

而这个==打分可以基于领域知识==， 相关性， 距离，缺失， 稳定性等等。



### 单一特征选择

根据每个特征属性和目标属性之间的计算值来进行排序选择：

#### a. Welch's t-Test  

来判断两个属性的**分布的均值、方差距离**。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092844.webp" alt="图片" style="zoom:80%;" />

#### b. Fish-Score

和Welch's t-Test类似， 计算两个分布的距离， **均值之差和方差之和的距离**。

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324092859.png)

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324093016.webp" alt="图片" style="zoom:80%;" />



#### c. Chi-Squared test

计算类别离散值之间的相关性。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324093021.png" alt="图片" style="zoom:80%;" />

#### d. Information Gain

计算两个划分的一致性。

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324093035.png)



### 多特征选择

根据多个特征属性和目标属性之间的计算值来进行排序选择：

#### e. Relief-F

根据随机选择的样本点，来计算属性之间的相关性。

<img src="https://mmbiz.qpic.cn/mmbiz_png/REGqUlN1rzLgvcGwrZjLDIzTq1ibY0oSzYTS8eBYL0sTqrn3Z8moVbzLugolH3icrTmP2YJ6gFgNlJDPgnktfB3w/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:80%;" />

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324093138.webp)

#### f. Correlation Feature Selection (CFS)

利用属性之间的相关性， 进行选择

<img src="http://mmbiz.qpic.cn/mmbiz_png/REGqUlN1rzLgvcGwrZjLDIzTq1ibY0oSzicGQic9S2mib5AhcUicjnFvhByYYPQu8AAZfvZiaGicO2IQLWCHaoTJScHtw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:80%;" />







## Wrapper 圆月弯刀

 实力无穷， 但是会不会用， 需要点悟性和魔力，还靠点运气



就是==先选定特定算法==，然后再根据算法效果来选择特征集合。  

一般会选用==普遍效果较好的算法==， 例如Random Forest， SVM， kNN等等。

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324093232.webp)

这可以使用前面提到的各种减小搜索空间的尝试。 

其中最经典的是使用启发式搜索（Heuristic Search）， 而概括的来说主要分为两大类：

#### g. Forward Selection

挑出一些属性， 然后慢慢增大挑出的集合。



#### h. Backward Elimination

删除一些属性，然后慢慢减小保留的集合。





## Embedded 高频电刀

模式单调，快速并且效果明显， 但是如何参数设置， 需要深厚的背景知识



利用正则化思想， 将部分特征属性的权重变成零。 

常见的正则化有L1的Lasso，L2的Ridge和混合的Elastic Net。

其中L1的算子有明显的特征选择的功能。

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324093313.webp)

在这里面，比较简单的就是会自动进行特征选择， 而且一次性就搞定了， 速度也不错

 难点就是：

- 损失函数的选择
- 缩放参数的选择



### 常见的损失函数

如下图所示：

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324100553.webp)

#### 1 Binary(0-1) loss

灰线 对应错误率



#### 2 Square loss

紫线 对应 最小二乘法

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324100643.webp)



#### 3 Hinge loss

浅蓝 对应 SVM

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324100704.webp)



#### 4 Logistic loss

红线 对应 逻辑回归

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324100711.webp)



#### 5 Exponential loss

绿线 对应 adaboost

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324100746.webp)



#### 6 结构化Lasso

除了上述的简单的Lasso，还有结构化的Lasso



#### i. Group Lasso

先将属性进行分组， 然后对每个分组，看成一个属性那样，采用Lasso的方法选择，

- 要么全要， 要么全部不要。 
- 再进一步， Sparse Group Lasso再在组内进行选择。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324100954.png" alt="image-20210324100953868" style="zoom:80%;" />



#### j. Tree-Structured Lasso

除了前面的扁平（Flat）的结构外， 还有层次化的结构。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324101021.webp" alt="图片" style="zoom:80%;" />

对于这种情况， 同样采用类似Group Lasso的思想。 

对于一个数的子结构， 要么全要或者全不要，也可以允许分支单独要。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324101048.jpeg" alt="图片" style="zoom:80%;" />



在这种树结构的思想下， 那么==每个树的分支的正则化惩罚==也可以采用不同的形式。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324101110.jpeg" alt="图片" style="zoom:80%;" />



#### k. Graph Structure Lasso

更进一步还可以推广到基于图的Lasso

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324101210.jpeg" alt="图片" style="zoom:80%;" />

在图的情况下， 那么**两两节点之间**要定义一个惩罚项。

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210324101229.png" alt="图片" style="zoom:80%;" />



树结构Lasso也可以利用图来表示：

![图片](http://mmbiz.qpic.cn/mmbiz_png/REGqUlN1rzLgvcGwrZjLDIzTq1ibY0oSz8R6iaU8ib9cicEXs0JJOV2RtIUia5ofhmc8425zP8II12B1VcqP9nDBonQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



# 总结

很多时候， 需要综合上面的三刀来进行综合选择。  

特征选择不是一个容易的任务，要用好需要自己实战体会的



下面再简单归纳下：

Filter （F刀）：

- 优点： 快速， 只需要基础统计知识。

- 缺点：特征之间的**组合效应**难以挖掘。



Wrapper（W刀）：

- 优点： 直接面向算法优化， 不需要太多知识。

- 缺点： 庞大的搜索空间， 需要定义启发式策略。



Embedded（E刀）：

- 优点： 快速， 并且面向算法。

- 缺点： 需要调整结构和参数配置， 而这==需要深入的知识和经验==。









# #参考文献

[link: 史春奇|大数据应用](https://mp.weixin.qq.com/s?__biz=MzIzMDA1MTM3Mg%3D%3D&mid=2653085166&idx=3&sn=0aaa7a0bf351117e5949a9e7027c4cb5&scene=45#wechat_redirect)

http://www.rokkincat.com/blog/2016/04/28/feature-selection

http://www.denizyuret.com/2014/02/machine-learning-in-5-pictures.html