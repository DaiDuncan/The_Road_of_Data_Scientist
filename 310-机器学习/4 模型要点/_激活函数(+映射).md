<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317150811.png" alt="image-20210317150811063" style="zoom:80%;" />

每个激活函数的输入都是一个数字，然后对其进行某种固定的数学操作 => 把输入的特征保留并映射下来

激活函数给神经元引入了非线性因素，如果不用激活函数的话，无论神经网络有多少层，输出都是输入的线性组合。



激活函数的发展经历了Sigmoid -> Tanh -> ReLU -> Leaky ReLU -> Maxout这样的过程，还有一个特殊的激活函数Softmax，因为它只会被用在网络中的最后一层，用来进行最后的分类和归一化。

![preview](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317150112.jpeg)

## Sigmoid

sigmoid非线性函数的数学公式是
$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$
函数图像如下图所示。它输入实数值并将其映射到0到1范围内，适合输出为概率的情况，可以用来做二分类。

- 在特征相差比较复杂，或是相差不是特别大时效果比较好

但是现在已经很少有人在构建神经网络的过程中使用sigmoid。



sigmoid 原函数及导数：

由图可知，导数从 0 开始很快就又趋近于 0 了，易造成“梯度消失”现象

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317150949.png" alt="image-20210317150949344" style="zoom:67%;" />

### 缺陷

- Sigmoid函数饱和使梯度消失。

  当神经元的激活在接近0或1处时会饱和，在这些区域梯度几乎为0，这就会导致==梯度消失，几乎就有没有信号通过神经传回上一层==。

- Sigmoid函数的输出不是零中心的。？？

  因为如果输入神经元的数据总是正数，那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降。







## Tanh

数学公式是：
$$
f(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}
$$

$$
tanh(x)=2\sigma(2x)-1
$$

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317151317.png" alt="image-20210317151316897" style="zoom:80%;" />

也称为双切正切函数，取值范围为[-1,1]。
tanh在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果。

- 与 sigmoid 的区别是，tanh 是均值为0的，因此实际应用中 tanh 会比 sigmoid 更好



### 缺陷

Tanh解决了Sigmoid的输出是不是零中心的问题，但==仍然存在饱和问题==



为了防止饱和，现在主流的做法会在激活函数前==多做一步batch normalization==

- 尽可能保证每一层网络的输入具有均值较小的、零中心的分布。



## ReLU

近些年来非常流行

Rectified Linear Unit(ReLU) - 用于隐层神经元输出
$$
\phi(x)=max(0, x)
$$
一维时：

![image-20210317151512668](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317151512.png)

- 输入信号 <0 时，输出都是0，

- \>0 的情况下，输出等于输入



二维时：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317153912.png" alt="image-20210317153911808" style="zoom: 67%;" />

### 优点

发现使用 ReLU 得到的 SGD 的收敛速度会比 sigmoid/tanh 快很多

- ReLU对于随机梯度下降的收敛有巨大的加速作用

- sigmoid和tanh在求导时含有指数运算，而ReLU求导几乎不存在任何计算量



对比sigmoid类函数主要变化是：

1）单侧抑制；

2）相对宽阔的兴奋边界；

3）稀疏激活性。





### 缺陷

训练的时候很”脆弱”，很容易就”die”了，而且是不可逆的，因此导致了数据多样化的丢失(在整个训练集中这些神经元都不会被激活)

例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.

如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了

- ==通过合理设置学习率，会降低神经元“死掉”的概率==



## Leaky ReLU

$$
f(y)=max(\epsilon y, y)
$$

ε是很小的负数梯度值，比如0.01

这样做目的是使负轴信息不会全部丢失，解决了ReLU神经元“死掉”的问题

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317151918.png" alt="image-20210317151918279" style="zoom:67%;" />



有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定



更进一步的方法是PReLU，即把ε==当做每个神经元中的一个参数==，是可以通过梯度下降求解的。

- Kaiming He 等人在 2015 年发布的论文 [Delving Deep into Rectifiers](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.01852) 中介绍了一种新方法PReLU，把负区间上的斜率当做每个神经元中的一个参数来训练。
- 然而该激活函数在在不同任务中表现的效果也没有特别清晰。



## Maxout⭐

Maxout是对ReLU和leaky ReLU的一般化归纳，函数公式是
$$
f(x)=max(w_1^Tx + b_1, w_2^Tx + b_2)
$$

- 具有ReLU的优点，如计算简单，不会 saturation
- 同时又没有ReLU的一些缺点，如容易go die

![image-20210317153253569](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317153253.png)

最后二次方图：使用多条直线的maxout来近似拟合二次函数



### 缺陷

每个神经元的参数double，这就导致整体参数的数量激增



## softmax函数⭐

用于多分类神经网络输出

![image-20210317152900032](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317152900.png)

Softmax是Sigmoid的扩展，当类别数k＝2时，Softmax回归退化为Logistic回归



示例：

![image-20210317152937313](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210317152937.png)

如果某一个 zj 大过其他 z, 那这个映射的分量就逼近于 1,其他就逼近于 0，主要应用就是多分类

- 为什么要取指数，第一个原因是要模拟 max 的行为，所以要让大的更大

- 第二个原因是==需要一个可导的函数==⭐



# sigmoid ReLU softmax的比较

1 sigmoid 的梯度消失问题，ReLU 的导数就不存在这样的问题

2 Sigmoid 和 Softmax 区别

- sigmoid将一个real value映射到（0,1）的区间，用来做二分类
- softmax 把一个 k 维的real value向量（a1,a2,a3,a4….）映射成一个（b1,b2,b3,b4….）其中 bi 是一个 0～1 的常数，输出神经元之和为 1.0，所以相当于概率值，然后可以根据 bi 的概率大小来进行多分类的任务
  - 二分类问题时 sigmoid 和 softmax 是一样的，求的都是 cross entropy loss

3 softmax建模使用的分布是多项式分布，而logistic则基于伯努利分布

- 多个logistic回归通过叠加也同样可以实现多分类的效果
  -  softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类
  - 多个logistic回归进行多分类，输出的类别并不是互斥的，即"苹果"这个词语既属于"水果"类也属于"3C"类别



# 如何选择？

通常来说，很少会把各种激活函数串起来在一个网络中使用的



根据各个函数的优缺点来配置，例如：

如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元

- 如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout.



最好不要用 sigmoid，可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout





# #参考文献

[link: 知乎专栏|常用激活函数的比较](https://zhuanlan.zhihu.com/p/32610035)  2018.01.05

 [CS231n](http://cs231n.stanford.edu/) 中关于激活函数部分的笔记  2017.02.13



待消化：

[link: 10个激活函数](https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247553850&idx=1&sn=afbdcd777edcde423c6e8e5b668b4131&chksm=e873c277df044b61ece0b2fb35fd686afb1cee39099186e8126ed1d8819f54f73e85ea24f64f&mpshare=1&scene=24&srcid=0305MCM1Jj3KjehbX4XTvQiJ&sharer_sharetime=1614896783831&sharer_shareid=04169dcd804a29a99e43aea044294546&key=9166a59242b53cf53d7f91472c23f2831d7690ef5121e6a35a1441dfa34944b33bda06902e2512f6964573e0a41d2bb99435ae117289a8f85fab1332530d6b6a8d0898ac2d5a8f9657ef4cf38d959238ad5498cdc7e04c884b79fb0540169bd951b55c9c5663dff18f399e274348d777b2b7768806041aa9fc8309ed4d837380&ascene=14&uin=MTUwNzc5NjY4MA%3D%3D&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=Ac8lBZ9xgocGatLJHeHJtJ4%3D&pass_ticket=LmTf02kT2tX3Osjcr9LS4TjjYXImJMzHvqGJPJT5%2BL2gWgjuxZJN9CfvaI%2FZcNl1&wx_header=0)

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210322161130.png" alt="image-20210322161130396" style="zoom:67%;" />

1 Sigmoid 

2 Tanh / 双曲正切

3 ReLU

4 Leaky ReLU

5 ELU 

![image-20210322161226068](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210322161226.png)

6 PReLU（Parametric ReLU）

![image-20210322161249734](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210322161250.png)

7 Softmax

8 Swish 

y = x * sigmoid (x)

Swish 的设计受到了 LSTM 和高速网络中 gating 的 sigmoid 函数使用的启发

使用相同的 gating 值来简化 gating 机制，这称为 self-gating



9 Maxout

由两个 Maxout 节点组成的 Maxout 层可以很好地近似任何连续函数

- 激活函数是输入的最大值

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210322161514.png" alt="图片" style="zoom:80%;" />



10 Softplus

f（x）= ln（1 + exp x）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210322161522.png" alt="图片" style="zoom:67%;" />





