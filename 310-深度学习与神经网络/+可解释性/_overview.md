![在现代机器学习算法中，可解释性与精确度难以两全其美。深度学习精确度最高，同时可解释性最低。](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210507094739.png)

作为IBM的一名研究科学家，迪米特里·马里奥托夫（Dmitry Malioutov）其实不太说得上来自己究竟打造了什么。

他的部分工作内容是打造机器学习系统、解决IBM公司客户面临的棘手问题。

例如，他曾为一家大型保险公司编写了一套程序。这项任务极具挑战性，要用到一套十分复杂的算法。

**在向客户解释项目结果时**，马里奥托夫更是大伤脑筋。“我们没办法向他们解释这套模型，因为他们没受过机器学习方面的培训。”



其实，就算这些客户都是机器学习专家，可能也于事无补。

因为马里奥托夫打造的模型为人工神经网络，要从特定类型的数据中寻找规律。

在上文提到的例子中，这些数据就是保险公司的客户记录。

此类网络投入实际应用已有半个世纪之久，但近年来又有愈演愈烈之势。

从语音识别到语言翻译，从下围棋的机器人到自动驾驶汽车，各行各业都在该技术的驱动下出现了新的突破。



虽然现代神经网络的表现令人激动，但也面临一个棘手的问题：**没人理解它们的运行机制**，这也就意味着，没人能预测它们何时可能失灵。

以机器学习专家里奇·卡鲁阿纳（Rich Caruana）和同事们前几年报告的一起事件为例：匹兹堡大学医学中心的一支研究团队曾利用机器学习技术预测肺炎患者是否会出现严重并发症。

他们希望将并发症风险较低的患者转移到门诊进行治疗，好腾出更多床位和人手。该团队试了几种不同的方法，包括各种各样的神经网络，以及由软件生成的**决策树，后者可总结出清晰易懂、能被人类理解的规则**。



神经网络的正确率比其它方法都要高。

但当研究人员和医生们分析决策树提出的规则时，==却发现了一些令人不安的结果：按照其中一条规则，医生应当让已患有哮喘的肺炎病人出院==，而医生们都知道，哮喘患者极易出现并发症。

这套模型完全遵从了指令：从数据中找出规律。它之所以给出了如此差劲的建议，其实是由数据中的一个巧合导致的。

按照医院政策，身患哮喘的肺炎患者需接受强化护理。而这项政策效果极佳，哮喘患者几乎从不会产生严重并发症。由于这些额外护理改变了该医院的患者记录，算法预测的结果也就截然不同了。

==这项研究充分体现了算法“可解释性”的价值所在==。

卡鲁阿纳解释道：“如果这套以规则为基础的系统学到了‘哮喘会降低并发症风险’这一规则，神经网络自然也会学到这一点。”但人类读不懂神经网络，因此很难预知其结果。马里奥托夫指出，若不是有一套可解释的模型，“这套系统可能真的会害死人。”



正因为如此，许多人迟疑不前、不敢对神秘莫测的神经网络下注。马里奥托夫为客户提供了两套模型：

- 一套是神经网络模型，虽然精确，但难以理解；
- 另一套则是以规则为基础的模型，能够用大白话向客户解释运作原理。

尽管保险公司对精确度要求极高，每个百分点都十分重要，但客户仍选择了精确度稍逊的第二套模型。“他们觉得第二套模型更容易理解，”马里奥托夫表示，“他们非常看重直观性。”



随着神秘难解的神经网络影响力与日俱增，就连政府都开始对其表示关注。

欧盟两年前提出，应给予公民“要求解释”的权利，算法决策需公开透明。

但这项立法或许难以实施，因为立法者并未阐明“透明”的含义。也不清楚这一省略是由于立法者忽略了这一问题、还是觉得其太过复杂导致。

事实上，有些人认为这个词根本无法定义。

目前我们虽然知道神经网络在做什么（毕竟它们归根到底只是电脑程序），但我们对“怎么做、为何做”几乎一无所知。

神经网络由成百上千万的独立单位、即神经元构成。每个神经元都可将大量数字输入转化为单个数字输出，再传递给另一个、或多个神经元。就像在人脑中一样，这些神经元也分成若干“层”。一组细胞接收下一层细胞的输入，再将输出结果传递给上一层。



神经网络可通过输入大量数据进行训练，同时不断调整各层之间的联系，直到该网络计算后输出的结果尽可能接近已知结果（通常分为若干类别）。

近年来该领域之所以发展迅猛，还要归功于几项可快速训练深度网络的新技术。

在深度网络中，初始输入和最终输出之间相隔多层。有一套叫AlexNet的著名深度网络，可对照片进行归类，根据照片的细微差别将其划入不同类别。

该网络含有超过6000万个“权重”，根据不同权重，神经元会对每项输入给予不同程度的关注。

隶属于康奈尔大学和AI初创公司Geometric Intelligence的计算机科学家杰森·尤辛斯基（Jason Yosinski）指出：“要想理解这个神经网络，你就要对这6000万个权重都有一定的了解。”

而就算能够实现这种可解读性，也未必是件好事。对可解读性的要求相当于制约了系统的能力，**使模型无法仅关注输入输出数据**、提供“纯粹”的解决方案，从而有降低精确度之嫌。

美国国防部高级研究计划局项目主管戴维·甘宁（David Gunning）曾在一次会议上对此进行了总结。

在他展示的图表中，==深度神经网络是现代机器学习方法中最难以理解的一种，而以规则为基础、重视可解释性胜过效率的决策树则是最容易理解的一种==。



现代机器学习技术为开发者提供了不同的选择：究竟是要精确获知结果，还是要以牺牲精确度为代价、了解出现该结果的原因？

- “了解原因”可帮助我们制定策略、做出适应、并预测模型何时可能失灵。
- 而“获知结果”则能帮助我们即刻采取恰当行动。



这实在令人左右为难。

但一些研究人员提出，如果既能保留深度网络的多层构造、又能理解其运作原理，岂不是最好？

令人惊奇的是，一些最受看好的研究所其实是将神经网络作为实验对象看待的，即沿袭生物科学的思路，而不是将其视作纯数学的研究对象。

尤辛斯基也表示，他试图“通过我们了解动物、甚至人类的方式来了解深度网络。”他和其他计算机科学家借鉴了生物研究技术，借神经科学家研究人脑的方式研究神经网络：对各个部件展开详细分析，记录各部件内部对微小输入变化的反应，甚至还会移除某些部分、观察其余部分如何进行弥补。

在从无到有地打造了一种新型智能之后，科学家如今又将其拆开，用数字形式的“显微镜”和“手术刀”对这些“虚拟器官”展开分析。



尤辛斯基坐在一台电脑前、对着网络摄像头说话。摄像头接收的数据被输入深度神经网络，而与此同时，该网络也在由尤辛斯基和同事们开发的Deep Visualization（深度可视化）软件工具包进行分析。尤辛斯基在几个屏幕间来回切换，然后将网络中的一个神经元放大。“这个神经元似乎能够对面部图像做出反应。”人脑中也有这种神经元，其中多数都集中在一处名为“梭状脸区”（fusiform face area）的脑区中。

该脑区最早由1992年开始的一系列研究发现，被视作人类神经科学最可靠的观察结果之一。

对脑区的研究还需借助正电子发射计算机断层扫描等先进技术，但尤辛斯基只需凭借代码、便可对人造神经元展开详细分析。

借助该方法，尤辛斯基可**将特定的人造神经元与人类能理解的概念或物体（如人脸）建立起映射关系**，从而将神经网络变为有力工具。该研究还挑明了图片中最容易激发面部神经元反映的特征。“眼睛颜色越深、嘴唇越红，神经元的反应就更为强烈。”



杜克大学计算机科学、电子与计算机工程教授辛西娅·鲁丁（Cynthia Rudin）认为，这些“事后解读”本身是有问题的。

她的研究重点为以规则为基础的机器学习系统，可应用于罪犯量刑、医疗诊断等领域。

在这些领域中，人类能够进行解读，且人类的解读十分关键。但在视觉成像等领域，“个人的解读结果纯属主观。”诚然，我们可以通过识别面部神经元、对神经网络的响应进行简化，但如何才能确定这就是该网络寻找的结果呢？无独有偶，有一套著名理论认为，不存在比人类视觉系统更简单的视觉系统模型。“对于一个复杂系统在做什么事情，可以有很多种解释，”鲁丁指出，“难道从中选出一个你‘希望’正确的解释就行了吗？”

尤辛斯基的工具包可以通过逆向工程的方式、找出神经网络自身“希望”正确的结果，从而在部分程度上解决上述问题。

该项目先从毫无意义的“雪花”图像开始，然后对像素进行逐个调整，通过神经网络训练的反向过程逐步修改图片，最终找出能够最大限度激发某个神经元响应的图片。

将该方法运用到AlexNet神经元上后，该系统生成了一些奇奇怪怪的照片，虽然看上去颇为诡异，但的确能看出属于它们被标记的类别。



这似乎支持了尤辛斯基的论断：这些面部神经元寻找的确实是面部图像。

但还有一个问题：在生成这些图像的过程中，该流程依赖了一种名为“自然图像优先”（natural image prior）的统计学约束，因此其生成的图像均会模仿真实物体照片的结构。

而当他去除这些规则后，该工具包仍会选取被其标记为“信度最大”的图片，但这些图片看上去就像电视机失去信号时的“雪花”一样。

事实上，尤辛斯基指出，AlexNet倾向于选择的大部分图片在人眼看来都是一片乱糟糟的“雪花”。他承认道：“很容易弄清如何让神经网络生成极端结果。”



为避免这些问题，弗吉尼亚理工大学电子与计算机工程助理教授杜鲁夫·巴特拉（Dhruv Batra）采用了一种更高级的实验方法对深度网络进行解读。

他**没有去试图寻找网络内部结构的规律**，而是用一种**眼动追踪技术分析神经网络的行为**。

在研究生阿比谢克·达斯（Abhishek Das）和哈什·阿格拉瓦尔（Harsh Agrawal）的带领下，巴特拉的团队向一个深度网络就某张图片提问，如房间窗户上是否有窗帘等等。不同于AlexNet或类似系统，**达斯的网络每次只关注图片的一小部分，然后“扫视”图片各处**，直到该网络认为已经得到了足够的信息、可以回答问题为止。

经过充分训练后，这一深度神经网络的表现已经非常出色，精确度足以与人类的最高水平媲美。



接下来，达斯、巴特拉和同事们还想了解该网络是如何做出决策的。

于是他们分析了该网络在图片上选取的观察点。

而结果令他们大吃一惊：在回答“图中是否有窗帘”的问题时，该网络根本没去寻找窗户，而是**先对图片底部进行观察，如果发现了床铺，就停下来不找了**。看来在用来训练该网络的数据集中，装有窗帘的窗户可能常出现在卧室里。



该方法虽然揭露了深度网络的一些**内部运行机制**，但也凸显了可解释性带来的挑战。

==巴特拉指出：“机器捕捉到的并不是关于这个世界的真相，而是关于数据集的真相。”==

这些机器严格按照训练数据进行了调整，因此很难总结出它们运作机制的普遍规则。

更重要的是，你要是不懂它如何运作，就无法预知它会如何失灵。而从巴特拉的经验来看，当它们失灵的时候，“就会输得一败涂地。”



下图为深度神经网络中的单个神经元（由绿框标出）对尤辛斯基的面部图像做出反应，就像人脑的某个脑区（标为黄色）也会对面部图像做出反应一样。

![](https://img.mix.sina.com.cn/auto/resize?img=%2F%2Fn.sinaimg.cn%2Ftech%2Ftransform%2F158%2Fw630h328%2F20180725%2FrKnF-hftenia0809229.png&size=640_0&blur=1&blur_sigma=2)



图为“深度视觉化”工具包生成的“理想猫脸”。

该程序先从类似电视机没信号时的“雪花”图像开始，对像素进行逐个调整，直到AlexNet神经网络的面部神经元产生最大响应为止。

![图为“深度视觉化”工具包生成的“理想猫脸”。该程序先从类似电视机没信号时的“雪花”图像开始，对像素进行逐个调整，直到AlexNet神经网络的面部神经元产生最大响应为止。](https://img.mix.sina.com.cn/auto/resize?img=%2F%2Fn.sinaimg.cn%2Ftech%2Ftransform%2F466%2Fw630h636%2F20180725%2FcY9_-hftenia0810116.png&size=640_0&blur=1&blur_sigma=2)





尤辛斯基和巴特拉等研究人员面临的一些障碍对人脑科学家来说也并不陌生。

例如，对神经成像的解读就常常遭到质疑。

2014年，认知神经科学家玛莎·法拉（Martha Farah）在一篇领域评述中写道：“令人担忧的是……（功能性脑部）图像更像是研究人员的创造发明、而非观察结果。”这一问题在各种智能系统中也屡屡出现，说明无论对人脑、还是对智能的研究而言，该问题都会成为一大障碍。





追求可解释性是否为一项愚蠢之举呢？2015年，加州大学圣地亚哥分校的扎克利·立顿（Zachary Lipton）发表了一篇名为《模型可解释性的迷思》（The Myth of Model Interpretability）的博文，批判性地探讨了解读神经网络背后的动机、以及为大型数据集打造可解读的机器学习模型的价值。

在2016年国际机器学习大会（ICML）上，他还向马里奥托夫与两名同事组织的“人类可解释性”专题研讨会提交了一篇与该话题相关的、颇具争议性的论文。

**立顿指出，许多学者并不赞同“可解释性”这一概念。**因此他认为，要么是人们对可解释性的理解还不够，要么是它有太多可能的含义。

无论是哪种情况，追求可解释性也许都无法满足我们对“简单易懂的神经网络输出”的渴求。

立顿在博文中指出，==当数据集过大时，研究人员完全可以抑制去解读的冲动，要相信“凭借经验也能成功”==。

他表示，该领域的目的之一，便是要“打造学习能力远超人类的模型”，如果太过注重可解释性，就难以使这类模型充分发挥潜力。



但这种能力既是特点也是缺陷：如果我们不明白网络输出是如何生成的，就无从知晓该网络需要何种输入。

1996年，英国苏塞克斯大学的艾德里安·汤普森（Adrian Thompson）采用与如今训练深度网络相似的技术、用软件设计了一款电路。

这一电路要执行的任务很简单：区分两个音频的音调。经过成千上万次调整和重排后，该软件终于找到了一种能近乎完美地完成任务的配置。

但汤普森惊讶地发现，**该电路所用元件数量比任何人类工程师的设计都要少**，甚至有几个元件根本没有和其它元件相连。

而要让电路顺利运作，这些元件应当不可或缺才对。



于是他对电路进行了剖析。做了几次实验后，他发现**该电路的相邻元件间存在微弱的电磁干扰**。

未与电路相连的元件通过干扰邻近电场、对整个电路造成了影响。

人类工程师通常会杜绝这类干扰，因为干扰的结果难以预料。

==果不其然，若用另一组元件复制该电路布局，甚至只是改变环境温度，同样的电路便会彻底失灵。==



该电路揭露了机器训练的一大重要特征：它们**总是尽可能紧凑简洁**，与所在环境完美相容，但往往难以适应其它环境。

它们能抓住工程师发现不了的规律，但不知道别处是否也有这一规律。

==机器学习研究人员想尽力避免这种名为“过拟合”（overfitting）的现象。==

但随着应用这些算法的情况愈发复杂多变，这一缺陷难免会暴露出来。



普林斯顿大学计算机科学教授桑吉夫·阿罗拉（Sanjeev Arora）认为，这一问题是人类追求可解释模型的主要动机，希望有了可解释模型后、能对网络进行干预和调整。

距阿罗拉表示，有两大问题可体现缺乏可解释性对机器性能造成的硬性限制。

- 一是“组合性”（composability）：当一项任务同时涉及多项决策时（如围棋或自动驾驶汽车），神经网络便无法高效判定是哪个决策导致了任务失败。“
  - 人类在设计某样东西时，会先弄清不同元件的作用，再将其组合在一起，因此能够对不适合当前环境的元件进行调整。”

- 二是阿罗拉所称的“域适应性”（domain adaptability），即将在某种情境中学到的知识灵活运用于其它情境的能力。
  - 人类非常擅长这项任务，但机器则会出现各种离奇错误。
  - 据阿罗拉描述，即使只对环境做了微调、人类调整起来不费吹灰之力，计算机程序也会遭遇惨败。
  - 例如，某个网络经过训练后、能对维基百科等正式文本的语法进行分析，但如果换成推特这样的口语化表达，就会变得毫无招架之力。



按这样来看，可解释性似乎不可或缺。

但我们真的理解它的意思吗？著名计算机科学家马文·闵斯基用“手提箱词汇”（suitcase word）来形容这类词汇，包括“意识”（consciousness）、“情绪”（emotion）等用来描述人类智能的单词。

闵斯基指出，这些词其实反映了多种多样的内在机制，但都被锁在“手提箱”中。

一旦我们用这些词代替了更根本性的概念、仅对这些词汇进行研究，我们的思维就会被语言所局限。

那么在研究智能时，“可解释性”会不会也是这样一个“手提箱词汇”呢？



虽然很多研究人员都持乐观态度，认为理论学家迟早能打开这个“手提箱”、发现某套主宰机器学习（或许也包括人类学习）的统一法则或原理，就像牛顿的力学原理一样。

==但也有人警告称，这种可能性微乎其微。==

纽约城市大学哲学教授马西莫·皮戈里奇（Massimo Pigliucci）指出，神经科学、乃至人工智能领域所谓的“理解”也许是一种“集群概念”，即可能有多个不同定义。如果该领域真的有“理解”之说，也许相对于物理学、会更接近进化生物学的情况。

也就是说，我们将发现的也许不是“基本力学原理”，而是“物种起源学说”。



当然，这并不意味着深度网络将预示着某种新型自主生命的出现。但深度网络就像生命本身一样费解。

该领域采用的渐进式实验手段和事后解读方式也许并不是在黑暗中苦苦等待理论之光时的绝望情绪，而是我们能够盼来的唯一光芒。

==可解释性也许会以碎片化的形式呈现出来，就像不同类别的“物种”一样，采用的分类法则根据具体情境而定。==



在国际机器学习大会的专题研讨会结束时，部分发言人参加了一场讨论会，试图给“可解释性”下一个定义。结果每个人都各执一词。

==进行了一系列讨论后，大家似乎达成了一点共识：一个模型要能被解释，就要具备“简单性”（simplicity）。==

但在简单性的定义问题上，大家又产生了分歧。“最简单”的模型究竟是指依赖最少特征的模型？还是程序规模最小的模型？还是有其它解释？一直到研讨会结束，大家都没有达成共识。

正如马里奥托夫说的那样：“简单性并不简单。”（叶子）























