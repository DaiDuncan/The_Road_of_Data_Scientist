RL：deep reinforcement learning



# [Udacity讲师 2021.05.24](https://www.youtube.com/watch?v=SgC6AZss478&list=RDCMUCgBncpylJ1kiVaPyP-PZauQ&index=6)

建模：网格图



## 1 导论|通用机器学习与强化学习

没有数据：只有环境Environment + Agent

- 根据奖励rewards和惩罚punishment行动

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618201727.png" alt="image-20210618201726653" style="zoom: 50%;" />



## 2 Markov decision processes (MDP)

Gridworld

- values
- states
- actions
- policy：决策decision



示例|继承临近栅格的value

- s表示状态，s'表示邻近/上一个状态
- V()表示value

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618202227.png" alt="image-20210618202227107" style="zoom:50%;" />









## 3 Rewards

考虑行动的Reward：假设每行动一步的消耗是1

- 惩罚随机游走 => 移动消耗

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618202507.png" alt="image-20210618202507085" style="zoom:50%;" />

问题：右下角 局部最优

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618202614.png" alt="image-20210618202614211" style="zoom:50%;" />

## 4 Discount factor

贬值 => 当前行动的“及时性”：抓住时机，及时行动

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618202729.png" alt="image-20210618202728755" style="zoom:50%;" />



考虑"贬值效应" => 激励及时行动

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618202803.png" alt="image-20210618202802488" style="zoom: 50%;" />



## 5 Bellman equation

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618203010.png" alt="image-20210618203009753" style="zoom:50%;" />





## 6 Solving the Bellman equation

假设discount factor $\gamma = 1$(不考虑“贬值效应”)

1 初步：基于`+4` => 以目标为起点@动态规划：以终为始

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618203258.png" alt="image-20210618203257794" style="zoom:50%;" />

2 优化：基于所有可能的栅格@HMM viterbi算法

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618203316.png" alt="image-20210618203315673" style="zoom:50%;" />

3 最终收敛：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618203336.png" alt="image-20210618203335713" style="zoom:50%;" />



如果grid很大，states、actions/policys很多 => 使用神经网络

- 神经网络的参数更少 => 提取重要的特征



## 7 Deterministic vs stochastic processes

随机性：多一些可能的探索

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618203723.png" alt="image-20210618203723142" style="zoom:50%;" />

Policy：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618203805.png" alt="image-20210618203804728" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618203841.png" alt="image-20210618203841292" style="zoom:50%;" />





## 8 Neural networks

特征：value大的栅格，周边栅格的value也很大(value为负值时，同理)

- policy也同理



value神经网络：

- 输入时栅格坐标x, y
- 输出是该栅格的value

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618204141.png" alt="image-20210618204140767" style="zoom:50%;" />

policy神经网络：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618204313.png" alt="image-20210618204313086" style="zoom:50%;" />



<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618204352.png" alt="image-20210618204352287" style="zoom:50%;" />

## 9 Value neural networks/Q-networks

目标值：bellman equation V(s)

- “带标签”：value是确定的(尤其是reward所在的栅格)

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618204752.png" alt="image-20210618204752322" style="zoom: 50%;" />



损失函数：

在减少损失函数的过程中，更新参数逐步变化(value)，而不是一步到位

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618204909.png" alt="image-20210618204909313" style="zoom:50%;" />



最终结果：

- 如何选择policy? => 直奔value最高的值

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618205033.png" alt="image-20210618205032693" style="zoom:50%;" />



## 10 Policy neural networks

优化目标：Gain的值越大，该policy的可能性越高

- x，y => 对应gain, direction是绑定的



1 随机walk 1

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618213854.png" alt="image-20210618213853862" style="zoom:50%;" />

2 随机walk2

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618214026.png" alt="image-20210618214026207" style="zoom:50%;" />

3 随机walk3

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618214057.png" alt="image-20210618214057016" style="zoom:50%;" />



4 最终：多次迭代后

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618214134.png" alt="image-20210618214133772" style="zoom:50%;" />





## 11 Training the policy neural network

优化目标：预测概率接近label

1 针对value神经网络

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618214306.png" alt="image-20210618214305610" style="zoom:50%;" />

❓me|Q：label=1是怎么来的？

- me|难道是随机walk中，路径的由来？(@参考policy神经网络的训练)



2 针对policy神经网络

- 增加Gain的信息
  - Gain和direction的来源：Bellman eqution => 决定了下一步的方向

Gain是正数：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618214439.png" alt="image-20210618214439337" style="zoom:50%;" />

Gain是负数：

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618214504.png" alt="image-20210618214504049" style="zoom:50%;" />

![image-20210618214557623](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618214558.png)



❓me：

Q1：Gain和direction如何确定？

- 随机walk的前提下：根据Bellman eqution => 决定了下一步的方向

Q2：调整程度的深浅，具体对应的是什么

- me|深浅：可以用Gain来表示

Q3：那么随机walk如何保证“广泛地遍历”，以使得结果收敛？



policy Gradient示例：针对栅格点(3, 2)

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210618215830.png" alt="image-20210618215823076" style="zoom:50%;" />





## 小结

> 作者感谢朋友：Udacity RL的老师

1 神经网络：本质识别某种特征

| 网络           | 输入 | 输出       | 训练     |
| -------------- | ---- | ---------- | -------- |
| value network  |      | value      | 目标函数 |
| police network |      | policy决策 |          |

