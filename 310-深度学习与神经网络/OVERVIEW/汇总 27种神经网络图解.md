[link: 公众号链接](https://mp.weixin.qq.com/s/8NYfGpCKIjtGwChWqt0t_w)

[link: 原文链接](https://medium.com/towards-data-science/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464) 2017.08.04

| 节点类型              |              |
| --------------------- | ------------ |
| 【Input Cell】        |              |
| Backfed-              | 后馈式       |
| Noisy-                | 噪声         |
|                       |              |
| 【Hidden Cell】       |              |
| Probablistic-         | 概率         |
| Spiking-              | 散发         |
|                       |              |
| 【Output Cell】       |              |
| Match Input-          | 匹配输入     |
|                       |              |
| 【特殊节点】          |              |
| Recurrent Cell        | 循环单元     |
| Memory Cell           | 存储单元     |
| Different Memory Cell | 不同存储单元 |
| Kernel                | 内核         |
| Convolution or Pool   | 卷积或池     |

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412163347.webp)

来自Asimov研究所的Fjodor van Veen编写了一个关于神经网络的精彩图表（就是上面那张大图）。

# 1 Perceptron 感知机

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412200553.webp)

最简单和最古老的神经元模型。接收一些输入，把它们加总，通过激活函数并传递到输出层。



# 2 前馈神经网络（FF）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412200601.webp" alt="图片" style="zoom:50%;" />

这也是一个很古老的方法——这种方法起源于50年代。它的工作原理通常遵循以下规则：

1.所有节点都完全连接

2.激活从输入层流向输出，无回环

3.输入和输出之间有一层（隐含层）

在大多数情况下，这种类型的网络使用**反向传播方法**进行训练。



# 3 RBF 神经网络

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412200628.webp" alt="图片" style="zoom:67%;" />

**RBF 神经网络**实际上是**激活函数是径向基函数**而非逻辑函数的FF前馈神经网络（FF）。

两者之间有什么区别呢？

- sigmoid逻辑函数将某个任意值映射到[0 ,... 1]范围内来，回答“是或否”问题。适用于分类决策系统，但不适用于连续变量。

- 相反，径向基函数能显示“我们距离目标有多远”。 这完美适用于函数逼近和==机器控制（例如作为PID控制器的替代）==。

简而言之，**这些只是具有不同激活函数和应用方向的前馈网络。**



# 4 DFF深度前馈神经网络⭐

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412200819.webp" alt="图片" style="zoom:67%;" />

**DFF深度前馈神经网络**在90年代初期开启了深度学习的潘多拉盒子。这些依然是前馈神经网络，但有不止一个隐含层。那么，它到底有什么特殊性？

在训练传统的前馈神经网络时，我们只向上一层传递了少量的误差信息。

由于堆叠更多的层次导致训练时间的指数增长，使得深度前馈神经网络非常不实用。



直到00年代初，我们开发了一系列有效的训练深度前馈神经网络的方法; **现在它们构成了现代机器学习系统的核心，能实现前馈神经网络的功能，但效果远高于此。**



# 5 RNN递归神经网络

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412200905.webp" alt="图片" style="zoom:67%;" />

**RNN递归神经网络**引入不同类型的神经元——==递归神经元==。

这种类型的第一个网络被称为约旦网络（Jordan Network），在网络中每个隐含神经元会收到它自己的**在固定延迟（一次或多次迭代）后的输出**。

除此之外，它与普通的模糊神经网络非常相似。



当然，它有许多变化：如传递状态到输入节点，**可变延迟**等，但主要思想保持不变。

这种类型的神经网络主要被使用在上下文很重要的时候：即过去的迭代结果和样本产生的决策会对当前产生影响。

最常见的上下文的例子是文本：一个单词只能**在前面的单词**或**句子的上下文中**进行分析。



# 6 LSTM长短时记忆网络

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412201135.webp)

**LSTM长短时记忆网络**引入了一个存储单元，一个特殊的单元，当数据有时间间隔（或滞后）时可以处理数据。

递归神经网络可以==通过“记住”前十个词来处理文本==，LSTM长短时记忆网络可以通过“记住”许多帧之前发生的事情处理==视频帧==。 

LSTM网络也广泛用于写作和语音识别。



存储单元实际上由一些元素组成，称为门，它们是递归性的，并控制信息如何被记住和遗忘。

下图很好的解释了LSTM的结构:

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412201214.webp)

上图的（x）是门，他们拥有自己的权重，有时也有激活函数。

在每个样本上，他们决定是否传递数据，擦除记忆等等 - 你可以在这里(http://colah.github.io/posts/2015-08-Understanding-LSTMs/)阅读更详细的解释。 

- 输入门(Input Gate)决定上一个样本有多少的信息将保存在内存中; 

- 输出门调节传输到下一层的数据量；
- 遗忘门(Forget Gate)控制存储记忆的损失率。



然而，**这是LSTM单元的一个非常简单的实现，还有许多其他架构存在。**





# 7 GRU？

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412201504.webp" alt="图片" style="zoom:67%;" />

**GRU是具有不同门的LSTM**。

听起来很简单，但缺少输出门可以更容易**基于具体输入重复多次相同的输出**，目前此模型在声音（音乐）和语音合成中使用得最多。



实际上的组合虽然有点不同：**但是所有的LSTM门都被组合成所谓的更新门(Update Gate)，并且复位门(Reset Gate)与输入密切相关。**

它们比LSTM消耗资源少，但几乎有相同的效果。





# 8 Autoencoders自动编码器

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412201553.webp" alt="图片" style="zoom:50%;" />

**Autoencoders自动编码器**用于==分类，聚类和特征压缩==。

当您训练前馈(FF)神经网络进行分类时，您主要必须在Y类别中提供X个示例，并且期望Y个输出单元格中的一个被激活。 这被称为“监督学习”。



另一方面，自动编码器==可以在没有监督的情况下进行训练==。

它们的结构：

- 当隐藏单元数量**小于**输入单元数量（并且输出单元数量等于输入单元数）时，
- 并且当自动编码器被训练时 输出尽可能接近输入的方式，强制自动编码器泛化数据并搜索常见模式。



# 9 变分自编码器

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412201743.webp" alt="图片" style="zoom:50%;" />

**变分自编码器**，与一般自编码器相比，它==压缩的是概率，而不是特征==。

尽管如此简单的改变，但是一般自编码器只能回答当“我们如何归纳数据？”的问题时，**变分自编码器回答了“两件事情之间的联系有多强大？我们应该在两件事情之间分配误差还是它们完全独立的？”**的问题。



在这里(https://github.com/kvfrans/variational-autoencoder)可以看到一些更深入的解释（含代码示例）。



# 10 降噪自动编码器（DAE）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412201816.webp" alt="图片" style="zoom:50%;" />

虽然自动编码器很酷，但它们**有时找不到最鲁棒的特征**，而只是适应输入数据（实际上是过拟合的一个例子）。



**降噪自动编码器（DAE）**在输入单元上**增加了一些噪声** - 通过随机位来改变数据，随机切换输入中的位，等等。

通过这样做，一个强制降噪自动编码器从一个有点嘈杂的输入重构输出，==使其更加通用，强制选择更常见的特征==。





# 11 稀疏自编码器（SAE）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412201924.webp" alt="图片" style="zoom:50%;" />

稀疏自编码器（SAE）是另外一个有时候可以抽离出数据中一些隐藏分组样试的自动编码的形式。

结构和AE是一样的，但**隐藏单元的数量**大于输入或输出单元的数量。





# 12 马尔可夫链（Markov Chain, MC）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412201954.webp" alt="图片" style="zoom:67%;" />

**马尔可夫链（Markov Chain, MC）**是一个比较老的图表概念了，它的==每一个端点都存在一种可能性==。

过去，我们用它来搭建像“在单词hello之后有0.0053％的概率会出现dear，有0.03551%的概率出现you”这样的文本结构。



这些马尔科夫链**并不是典型的神经网络**，它可以被用作：

- 基于概率的分类（像贝叶斯过滤）
- 用于聚类（对某些类别而言）
- 也被用作有限状态机。





# 13 霍普菲尔网络（HN）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202041.webp" alt="图片" style="zoom:67%;" />

**霍普菲尔网络（HN）**对**一套有限的样本进行训练**，所以它们用相同的样本对已知样本作出反应。

在训练前，**每一个样本都作为输入样本，在训练之中作为隐藏样本，使用过之后被用作输出样本。**



在HN试着重构受训样本的时候，他们可以用于给输入值降噪和修复输入。

如果给出一半图片或数列用来学习，它们可以反馈全部样本。





# 14 波尔滋曼机（BM）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202110.webp" alt="图片" style="zoom:67%;" />

**波尔滋曼机（BM）**和HN非常相像，有些单元被标记为输入同时也是隐藏单元。

在隐藏单元更新其状态时，输入单元就变成了输出单元。（在训练时，BM和HN一个一个的更新单元，而非并行）。

这是第一个==成功保留模拟退火方法==的网络拓扑。



多层叠的波尔滋曼机可以用于所谓的深度信念网络（等一下会介绍），==深度信念网络可以用作特征检测和抽取==。





# 15 限制型波尔滋曼机（RBM）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202144.webp" alt="图片" style="zoom:50%;" />

在结构上，**限制型波尔滋曼机（RBM）**和BM很相似，但由于受限RBM被允许像FF一样用反向传播来训练（唯一的不同的是在**反向传播**经过数据之前RBM会经过一次输入层）。





# 16 深度信念网络（DBN）

![图片](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202214.webp)

像之前提到的那样，**深度信念网络（DBN）**实际上是**许多波尔滋曼机**（被VAE包围）。

他们能被连在一起（在一个神经网络训练另一个的时候），并且可以用已经学习过的样式来生成数据。





# 17 深度卷积网络（DCN）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202246.webp" alt="图片" style="zoom:67%;" />

当今，**深度卷积网络（DCN）**是人工神经网络之星。

它具有卷积单元（或者池化层）和内核，每一种都用以不同目的。



卷积核事实上用来处理输入的数据，池化层是用来简化它们（大多数情况是用非线性方程，比如max），来减少不必要的特征。



他们通常被用来做图像识别，它们在图片的一小部分上运行（大约20x20像素）。

输入窗口一个像素一个像素的沿着图像滑动。然后数据流向卷积层，卷积层形成一个漏斗（压缩被识别的特征）。

从图像识别来讲：

- 第一层识别梯度
- 第二层识别线
- 第三层识别形状

- **以此类推，直到特定的物体那一级。**

**DFF通常被接在卷积层的末端方便未来的数据处理。**





# 18 去卷积网络（DN）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202331.webp" alt="图片" style="zoom:50%;" />

**去卷积网络（DN）**是将DCN颠倒过来。

DN能在获取猫的图片之后生成像（狗：0，蜥蜴：0，马：0，猫：1）一样的向量。

==DNC能在得到这个向量之后，能画出一只猫==。



# 19 深度卷积反转图像网络（DCIGN）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202353.webp" alt="图片" style="zoom:67%;" />

**深度卷积反转图像网络（DCIGN）**，长得像DCN和DN粘在一起，但也不完全是这样。



事实上，它是一个**自动编码器**，DCN和DN并不是作为两个分开的网络，而是承载网路输入和输出的间隔区。

大多数这种神经网络可以被用作图像处理，并且可以处理他们以前没有被训练过的图像。

由于其抽象化的水平很高，这些网络可以用于**将某个事物从一张图片中移除，重画**，或者像大名鼎鼎的CycleGAN一样将一匹马换成一个斑马。



# 20 生成对抗网络（GAN）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202427.webp" alt="图片" style="zoom:67%;" />

**生成对抗网络（GAN）**代表了有生成器和分辨器组成的双网络大家族。

它们一直在相互伤害——生成器试着生成一些数据，而分辨器接收样本数据后试着分辨出哪些是样本，哪些是生成的。

只要你能够保持两种神经网络训练之间的平衡，在不断的进化中，这种神经网络可以生成实际图像。



# 21 液体状态机（LSM）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202450.webp" alt="图片" style="zoom:67%;" />

**液体状态机（LSM）**是一种稀疏的，激活函数被阈值代替了的（并不是全部相连的）神经网络。

**只有达到阈值的时候，单元格从连续的样本和释放出来的输出中积累价值信息，并再次将内部的副本设为零。**



这种想法来自于人脑，这些神经网络被广泛的应用于计算机视觉，语音识别系统，但目前还没有重大突破。



# 22 极端学习机（ELM）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202519.webp" alt="图片" style="zoom:50%;" />

**极端学习机（ELM）**是通过产生稀疏的随机连接的隐藏层来减少FF网络背后的复杂性。

它们需要用到更少计算机的能量，实际的效率很大程度上取决于任务和数据。





# 23 回声状态网络（ESN）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202537.jpeg" alt="图片" style="zoom:67%;" />

**回声状态网络（ESN）**是重复网络的细分种类。

数据会经过输入端，如果被监测到进行了多次迭代（请允许重复网路的特征乱入一下），只有在隐藏层之间的权重会在此之后更新。



据我所知，除了多个理论基准之外，我不知道这种类型的有什么实际应用。





# 24 深度残差网络（DRN）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202601.webp" alt="图片" style="zoom:67%;" />

**深度残差网络（DRN）**是有些输入值的部分会传递到下一层。

这一特点可以让它可以做到很深的层级（达到300层），但事实上它们是一种没有明确延时的RNN。





# 25 Kohonen神经网络（KN）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202622.webp" alt="图片" style="zoom:67%;" />

**Kohonen神经网络（KN）**引入了“单元格距离”的特征。

大多数情况下用于分类，**这种网络试着调整它们的单元格使其对某种特定的输入作出最可能的反应。**

当一些单元格更新了， 离他们最近的单元格也会更新。



像SVM一样，这些网络总被认为不是“真正”的神经网络。



# 26 支持向量机（SVM）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202645.webp" alt="图片" style="zoom:67%;" />

**支持向量机（SVM）**用于二元分类工作，无论这个网络处理多少维度或输入，结果都会是“是”或“否”。

SVM不是所有情况下都被叫做神经网络。



# 27 神经图灵机（NTM）

<img src="https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img2/20210412202702.webp" alt="图片" style="zoom:67%;" />

神经网络像是黑箱——我们可以训练它们，得到结果，增强它们，但实际的决定路径大多数我们都是不可见的。



**神经图灵机（NTM）**就是在尝试解决这个问题——它是一个**提取出记忆单元之后的FF**。

一些作者也说它是一个抽象版的LSTM。



记忆是被内容编址的，**这个网络可以基于现状读取记忆，编写记忆，也代表了图灵完备神经网络。**