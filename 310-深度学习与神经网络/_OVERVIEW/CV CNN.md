[知乎专栏|CV 前篇 -- 卷积神经网络](https://zhuanlan.zhihu.com/p/94694563)

## 历史

**1987--第一个卷积网络TDNN：1987年Alexander Waibel 提出时间延迟网络（Time Delay Neural Network, TDNN）**

- 应用于**语音识别**问题
- 隐含层由2个一维卷积核组成，以提取==频率域上的平移不变特征==
- 表现超过了同等条件下的[隐马尔可夫模型](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/7932524)（Hidden Markov Model, HMM）（二十世纪80年代语音识别的主流算法）

**1988--第一个二维卷积网络SIANN：1988年，Wei Zhang提出了平移不变人工神经网络（SIANN）**

- 应用于检测医学影像

**1989--第一个应用于计算机视觉的卷积神经网络LeNet：1989年Yann LeCun提出LeNet最初版本**

- LeNet包含**两个卷积层**，**2个全连接层**，共计6万个学习参数，规模远超TDNN和SIANN，且在结构上与现代的卷积神经网络十分接近
- LeCun (1989)==对权重进行随机初始化后使用了随机梯度下降==（Stochastic Gradient Descent, SGD）进行学习，这一策略被其后的深度学习研究所保留
- LeCun (1989)在论述其网络结构时首次使用了“卷积”一词，“卷积神经网络”也因此得名。

**1993--LeNet应用：1993年由[贝尔实验室](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E8%B4%9D%E5%B0%94%E5%AE%9E%E9%AA%8C%E5%AE%A4/686816)（AT&T Bell Laboratories）完成代码开发并被部署于NCR（National Cash Register Coporation）的支票读取系统**

- 总体而言，由于数值计算能力有限、学习样本不足，加上==同一时期以[支持向量机](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/9683835)（Support Vector Machine, SVM）为代表的核学习（kernel learning）方法的兴起==，这一时期为各类图像处理问题设计的卷积神经网络停留在了研究阶段，应用端的推广较少

**1998--LeNet-5在手写数字识别问题中取得成功：1998年Yann LeCun及其合作者构建了更加完备的卷积神经网络LeNet-5并在手写数字的识别问题中取得成功**

- **命名：**源自其作者姓LeCun。

- **成就：**被成功用于ATM以对支票中的手写数字进行识别

- **网络基本架构为：**conv1 (6) -> pool1 -> conv2 (16) -> pool2 -> fc3 (120) -> fc4 (84) -> fc5 (10) -> softmax。括号中的数字代表通道数。

- - 5含义：卷积层两个，全连接层3个都是有可训练参数的，共计5层，所以5的意思是训练参数层数为5

- **参数量：**60k参数

**关键点：**

- 增加了**池化层对于输入特征进行筛选**，定义了现代卷积神经网络的基本结构
- ==卷积层-池化层被认为能够提取输入图像的平移不变特征==

![image-20210609162903846](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210609162904.png)

**2003--微软使用卷积神经网络开发了光学字符读取系统（Optical Character Recognition, OCR）**

**2012--AlexNet提出，得到GPU计算集群支持的复杂卷积神经网络多次成为[ImageNet](https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/ImageNet/17752829)大规模视觉识别竞赛的优胜算法**

- **命名：**源自其作者名Alex
- **成就：**ILSVRC 2012 冠军
- **网络基本架构为：**conv1 (96) relu1 norm1 -> pool1 -> conv2 (256) relu2 norm2 -> pool2 -> conv3 (384) relu3 -> conv4 (384) relu4 -> conv5 (256) relu5 -> pool5 -> fc6 (4096) relu6 dropout6-> fc7 (4096) relu7 dropout7 -> fc8 (1000) -> softmax。
- **参数量：**60M参数

**关键改进：**

- **使用了ReLU激活函数：**⭐

- - **使得网络训练更快：**梯度下降的训练时间更短，比tanh快几倍，tanh和sigmoid的导数包含指数且表达式复杂，函数的导数求解慢一些
  - **增加网络的非线性：**==卷积、pooling都是线性操作，非线性函数使得网络可以拟合非线性函数==
  - **防止梯度消失（弥散）：**当数值过大或者过小时，sigmoid，tanh导数接近0，会导致反向传播的梯度消失的问题，relu为非饱和函数不存在这个问题
  - **使网络具有稀疏性：**relu可以使一些神经元输出为0，因此可以**增加网络的稀疏性**

- **使用了随机失活(dropout)**⭐

- - 对于有n个节点的神经网络，可以看作2n个模型的集合
  - ==相当于机器学习中的ensemble==

- **数据扩充**

![img](https://pic1.zhimg.com/80/v2-3f5873d333bce4850185f6af38ae7dc0_720w.jpg)

**2014--VGGNet**

- **命名：**源自作者所处研究组名(Visual Geometry Group)。
- **成就：**ILSVRC 2014 亚军
- **VGG-16的基本架构为：**conv1^2 (64) -> pool1 -> conv2^2 (128) -> pool2 -> conv3^3 (256) -> pool3 -> conv4^3 (512) -> pool4 -> conv5^3 (512) -> pool5 -> fc6 (4096) -> fc7 (4096) -> fc8 (1000) -> softmax。 ^3代表重复3次。
- **参数量：**138M参数
- **VGG-19 VS VGG-16:** VGG-19结构类似于VGG-16，有略好于VGG-16的性能，但VGG-19需要消耗更大的资源，因此实际中VGG-16使用得更多。

**关键点：**

- **将大的卷积核分解成小卷积核：**3×3卷积，2×2 pooling⭐
- **结构简单：**重复堆叠相同的模块组合。卷积层不改变空间大小，每经过一次汇合层，空间大小减半。
- **参数量大**，16层，大部分参数集中在全连接层。
- **初始化和BN：**合适的网络初始化及batch normalization⭐

![img](https://pic2.zhimg.com/80/v2-b24a388cc7c26b338c7d2054f561cc8d_720w.jpg)

**2014--GoogLeNet**

- **GoogLeNet命名：**源自作者所处单位(Google)，L大写为了向LeNet致敬
- **Inception命名：**源自盗梦空间中的"we need to go deeper"
- **成就：**ILSVRC 2014 冠军
- **基本架构为：**conv1 (64) -> pool1 -> conv2^2 (64, 192) -> pool2 -> inc3 (256, 480) -> pool3 -> inc4^5 (512, 512, 512, 528, 832) -> pool4 -> inc5^2 (832, 1024) -> pool5 -> fc (1000)。
- **参数量：**5M参数

**关键点：**

- **多分支分别处理，级联结果：**同时用1×1、3×3、5×5卷积和3×3汇合，并保留所有结果。而不需要设计者来设计尺寸
- **减小参数量：**用了1×1卷积降维（Network In Network先使用）和 Global Average Pooling

![img](https://pic2.zhimg.com/80/v2-470936b0b1a627724bef6878e3e009cd_720w.jpg)



**Inception v3/v4**

**关键点：**

- ==将7×7和5×5卷积分解成若干等效3×3卷积==，并在网络中后部分把3×3卷积分解为1×3和3×1卷积。这使得在相似的网络参数下网络可以部署到42层。
- **Inception v3：**使用了批量归一层。Inception v3是GoogLeNet计算量的2.5倍，而错误率较后者下降了3%。
- **Inception v4：**在Inception模块基础上结合了residual模块(见下文)，进一步降低了0.4%的错误率。

![img](https://pic3.zhimg.com/80/v2-24afc5f942f527cc8abeb5dea368ea56_720w.jpg)

**2015--ResNet**

- **命名：**源自Residual block
- **成就：**ILSVRC 2015 冠军，在ImageNet上取得了超过人的准确率。
- **基本网络结构：**包含两个3×3卷积和一个Residual

**关键点：**

- 提出Residual block：==解决网络加深后训练难度增大的现象，有效缓解反向传播时由于深度过深导致的梯度消失现象，这使得网络加深之后性能不会变差==。
- 大量使用BN(batch normalization)
- 对于超过50层的网络，使用**瓶颈(bottleneck)结构**。

![img](https://pic1.zhimg.com/80/v2-870500a82942f2279cb336f7e521423c_720w.jpg)



------

## 卷积网络结构

### 输入层

卷积神经网络的输入层可以处理多维数据：

- 一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能==包含多个通道==；
- 二维卷积神经网络的输入层接收二维或三维数组；
- 三维卷积神经网络的输入层接收四维数组

在计算机视觉领域中，通常使用二维卷积神经网络来处理三维输入数据，即平面上的二维像素点和**RGB通道**。

卷积神经网络的输入特征通常会进行标准化处理，即将分布于 [0, 255] 的原始像素值归一化至 [0, 1] 区间。

- 输入特征的标准化有利于提升卷积神经网络的学习的效率和表现。



### 隐含层

卷积神经网络隐含层==标配：卷积-->池化-->全连接==，其中卷积层中的卷积核包含**权重系数**，而池化层不包含权重系数。

更为现代的算法中可能包括Inception模块、残差模块等



### 隐含层--卷积层

**为什么要用卷积层？**

- 图像输入通常维数很高，例如，1,000×1,000大小的彩色图像特征数量为300w，如果使用全连接层则需要巨大的参数量，导致**难以训练及过拟合**的风险。

- 相比全连接，卷积层有如下的特点以实现大大降低参数量：⭐

- - 局部连接：

  - - 工作：在通道方向保持全连接，在==空间方向只和一小部分输入神经元相连==
    - 在图像上为什么work？图像中关键性的图像特征、边缘信息**通常由临近的像素点决定，而距离很远的像素点之间影响较小**@常识

  - 共享参数：=> 减少参数的数量

  - - 如果**一组权重**在图像的某个区域可以提取有效表示，则在图像的另外区域中也能提出有效表示
    - ==每一个filter对应一种图像中的pattern==



**卷积核**

通常称为卷积核(convolution kernel)或者滤波器(filter)

- 作用：卷积层的功能是对于输入数据进行**特征的提取**，通常指**捕获图像的局部信息**

- - 不同层卷积层提取特征的改变：通过多层卷积层的**堆叠**，各层提取到的特征逐渐由边缘、纹理、方向等低层级特征过渡到文字、车轮、人脸等高级特征

- 卷积核：内部包括多个卷积核，每个卷积核包括**权重系数**和**偏差量**

- 工作原理：卷积核在工作时，在感受野内对输入特征做**矩阵元素乘法求和并叠加==偏差向量==**

- 感受野：某一层输出结果中每个神经元与前一层中多个相近的神经元相连，区域大小被称为感受野，**取决于卷积核大小**



**卷积层参数**

- 卷积层参数包括卷积核数量、大小、**步长**和**填充**，三者共同决定了卷积层输出特征图的尺寸

- - **卷积核大小：**卷积核大小可以指定为**小于输入图像尺寸的任意值**

  - - 卷积核越大，可提取的输入特征越复杂

    - **更通用的是小卷积核，有如下好处：**

    - - **效果与大卷积核可比：**通过堆叠小卷积核，可以取得与大卷积核相同的感受野

      - - ==3层3*3卷积层与一层7×7卷积感受野相同==

      - **更少参数量：**假设通道数量为D

      - - 3层3\*3卷积层参数量：3×(*D*×*D*×3×3)=27*D*^2
        - 1层7\*7卷积层参数量：*D*×*D*×7×7=49*D*^2

      - **更多非线性：**如上述例子中的3*3卷积核，经过了**三次非线性激活函数**

  - **卷积核channel：**每个卷积核的channel数量与输入内容的channel数相同

  - **卷积核数量：**卷积核个数与输出特征图的channel数量相同

- **一种特殊的卷积核：1\*1卷积**

- - 对每个空间位置的**D维向量**做相同的线性变换，作用如下：

  - - 增加非线性
    - 降维：在通道数方向上进行压缩



**padding类型**

随着卷积的进行，图像大小将缩小，图像边缘的信息将逐渐丢失。

填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。

常见的填充方法为**按0填充**和**重复边界值填充**（replication padding）

- **有效填充（valid padding）：**

- - 完全不使用填充
  - 使用有效填充的卷积被称为“窄卷积（narrow convolution）

- **相同填充/半填充（same/half padding）：**

- - 进行足够的填充来保持输出和输入的特征图尺寸相同
  - 使用相同填充的卷积被称为“等长卷积（equal-width convolution）

- **全填充（full padding）：**

- - 进行足够多的填充使得每个像素在每个方向上被访问的次数相同
  - 使用全填充的卷积被称为“宽卷积（wide convolution）



直观的理解：

- 有效填充（valid padding）：当filter全部在image里面的时候

![img](https://pic2.zhimg.com/80/v2-7c6162f5360dd86e0ff80ba6d17c0b15_720w.jpg)

- 相同填充/半填充（same/half padding）：当filter的中心(K)与image的边角重合时，开始做卷积运算

![img](https://pic2.zhimg.com/80/v2-9ee28472de8af6e55227463ba88ba2a1_720w.jpg)

- 全填充（full padding）：从filter和image**刚相交**开始做卷积

![img](https://pic4.zhimg.com/80/v2-7f69593c499d85c84fb7136c752a17a7_720w.jpg)



### **隐含层--池化层**

根据特征图上的局部统计信息进行下采样，在保留有用信息的同时减少特征图的大小。

池化层选取池化区域与卷积核扫描特征图步骤相同，**由池化大小、步长和填充控制**。

- 类型：

- - 最大池化：使用最多
  - 平均池化

- 参数：无！

- 作用：

- - **增加特征平移不变性。**汇合可以==提高网络对微小位移的容忍能力==。
  - **减小特征图大小。**汇合层对空间局部区域进行下采样，==使下一层需要的参数量和计算量减少，并降低过拟合风险==。



### 隐含层--**全连接层**

全连接层位于卷积神经网络隐含层的最后部分，并**只向其它全连接层传递信号**。

特征图在全连接层中会**失去空间拓扑结构，被展开为向量**。

==全连接层的参数在卷积网络中占比最高，通常会成为性能瓶颈==。

按照表征学习的观点：

- 卷积层和池化层能够对输入数据进行特征提取
- 全连接层的作用则是对提取的特征**进行非线性组合以得到输出**，全连接本身**不被期望具有特征提取的能力**，而是试图利用现有的高阶特征完成学习目标。



### 隐含层--**全局均值池化（替代全连接之一）**

在一些卷积神经网络中，全连接层的功能可能由全局均值池化（global average pooling）取代，全局均值池化会将特征图每个通道的所有值取平均

例子：

- 如果有7×7×256的特征图，全局均值池化将返回一个256的向量

- 池化层参数：

- - 每个元素都是7×7
  - 步长为7
  - 无填充



### 隐含层--**卷积层（替代全连接之二）**

在经典分类网络，比如LeNet、AlexNet中，在前面的卷积层提取特征之后都串联全连接层来做分类。

但是近些年来，越来越多的网络，比如SSD，FasterRCNN的RPN，MTCNN中的PNet，都**使用卷积层来代替全连接，也一样可以做到目标分类的效果**

==由于全连接层和卷积层都是做点乘，这两种操作可以互相等效==。

只需要设定好下面几个量：

- 滤波器个数 = 原全连接层输出神经元个数

- - 例如分类的类别数量，logits维度

- 感受野 = 输入空间大小

- - w，h，c都相同

- 没有padding

- stride为1

**why？**

1. 更灵活，不需要限定输入图像的分辨率；输出图中每一个像素点都对应这输入图片的一个区域。

2. 更高效，**只需要做一次前向计算**。例如一张图片分成10*10的网格，现在要识别这100个格子中数字的位置

3. 1. 全连接网络：至少需要做100次前向
   2. 全卷积网络：输入和输出都是二维的图像，具有相对应的空间结构，可以将输出看作一张heat-map，用**热度来代表待检测的原图位置出现目标的概率**，而只需要做一次前向就可以得到所有位置的分类概率。



例子：

以输入10 x 10 x 3的特征图，输出10 x 10 x 1的特征图为例子：

- 全连接：将输入reshape成300个输入神经元，每个输出神经元都是这300个输入神经元的线性组合，输出再reshape成10 x 10 x 1的形状
- 卷积层：用100个10 x 10 x 3的滤波器，对输入进行卷积，得到100个一维的输出，然后reshape成10 x 10 x 1的形状，每个输出也是由输入特征图的300个像素点线性组合而来的。



### 输出层

- 分类：使用==逻辑函数或者softmax输出分类标签==
- 物体识别：输出物体中心坐标、大小、分类
- 图像语义分割：输出每个像素的分类结果

------

## 卷积层特性

### 平移不变性

在欧几里得几何中，平移是一种几何变换，表示把一幅图像或一个空间中的每一个点在相同方向移动相同距离。

比如对图像分类任务来说，图像中的目标不管被移动到图片的哪个位置，得到的结果（标签）应该是相同的，这就是卷积神经网络中的平移不变性。



对应的是平移同变性，**意味着系统在不同位置的工作原理相同，但它的响应随着目标位置的变化而变化**。

比如，实例分割任务，就需要平移同变性，目标如果被平移了，那么输出的实例掩码也应该相应地变化。

也就是说，同一个像素==在不同的相对位置，具有不同的语义，对应着不同的响应，这说的也是平移同变性==。