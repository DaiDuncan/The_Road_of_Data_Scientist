- [Load less data](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#load-less-data)
- [Use efficient datatypes](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#use-efficient-datatypes)
- [Use chunking](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#use-chunking)
- [Use other libraries](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#use-other-libraries)

---

# 使用大型数据集

不建议用pandas使用大型数据集，因为中间过程需要保存副本。

- PostgreSQL

```python
In [1]: import pandas as pd
In [2]: import numpy as np
```



## 加载小型数据集

```python
               id_0    name_0       x_0       y_0  id_1   name_1       x_1  ...  name_8       x_8       y_8  id_9   name_9       x_9       y_9
timestamp                                                                         ...
2000-01-01 00:00:00  1015   Michael -0.399453  0.095427   994    Frank -0.176842  ...     Dan -0.315310  0.713892  1025   Victor -0.135779  0.346801
2000-01-01 00:01:00   969  Patricia  0.650773 -0.874275  1003    Laura  0.459153  ...  Ursula  0.913244 -0.630308  1047    Wendy -0.886285  0.035852
2000-01-01 00:02:00  1016    Victor -0.721465 -0.584710  1046  Michael  0.524994  ...     Ray -0.656593  0.692568  1064   Yvonne  0.070426  0.432047
2000-01-01 00:03:00   939     Alice -0.746004 -0.908008   996   Ingrid -0.414523  ...   Jerry -0.958994  0.608210   978    Wendy  0.855949 -0.648988
2000-01-01 00:04:00  1017       Dan  0.919451 -0.803504  1048    Jerry -0.569235  ...   Frank -0.577022 -0.409088   994      Bob -0.270132  0.335176
...                   ...       ...       ...       ...   ...      ...       ...  ...     ...       ...       ...   ...      ...       ...       ...
2000-12-30 23:56:00   999       Tim  0.162578  0.512817   973    Kevin -0.403352  ...     Tim -0.380415  0.008097  1041  Charlie  0.191477 -0.599519
2000-12-30 23:57:00   970     Laura -0.433586 -0.600289   958   Oliver -0.966577  ...   Zelda  0.971274  0.402032  1038   Ursula  0.574016 -0.930992
2000-12-30 23:58:00  1065     Edith  0.232211 -0.454540   971      Tim  0.158484  ...   Alice -0.222079 -0.919274  1022      Dan  0.031345 -0.657755
2000-12-30 23:59:00  1019    Ingrid  0.322208 -0.615974   981   Hannah  0.607517  ...   Sarah -0.424440 -0.117274   990   George -0.375530  0.563312
2000-12-31 00:00:00   937    Ursula -0.906523  0.943178  1018    Alice -0.564513  ...   Jerry  0.236837  0.807650   985   Oliver  0.777642  0.783392

[525601 rows x 40 columns]


### 加载所有数据
In [3]: columns = ['id_0', 'name_0', 'x_0', 'y_0']

In [4]: pd.read_parquet("timeseries_wide.parquet")[columns]
Out[4]: 
                     id_0    name_0       x_0       y_0
timestamp                                              
2000-01-01 00:00:00  1015   Michael -0.399453  0.095427
2000-01-01 00:01:00   969  Patricia  0.650773 -0.874275
2000-01-01 00:02:00  1016    Victor -0.721465 -0.584710
2000-01-01 00:03:00   939     Alice -0.746004 -0.908008
2000-01-01 00:04:00  1017       Dan  0.919451 -0.803504
...                   ...       ...       ...       ...
2000-12-30 23:56:00   999       Tim  0.162578  0.512817
2000-12-30 23:57:00   970     Laura -0.433586 -0.600289
2000-12-30 23:58:00  1065     Edith  0.232211 -0.454540
2000-12-30 23:59:00  1019    Ingrid  0.322208 -0.615974
2000-12-31 00:00:00   937    Ursula -0.906523  0.943178

[525601 rows x 4 columns]


### 加载需要的目标列：内存使用降为1/10
In [5]: pd.read_parquet("timeseries_wide.parquet", columns=columns)
Out[5]: 
                     id_0    name_0       x_0       y_0
timestamp                                              
2000-01-01 00:00:00  1015   Michael -0.399453  0.095427
2000-01-01 00:01:00   969  Patricia  0.650773 -0.874275
2000-01-01 00:02:00  1016    Victor -0.721465 -0.584710
2000-01-01 00:03:00   939     Alice -0.746004 -0.908008
2000-01-01 00:04:00  1017       Dan  0.919451 -0.803504
...                   ...       ...       ...       ...
2000-12-30 23:56:00   999       Tim  0.162578  0.512817
2000-12-30 23:57:00   970     Laura -0.433586 -0.600289
2000-12-30 23:58:00  1065     Edith  0.232211 -0.454540
2000-12-30 23:59:00  1019    Ingrid  0.322208 -0.615974
2000-12-31 00:00:00   937    Ursula -0.906523  0.943178

[525601 rows x 4 columns]
```



使用pandas.read_csv()，可以指定usecols限制读入内存的列



## 使用恰当的数据类型

默认的pandas数据类型不是最有效的内存

- 对于唯一值相对较少（通常称为“低基数”数据）的文本数据列，尤其如此

```python
In [6]: ts = pd.read_parquet("timeseries.parquet")

In [7]: ts
Out[7]: 
                       id      name         x         y
timestamp                                              
2000-01-01 00:00:00  1029   Michael  0.278837  0.247932
2000-01-01 00:00:30  1010  Patricia  0.077144  0.490260
2000-01-01 00:01:00  1001    Victor  0.214525  0.258635
2000-01-01 00:01:30  1018     Alice -0.646866  0.822104
2000-01-01 00:02:00   991       Dan  0.902389  0.466665
...                   ...       ...       ...       ...
2000-12-30 23:58:00   992     Sarah  0.721155  0.944118
2000-12-30 23:58:30  1007    Ursula  0.409277  0.133227
2000-12-30 23:59:00  1009    Hannah -0.452802  0.184318
2000-12-30 23:59:30   978     Kevin -0.904728 -0.179146
2000-12-31 00:00:00   973    Ingrid -0.370763 -0.794667

[1051201 rows x 4 columns]


### 分析数据类型和内存使用情况
In [8]: ts.dtypes
Out[8]: 
id        int64
name     object
x       float64
y       float64
dtype: object
    
    
In [9]: ts.memory_usage(deep=True)  # memory usage in bytes
Out[9]: 
Index     8409608
id        8409608
name     65537768
x         8409608
y         8409608
dtype: int64
    
    
'''分析
name列占用的内存比其他任何内存都要多。它只有几个唯一值，因此是转换为的理想选择 Categorical
'''
In [10]: ts2 = ts.copy()

In [11]: ts2['name'] = ts2['name'].astype('category')

In [12]: ts2.memory_usage(deep=True)
Out[12]: 
Index    8409608
id       8409608
name     1054102
x        8409608
y        8409608
dtype: int64
    
    
### 更进一步，并使用将数字列缩减为最小的类型pandas.to_numeric()
In [13]: ts2['id'] = pd.to_numeric(ts2['id'], downcast='unsigned')

In [14]: ts2[['x', 'y']] = ts2[['x', 'y']].apply(pd.to_numeric, downcast='float')

In [15]: ts2.dtypes
Out[15]: 
id        uint16
name    category
x        float32
y        float32
dtype: object
    
    
In [16]: ts2.memory_usage(deep=True)
Out[16]: 
Index    8409608
id       2102402
name     1054102
x        4204804
y        4204804
dtype: int64
    
    
### 内存占用减少为1/5
In [17]: reduction = (ts2.memory_usage(deep=True).sum()
   ....:              / ts.memory_usage(deep=True).sum())
   ....: 

In [18]: print(f"{reduction:0.2f}")
0.20
```





## 使用chunk/分块

将一个大问题（如“将许多CSV文件，转换为parquet”）分解为一堆小问题 => 一个CSV转化为一个Parquet文件



### parquet文件目录

```python
### 每一个文件可能包含某一段时间@老纪：分析泵的信号
data
└── timeseries
    ├── ts-00.parquet
    ├── ts-01.parquet
    ├── ts-02.parquet
    ├── ts-03.parquet
    ├── ts-04.parquet
    ├── ts-05.parquet
    ├── ts-06.parquet
    ├── ts-07.parquet
    ├── ts-08.parquet
    ├── ts-09.parquet
    ├── ts-10.parquet
    └── ts-11.parquet
    
    
    
In [19]: %%time
   ....: files = pathlib.Path("data/timeseries/").glob("ts*.parquet") #re表示文件路径
   ....: counts = pd.Series(dtype=int)
   ....: for path in files:
   ....:     df = pd.read_parquet(path)
   ....:     counts = counts.add(df['name'].value_counts(), fill_value=0)
   ....: counts.astype(int)
   ....: 
CPU times: user 842 ms, sys: 80.9 ms, total: 923 ms
Wall time: 686 ms
Out[19]: 
Alice      229802
Bob        229211
Charlie    229303
Dan        230621
Edith      230349
            ...  
Victor     230502
Wendy      230038
Xavier     229553
Yvonne     228766
Zelda      229909
Length: 26, dtype: int64
```

某些阅读器（例如pandas.read_csv()）提供参数来控制chunksize读取单个文件时的参数 





## 其他库

Pandas只是一个提供DataFrame API的库。由于其受欢迎程度，熊猫的API已成为其他库实施的标准

[DASK](https://dask.org/)，==并行计算库==，具有[dask.dataframe](https://docs.dask.org/en/latest/dataframe.html)，类似pandas的API

- 多线程
- 多进程
- 机器集群

```python
In [20]: import dask.dataframe as dd

In [21]: ddf = dd.read_parquet("data/timeseries/ts*.parquet", engine="pyarrow")

In [22]: ddf
Out[22]: 
Dask DataFrame Structure:
                   id    name        x        y
npartitions=12                                 
                int64  object  float64  float64
                  ...     ...      ...      ...
...               ...     ...      ...      ...
                  ...     ...      ...      ...
                  ...     ...      ...      ...
Dask Name: read-parquet, 12 tasks
```



### ddf对象

- 属性
  - .columns
  - .dtypes
- 方法
  - .groupby
  - .sum
- 新的属性
  - .npartitions
  - .divisions



Dask并行计算的原理：

- partitions
- divisions

A Dask DataFrame is made up of many Pandas DataFrames.

主要区别：dask.dataframeAPI是lazy。注意到这些值实际上并没有打印出来。只是列名和dtypes。这是因为Dask==尚未真正读取数据==。进行操作可以建立任务图，而不是立即执行

```python
In [23]: ddf.columns
Out[23]: Index(['id', 'name', 'x', 'y'], dtype='object')

In [24]: ddf.dtypes
Out[24]: 
id        int64
name     object
x       float64
y       float64
dtype: object

In [25]: ddf.npartitions
Out[25]: 12
    

### 只打印列名
In [26]: ddf
Out[26]: 
Dask DataFrame Structure:
                   id    name        x        y
npartitions=12                                 
                int64  object  float64  float64
                  ...     ...      ...      ...
...               ...     ...      ...      ...
                  ...     ...      ...      ...
                  ...     ...      ...      ...
Dask Name: read-parquet, 12 tasks

In [27]: ddf['name']
Out[27]: 
Dask Series Structure:
npartitions=12
    object
       ...
     ...  
       ...
       ...
Name: name, dtype: object
Dask Name: getitem, 24 tasks

In [28]: ddf['name'].value_counts()
Out[28]: 
Dask Series Structure:
npartitions=1
    int64
      ...
Name: name, dtype: int64
Dask Name: value-counts-agg, 39 tasks
    
    
### 要获得实际结果：调用.compute()
In [29]: %time ddf['name'].value_counts().compute()
CPU times: user 926 ms, sys: 118 ms, total: 1.04 s
Wall time: 729 ms
Out[29]: 
Laura      230906
Ingrid     230838
Kevin      230698
Dan        230621
Frank      230595
            ...  
Ray        229603
Xavier     229553
Charlie    229303
Bob        229211
Yvonne     228766
Name: name, Length: 26, dtype: int64
```



### 多个机器集群

默认情况下，`dask.dataframe`操作使用线程池并行执行操作。我们还可以连接到群集，以将工作分配到许多计算机上。在这种情况下，我们将连接到由单台计算机上的多个进程组成的本地“集群”。

```python
>>> from dask.distributed import Client, LocalCluster

>>> cluster = LocalCluster()
>>> client = Client(cluster)
>>> client
<Client: 'tcp://127.0.0.1:53349' processes=4 threads=8, memory=17.18 GB>
```

一旦client被创建，所有的DASK的计算将采取集群的地方（这就是在这种情况下处理）



### Dask实现了Pandas API中最常用的部分

- groupby

```python
In [30]: %time ddf.groupby('name')[['x', 'y']].mean().compute().head()
CPU times: user 1.7 s, sys: 226 ms, total: 1.92 s
Wall time: 1.01 s
Out[30]: 
                x         y
name                       
Alice    0.000086 -0.001170
Bob     -0.000843 -0.000799
Charlie  0.000564 -0.000038
Dan      0.000584  0.000818
Edith   -0.000116 -0.000044



### 手动分隔
In [31]: N = 12

In [32]: starts = [f'20{i:>02d}-01-01' for i in range(N)]
In [33]: ends = [f'20{i:>02d}-12-13' for i in range(N)]
In [34]: divisions = tuple(pd.to_datetime(starts)) + (pd.Timestamp(ends[-1]),) # 手动分隔

In [35]: ddf.divisions = divisions

In [36]: ddf
Out[36]: 
Dask DataFrame Structure:
                   id    name        x        y
npartitions=12                                 
2000-01-01      int64  object  float64  float64
2001-01-01        ...     ...      ...      ...
...               ...     ...      ...      ...
2011-01-01        ...     ...      ...      ...
2011-12-13        ...     ...      ...      ...
Dask Name: read-parquet, 12 tasks
    
    
# 快速索引
# Dask知道只需在2002年的第3个分区中选择值：不需要查看任何其他数据
In [37]: ddf.loc['2002-01-01 12:01':'2002-01-01 12:05'].compute()
Out[37]: 
                       id    name         x         y
timestamp                                            
2002-01-01 12:01:00   983   Laura  0.243985 -0.079392
2002-01-01 12:02:00  1001   Laura -0.523119 -0.226026
2002-01-01 12:03:00  1059  Oliver  0.612886  0.405680
2002-01-01 12:04:00   993   Kevin  0.451977  0.332947
2002-01-01 12:05:00  1014  Yvonne -0.948681  0.361748
```



```python
### 示例：重采样
In [38]: ddf[['x', 'y']].resample("1D").mean().cumsum().compute().plot()
Out[38]: <AxesSubplot:xlabel='timestamp'>
```

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20201222222038.png" alt="image-20201222222037752" style="zoom:80%;" />

[Link: Dask官网](https://docs.dask.org/en/latest/setup.html)

[Link: Dask示例](https://examples.dask.org/)