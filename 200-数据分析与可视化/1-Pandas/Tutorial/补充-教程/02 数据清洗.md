源数据：

- The data set contains some empty cells ("Date" in row 22, and "Calories" in row 18 and 28).
- The data set contains wrong format ("Date" in row 26).
- The data set contains wrong data ("Duration" in row 7).
- The data set contains duplicates (row 11 and 12).

```python
     Duration          Date  Pulse  Maxpulse  Calories
  0         60  '2020/12/01'    110       130     409.1
  1         60  '2020/12/02'    117       145     479.0
  2         60  '2020/12/03'    103       135     340.0
  3         45  '2020/12/04'    109       175     282.4
  4         45  '2020/12/05'    117       148     406.0
  5         60  '2020/12/06'    102       127     300.0
  6         60  '2020/12/07'    110       136     374.0
  7        450  '2020/12/08'    104       134     253.3
  8         30  '2020/12/09'    109       133     195.1
  9         60  '2020/12/10'     98       124     269.0
  10        60  '2020/12/11'    103       147     329.3
  11        60  '2020/12/12'    100       120     250.7
  12        60  '2020/12/12'    100       120     250.7
  13        60  '2020/12/13'    106       128     345.3
  14        60  '2020/12/14'    104       132     379.3
  15        60  '2020/12/15'     98       123     275.0
  16        60  '2020/12/16'     98       120     215.2
  17        60  '2020/12/17'    100       120     300.0
  18        45  '2020/12/18'     90       112       NaN
  19        60  '2020/12/19'    103       123     323.0
  20        45  '2020/12/20'     97       125     243.0
  21        60  '2020/12/21'    108       131     364.2
  22        45           NaN    100       119     282.0
  23        60  '2020/12/23'    130       101     300.0
  24        45  '2020/12/24'    105       132     246.0
  25        60  '2020/12/25'    102       126     334.5
  26        60    2020/12/26    100       120     250.0
  27        60  '2020/12/27'     92       118     241.0
  28        60  '2020/12/28'    103       132       NaN
  29        60  '2020/12/29'    100       132     280.0
  30        60  '2020/12/30'    102       129     380.3
  31        60  '2020/12/31'     92       115     243.0
```

# 缺失值

> 数据第18、28条

## 1 替换缺失值

```python
import pandas as pd

df = pd.read_csv('data.csv')
df.fillna(130, inplace = True)


#==# 针对特定column
df = pd.read_csv('data.csv')
df["Calories"].fillna(130, inplace = True)
```



用统计值替代： `mean()` `median()` and `mode()` 

```python
import pandas as pd

df = pd.read_csv('data.csv')
x = df["Calories"].mean()	#x = df["Calories"].median(), x = df["Calories"].mode()[0]
df["Calories"].fillna(x, inplace = True)
```



## 2 删除缺失行

> 当数据量很大：删除不影响时

```python
import pandas as pd

df = pd.read_csv('data.csv')
# 返回一个新的dataframe
new_df = df.dropna()	
print(new_df.to_string())

# 原地变化
df.dropna(inplace = True)
print(df.to_string())
```





# 格式|错误

> 数据第22、26条

## 1 首先统一格式

```python
import pandas as pd

df = pd.read_csv('data.csv')
df['Date'] = pd.to_datetime(df['Date'])
print(df.to_string())
```



结果：

```python
…………
  22        45           NaT    100       119     282.0
  23        60  '2020/12/23'    130       101     300.0
  24        45  '2020/12/24'    105       132     246.0
  25        60  '2020/12/25'    102       126     334.5
  26        60  '2020/12/26'    100       120     250.0
  27        60  '2020/12/27'     92       118     241.0
  28        60  '2020/12/28'    103       132       NaN
  29        60  '2020/12/29'    100       132     280.0
  30        60  '2020/12/30'    102       129     380.3
  31        60  '2020/12/31'     92       115     243.0
```



## 2 删除错误行|NaT

```python
df.dropna(subset=['Date'], inplace = True)
```



# 数据|错误

> - 人为|统计错误：比如199写成了1.99
> - 不合理|客观错误：workout持续450min？ 

## 1 合情推理 => 替换错误值

```python
#==# 针对小型数据集
df.loc[7, 'Duration'] = 45

#==# 针对大型数据集：设立规则
for x in df.index:
    if df.loc[x, "Duration"] > 120:
        df.loc[x, "Duration"] = 120
```



## 2 删除错误行

```python
for x in df.index:
  	if df.loc[x, "Duration"] > 120:
         df.drop(x, inplace = True)
```



# 数据|重复

> 数据第11、12条

```python
print(df.duplicated())
df.drop_duplicates(inplace = True)
```

