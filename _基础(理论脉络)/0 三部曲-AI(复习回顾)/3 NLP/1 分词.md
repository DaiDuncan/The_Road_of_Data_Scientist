[分词 – Tokenization 2019.08.08](https://easyai.tech/ai-definition/tokenization/)



- [什么是分词？](https://easyai.tech/ai-definition/tokenization/#what)
- [为什么要分词？](https://easyai.tech/ai-definition/tokenization/#why)
- [中英文分词的3个典型区别](https://easyai.tech/ai-definition/tokenization/#qubie)
- [中文分词的3大难点](https://easyai.tech/ai-definition/tokenization/#nandian)
- [3种典型的分词方法](https://easyai.tech/ai-definition/tokenization/#3ways)
- [中文分词工具](https://easyai.tech/ai-definition/tokenization/#tools-cn)
- [英文分词工具](https://easyai.tech/ai-definition/tokenization/#toos-en)
- [总结](https://easyai.tech/ai-definition/tokenization/#end)
- [百度百科+维基百科](https://easyai.tech/ai-definition/tokenization/#wiki)

> 分词是 [NLP](https://easyai.tech/ai-definition/nlp/) 的基础任务，将句子，段落**分解为字词单位**，方便后续的处理的分析。
>
> 本文将介绍分词的原因，中英文分词的3个区别，中文分词的3大难点，分词的3种典型方法。
>
> 最后将介绍中文分词和英文分词常用的工具。

 

## 什么是分词？

分词是 [自然语言理解 – NLP](https://easyai.tech/ai-definition/nlp/) 的重要步骤。

分词就是将**句子、段落、文章**这种长文本，**分解为以字词**为单位的数据结构，方便后续的处理分析工作。

![什么是分词？](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-08-08-fenci-shili.png)

 

## 为什么要分词？

**1.将复杂问题转化为数学问题**

在 [机器学习的文章](https://easyai.tech/ai-definition/machine-learning/) 中讲过，==机器学习之所以看上去可以解决很多复杂的问题，是因为它把这些问题都转化为了数学问题。==

而 NLP 也是相同的思路，文本都是一些「**非结构化数据**」，我们需要先将这些数据转化为「**结构化数据**」，结构化数据就可以转化为数学问题了，而分词就是转化的第一步。

![为什么要分词？](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-08-08-shuxue.png)



**2.词是一个比较合适的粒度**

词是表达完整含义的最小单位。

**字的粒度太小，无法表达完整含义**，比如”鼠“可以是”老鼠“，也可以是”鼠标“。

而**句子的粒度太大，承载的信息量多，很难复用**。



比如”传统方法要分词，一个重要原因是传统方法对远距离依赖的建模能力较弱。

![词是合适的粒度](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-08-08-lidu.png)



**3. 深度学习时代，部分任务中也可以「分字」**

深度学习时代，随着**数据量和算力的爆炸式增长**，很多传统的方法被颠覆。

分词一直是 NLP 的基础，**但是现在也不一定了**，感兴趣的可以看看这篇论文：《[Is Word Segmentation Necessary for Deep Learning of Chinese Representations?](https://arxiv.org/pdf/1905.05526.pdf)》。

![Is Word Segmentation Necessary for Deep Learning of Chinese Representations?](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-08-08-paper.png)



不过在一些特定任务中，分词还是必要的。

如：关键词提取、命名实体识别等。

 

## 中英文分词的3个典型区别

![中英文分词的3个典型区别](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-08-08-qubie.png)

**区别1：分词方式不同，中文更难**

英文有天然的空格作为分隔符，但是中文没有。

所以如何切分是一个难点，再加上**中文里一词多意的情况非常多**，导致很容易出现歧义。

下文中难点部分会详细说明。

 

**区别2：英文单词有多种形态**

英文单词存在丰富的变形变换。

为了应对这些复杂的变换，英文NLP相比中文存在一些独特的处理步骤，我们称为词形还原（Lemmatization）和词干提取（[Stemming](https://easyai.tech/ai-definition/stemming-lemmatisation/)）。

中文则不需要

- 词性还原：does，done，doing，did 需要通过词性还原恢复成 do。

- 词干提取：cities，children，teeth 这些词，需要转换为 city，child，tooth”这些基本形态

 

**区别3：中文分词需要考虑粒度问题**

例如「中国科学技术大学」就有很多种分法：

- 中国科学技术大学
- 中国 \ 科学技术 \ 大学
- 中国 \ 科学 \ 技术 \ 大学

粒度越大，表达的意思就越准确，但是也会导致**召回比较少**。

所以中文需要不同的场景和要求选择不同的粒度。

这个在英文中是没有的。

 

## 中文分词的3大难点

![中文分词的3大难点](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-08-08-nandian.png)

**难点 1：没有统一的标准**

目前中文分词没有统一的标准，也没有公认的规范。

不同的公司和组织各有各的方法和规则。

 

**难点 2：歧义词如何切分**

例如「兵乓球拍卖完了」就有2种**分词方式表达了2种不同的含义**：

- 乒乓球 \ 拍卖 \ 完了
- 乒乓 \ 球拍 \ 卖 \ 完了

 

**难点 3：新词的识别**

信息爆炸的时代，三天两头就会冒出来一堆新词，如何快速的识别出这些新词是一大难点。

比如当年「蓝瘦香菇」大火，就需要快速识别。

 

## 3种典型的分词方法

![3种典型的分词方法](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-08-08-3ways.png)

分词的方法大致分为 3 类：

1. 基于词典匹配
2. 基于统计
3. 基于深度学习

1 **基于词典匹配的分词方式**

优点：速度快、成本低

缺点：**适应性不强，不同领域效果差异大**

基本思想是基于词典匹配，将待分词的中文文本根据一定规则切分和调整，然后跟词典中的词语进行匹配，匹配成功则按照词典的词分词，匹配失败通过调整或者重新选择，如此反复循环即可。

代表方法有**基于正向最大匹配**和**基于逆向最大匹配及双向匹配法**。

 

2 **基于统计的分词方法**

优点：适应性较强

缺点：成本较高，速度较慢

这类目前常用的是算法是**HMM、CRF、[SVM](https://easyai.tech/ai-definition/svm/)、深度学习**等算法，比如stanford、Hanlp分词工具是基于CRF算法。

以CRF为例，基本思路是对汉字进行**标注训练**，不仅考虑了**词语出现的频率，还考虑上下文**，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。

 

3 **基于深度学习**

优点：准确率高、适应性强

缺点：成本高，速度慢

例如有人员尝试使用双向[LSTM](https://easyai.tech/ai-definition/lstm/)+CRF实现分词器，其本质上是**序列标注**，所以有通用性，**命名实体识别等都可以使用该模型**，据报道其分词器字符准确率可高达97.5%。

**常见的分词器都是使用==机器学习算法和词典相结合==，**

- 一方面能够提高分词准确率，
- 另一方面能够改善领域适应性。

 

## 中文分词工具

下面排名根据 GitHub 上的 star 数排名：

1. [Hanlp](https://github.com/hankcs/HanLP)
2. [Stanford 分词](https://github.com/stanfordnlp/CoreNLP)
3. [ansj 分词器](https://github.com/NLPchina/ansj_seg)
4. [哈工大 LTP](https://github.com/HIT-SCIR/ltp)
5. [KCWS分词器](https://github.com/koth/kcws)
6. [jieba](https://github.com/yanyiwu/cppjieba) ⭐
7. [IK](https://github.com/wks/ik-analyzer)
8. [清华大学THULAC](https://github.com/thunlp/THULAC)
9. [ICTCLAS](https://github.com/thunlp/THULAC)

 

## 英文分词工具

1. [Keras](https://github.com/keras-team/keras)
2. [Spacy](https://github.com/explosion/spaCy)
3. [Gensim](https://github.com/RaRe-Technologies/gensim)
4. [NLTK](https://github.com/nltk/nltk) ⭐

 

## 总结

分词就是将句子、段落、文章这种长文本，分解为以字词为单位的数据结构，方便后续的处理分析工作。

**分词的原因：**

1. 将复杂问题转化为数学问题
2. 词是一个比较合适的粒度
3. 深度学习时代，部分任务中也可以「分字」

**中英文分词的3个典型区别：**

1. 分词方式不同，中文更难
2. 英文单词有多种形态，需要词性还原和词干提取
3. 中文分词需要考虑粒度问题

**中文分词的3大难点**

1. 没有统一的标准
2. 歧义词如何切分
3. 新词的识别

**3个典型的分词方式：**

1. 基于词典匹配
2. 基于统计
3. 基于深度学习

 

## 百度百科+维基百科

百度百科版本

中文分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。我们知道，在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符，虽然英文也同样存在短语的划分问题，不过在词这一层上，中文比之英文要复杂得多、困难得多。

[查看详情](https://baike.baidu.com/item/中文分词/371496?fr=aladdin)

 

维基百科版本

*分词*是对一串输入字符的部分进行划分和可能分类的过程。然后将得到的标记传递给某种其他形式的处理。该过程可以被认为是解析输入的子任务。

[查看详情](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)