[ç»å…¸è®²è§£è§†é¢‘|è´ªå¿ƒç§‘æŠ€](https://www.bilibili.com/video/BV1ik4y167kr)

èŠ±äº†ä¸€ä¸ªå°æ—¶æ¢³ç†ä»£ç 



# å¯¹è±¡|å•è¯ => POS tag

## 0 åŸå§‹æ•°æ®

- `"data.txt"` => raw_dataè¯»å–è¡Œï¼šæ ¼å¼åŒ– => å°å†™ï¼Œå˜æˆå•è¯
- `full_corpus`ï¼šæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå•è¯åˆ—è¡¨
  - 21928 SÃ¤tze
  - åŒ…å«æ ‡ç‚¹ç¬¦å·
- `full_tagged_corpus`ï¼šå¯¹åº”full_corpusçš„æ ‡ç­¾åˆ—è¡¨

```python
with open("data.txt", encoding="utf-8") as corpus_file:
    raw_data = [line.lower().strip().split() for line in corpus_file] #è¯»å–è¡Œï¼šæ ¼å¼åŒ– => å°å†™ï¼Œå˜æˆå•è¯
full_corpus = raw_data[::2] #æå–ä¸€è¡Œï¼šæ˜¯ä¸€ä¸ªå•è¯åˆ—è¡¨
full_tagged_corpus = raw_data[1::2]
del raw_data	#å‡å°‘å†…å­˜ç©ºé—´çš„æ¶ˆè€—

assert len(full_corpus) == len(full_tagged_corpus)	
print(f"{len(full_corpus)} SÃ¤tze geladen")	#21928 SÃ¤tze geladen
```



## 1 æ•°æ®åˆ’åˆ†

+æ“ä½œæŠ€å·§|æ‰“ä¹±æ’åº

- `test_corpus` => `test_tagged_corpus` å‰2000è¡Œ
- `corpus` => `tagged_corpus`

```python
import random
random.seed(1232)
shuffled_indices = random.sample(range(len(full_corpus)), len(full_corpus))
corpus = [full_corpus[i] for i in shuffled_indices] #æ‰“ä¹±é¡ºåº
tagged_corpus = [full_tagged_corpus[i] for i in shuffled_indices]

test_corpus, corpus = corpus[:2000], corpus[2000:]
test_tagged_corpus, tagged_corpus = tagged_corpus[:2000], tagged_corpus[2000:]


print(corpus[0])
print(list(zip(corpus[0], tagged_corpus[0])))
'''
['discount', 'rate', ':', '7', '%', '.', 'the', 'charge', 'on', 'loans', 'to', 'depository', 'institutions', 'by', 'the', 'new', 'york', 'federal', 'reserve', 'bank', '.']
[('discount', 'nn'), ('rate', 'nn'), (':', ':'), ('7', 'cd'), ('%', 'nn'), ('.', '.'), ('the', 'dt'), ('charge', 'nn'), ('on', 'in'), ('loans', 'nns'), ('to', 'to'), ('depository', 'nn'), ('institutions', 'nns'), ('by', 'in'), ('the', 'dt'), ('new', 'nnp'), ('york', 'nnp'), ('federal', 'nnp'), ('reserve', 'nnp'), ('bank', 'nnp'), ('.', '.')]
'''
```





## 2 ç»Ÿè®¡|å•è¯çš„ç‰¹å¾ä¹‹ä¸€ğŸ’¡

- `vocabulary=Counter()`
  - countsï¼šç»Ÿè®¡æ•°é‡åˆ—è¡¨(æ’åº)

```python
from collections import Counter #ç»Ÿè®¡å€¼ä½œä¸ºå•è¯çš„è¡¨å¾

vocabulary = Counter()
for sentence in corpus:
    vocabulary.update(sentence)

tag_vocabulary = Counter()
for sentence in tagged_corpus:
    tag_vocabulary.update(sentence)

print(f"Im Text kommen {len(vocabulary)} unterschiedliche WÃ¶rter und {len(tag_vocabulary)} unterschiedliche tags vor.")
print("Die 10 hÃ¤ufigsten WÃ¶rter sind:")
print(vocabulary.most_common(10)) #Counter()çš„å±æ€§
'''
[(',', 53567), ('the', 53148), ('.', 42888), ('of', 25225), ('to', 24567), ('a', 22034), ('in', 18669), ('and', 18116), ("'s", 10269), ('that', 9383)]
'''
```



```python
import matplotlib.pyplot as plt
import numpy as np
counts = np.array([count for _, count in vocabulary.most_common()])

plt.figure()
plt.yscale("log")
plt.plot(counts)
plt.show()
```

![image-20210604105515752](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210604105516.png)

- `cumulative = np.cumsum(counts)` å•è¯æ•°é‡
- åˆ†ä½æ•°ï¼špercentile_index => 95%ä¸€å…±æœ‰10808å•è¯ => ==é‡ç‚¹å•è¯==

```python
cumulative = np.cumsum(counts)
cumulative = cumulative / cumulative[-1]	#æ‰€æœ‰countsçš„æ¯”ä¾‹æ˜¯1(cumulativeçš„æœ€åä¸€ä¸ªå…ƒç´ å°±æ˜¯æ€»å’Œ)
percentile_index = np.searchsorted(cumulative, 0.95) #0.95åˆ†ä½æ•°
print(f"{len(np.where(counts == 1)[0])} WÃ¶rter kommen nur einmal vor")	#18635 WÃ¶rter kommen nur einmal vor
print(f"{percentile_index+1} unterschiedliche WÃ¶rter decken 95% des Corpus ab")	#10808
```



## 3 æ·»åŠ é¦–å°¾æ ‡å¿—ğŸ’¡

```python
vocabulary = ["</s>", "<unk>"] + [word for word, count in vocabulary.most_common(percentile_index+1)]
tag_vocabulary = ["EOS"] + [tag for tag, count in tag_vocabulary.most_common()]
```



==æ„å»ºæŸ¥è¯¢å­—å…¸ï¼šç”¨æ•°å­—ç´¢å¼•ä»£æ›¿å•è¯==ğŸ’¡

```python
vocabulary_lookup = {word: i for i, word in enumerate(vocabulary)}	#10810
'''
{'</s>': 0,	#æ²¡æœ‰æ·»åŠ èµ·å§‹æ ‡è®°
 '<unk>': 1, #æ²¡æœ‰å‡ºç°çš„å•è¯å°±æ ‡è®°ä¸º1 dict.get(word, 1)
 ',': 2,
 'the': 3,
 '.': 4,
 'of': 5,
 'to': 6,
 'a': 7,
 'in': 8,
 'and': 9,
 "'s": 10,
 â€¦â€¦}
'''
tags_lookup = {tag: i for i, tag in enumerate(tag_vocabulary)}	#4
'''
{'EOS': 0,
 'nn': 1,
 'in': 2,
 'nnp': 3,
 'dt': 4,
 'jj': 5,
 'nns': 6,
 ',': 7,
 '.': 8,
 'cd': 9,
 'rb': 10,
 â€¦â€¦}
'''
```



# å¤„ç†æ•°æ®

| å¯¹è±¡               | å«ä¹‰                           | ç»´åº¦                                  |
| ------------------ | ------------------------------ | ------------------------------------- |
| start_counts       | freq(Sequenz => å¼€å¤´ç‰¹å®šçš„Tag) |                                       |
| bigram_counts      | freq(Tag => Tag)               |                                       |
|                    |                                |                                       |
| label_counts       | freq(Wort => ç‰¹å®šçš„Tag)        | ç»´åº¦ (vocabulary, tag_vocabulary)     |
|                    |                                |                                       |
| word_start_counts  | freq(Satz => å¼€å¤´ç‰¹å®šçš„Wort)   | vocabularyæ•°é‡(å«æ ‡ç‚¹)                |
| word_bigram_counts | freq(Wort => Wort)             | ç»´åº¦ (vocabulary, vocabulary)         |
| tag_start_counts   |                                | tag_vocabularyæ•°é‡                    |
| tag_bigram_counts  |                                | ç»´åº¦ (tag_vocabulary, tag_vocabulary) |

- ä»¥ä¸Šå¯¹è±¡çš„indexå¯¹åº”`vocabulary_lookup`çš„value(ç›¸åº”çš„keyä¹Ÿå°±æ˜¯å•è¯Wort)

```python
word_start_counts = np.zeros((len(vocabulary),), dtype=int)
word_bigram_counts = np.zeros((len(vocabulary), len(vocabulary)), dtype=int)
tag_start_counts = np.zeros((len(tag_vocabulary),), dtype=int)
tag_bigram_counts = np.zeros((len(tag_vocabulary), len(tag_vocabulary)), dtype=int)
label_counts = np.zeros((len(vocabulary), len(tag_vocabulary)), dtype=int)

for sentence, tagged in zip(corpus, tagged_corpus): #è®­ç»ƒé›†ä¸­çš„è¯­æ–™
    words = [vocabulary_lookup.get(word, 1) for word in sentence] + [0] #ä¸å­˜åœ¨å°±æ˜¯ç´¢å¼•1ï¼Œå¯¹åº”<unk>ï¼Œæ·»åŠ å°¾éƒ¨æ ‡è®° </s>
    tags = [tags_lookup[tag] for tag in tagged] + [0]	#æ·»åŠ å°¾éƒ¨æ ‡è®°EOS
    
    if len(words) == 0:
        continue

    word_start_counts[words[0]] += 1 #å¥å­é¦–å•è¯
    tag_start_counts[tags[0]] += 1 #å¥å­é¦–å•è¯çš„tag
    
    word_pairs = zip(words[:-1], words[1:]) #é¡ºåºè¿æ¥çš„è¯å¯¹
    for first, second in word_pairs:
        word_bigram_counts[first, second] += 1

    tag_pairs = zip(tags[:-1], tags[1:]) #é¡ºåºè¿æ¥çš„è¯çš„tagå¯¹
    for first, second in tag_pairs:
        tag_bigram_counts[first, second] += 1    
    
    for word, tag in zip(words, tags): #è¯ä¸ç›¸åº”tag
        label_counts[word, tag] += 1
```

```python
label_counts
'''array([[19928,     0,     0, ...,     0,     0,     0],
       [    0,  8736,    31, ...,    36,     9,     2],
       [    0,     1,     1, ...,     1,     1,     1],
       ...,
       [    0,     6,     1, ...,     1,     1,     1],
       [    0,     1,     1, ...,     1,     1,     1],
       [    0,     1,     1, ...,     1,     1,     1]])
'''
```



## æ³¨æ„|æ•°æ®ç¨€ç–é—®é¢˜ğŸ’¡

æ²¡æœ‰å‡ºç° => ä¼°è®¡æ¦‚ç‡ä¸º0ï¼šåˆ†æ¯ä¸èƒ½ä¸º0

- é¢‘ç‡åŠ 1ï¼šéƒ½è‡³å°‘å‡ºç°1æ¬¡
  - me|ä»¥ä¸‹ä»£ç ï¼šå¿…é¡»åŠ 1ä¹Ÿå¯ä»¥ => å› ä¸ºæœ€åçš„ç»“æœæ˜¯æ¦‚ç‡å€¼ï¼Œå¦‚æœæ˜¯0ï¼Œé‚£ä¹ˆlog(P=0)å°±æ˜¯-infï¼Œæ— æ³•å‚ä¸åç»­è®¡ç®—

```python
# æ³¨æ„ï¼šç¬¬ä¸€ä¸ªæ˜¯å¼€å§‹æ ‡è®°
word_start_counts[1:] += 1
word_bigram_counts[1:] += 1
tag_start_counts[1:] += 1
'''
array([   0,  928, 2708, 5697, 4108,  912,  900,    0,    0,  226, 1025,
         13,   55,  737,   62,   34,   99,  363,  263,   14,    9,    0,
         61,    4, 1044,    0,  106,   16,   30,   54,   59,  116,   32,
         35,    0,    0,   59,   67,   10,   13,    5,    0,    0,    7,
         28,   29])
'''
tag_bigram_counts[1:] += 1
label_counts[1:,1:] += 1

word_unigram_counts = label_counts.sum(1) #å•è¯æ•°é‡ (10810,) 95%çš„é‡ç‚¹å•è¯
'''array([19928, 52370, 53612, ...,    50,    50,    50])'''
tag_unigram_counts = label_counts.sum(0) #æ ‡ç­¾æ•°é‡ (46,)
'''
array([ 19928, 156972, 120240, 112807, 101125,  77666,  76850,  64381,
        54193,  51110,  44901,  44502,  39798,  36910,  35378,  34303,
        32910,  29899,  27051,  24410,  21512,  20462,  19971,  18850,
        18682,  18681,  16333,  15540,  14392,  13578,  13375,  13140,
        12958,  12717,  12674,  12397,  12378,  11740,  11282,  11201,
        11040,  10995,  10970,  10909,  10869,  10861])
'''


word_start_probs = word_start_counts / word_start_counts.sum()
word_bigram_probs = word_bigram_counts / word_unigram_counts[:, np.newaxis] #(10810, 10810) / (10810, 1)
word_label_probs = label_counts / word_unigram_counts[:, np.newaxis] #(10810, 46) /(10810, 1)

tag_start_probs = tag_start_counts / tag_start_counts.sum()
tag_bigram_probs = tag_bigram_counts / tag_unigram_counts[:, np.newaxis] #(46, 46) /(46, 1) => (46, 46)
tag_label_probs = label_counts / tag_unigram_counts #(10810, 46)
```



```python
a = np.array([0,1,2])
a[1:] +=1
a	#array([0, 2, 3])
```



# ç®€å•æ¨¡å‹: æ¦‚ç‡ç›´æ¥ä»£å…¥

## è¯„ä»·å‡½æ•°|å‡†ç¡®ç‡

- .classify()å¾—åˆ°æ ‡ç­¾çš„ç´¢å¼•å€¼ => ç»Ÿè®¡å¾—åˆ°æµ‹è¯•è¯­æ–™çš„tagçš„å‡†ç¡®ç‡

```python
class POSTagger:
    # Abstract base class
    def classify(self, sentence):
        # sentence: 1D np.array von Wortindizes
        # return: Gleich langes 1D array mit Tagindizes
        raise NotImplementedError #å‡å¦‚æ²¡æœ‰å®ç°ï¼šå°±æ²¡æœ‰ç±»çš„å¤šæ€è¦†ç›– => æ‰§è¡Œæ­¤å¤„çš„classifyï¼Œæœ€ç»ˆæŠ¥é”™
        
def evaluate(tagger): #taggeræ˜¯ä¸€ä¸ªç±»ï¼šå¸¦æœ‰classifyçš„æ–¹æ³•
    total, correct = 0, 0
    
    #==# æµ‹è¯•é›†ï¼šæ‰€æœ‰æµ‹è¯•é›†ä¸­çš„è¯­æ–™
    for sentence, tagged in zip(test_corpus, test_tagged_corpus): 
        words = np.array([vocabulary_lookup.get(word, 1) for word in sentence] + [0])
        labels = np.array([tags_lookup[tag] for tag in tagged])

        prediction = tagger.classify(words) #predictionå¾—åˆ°æ ‡ç­¾çš„ç´¢å¼•å€¼

        total += len(labels) #è¡¨ç¤ºä¸€ä¸ªå¥å­çš„å•è¯æ•°é‡
        correct += np.sum(prediction[:-1] == labels)

    print(f"Accuracy: {correct / total:.1%}")
```

- `word_label_probs`ï¼šæ¯ä¸ªå•è¯å¯¹åº”ä¸åŒæ ‡ç­¾çš„æ¦‚ç‡
  - self.assignments = assignment_probs.argmax(1) => æ¯ä¸ªå•è¯æœ€å¯èƒ½çš„æ ‡ç­¾tag

- 

```python
class SimpleTagger(POSTagger):
    def __init__(self, assignment_probs):
        self.assignments = assignment_probs.argmax(1) #æ²¿ç€axis=1(ä¹Ÿå°±æ˜¯46æ‰€å¯¹åº”çš„åˆ—)ï¼šæŒ‘é€‰å‡ºæ¦‚ç‡æœ€å¤§çš„å€¼çš„index => ç»´åº¦(10810,)
        print(self.assignments) #[0 3 7 ... 1 5 6] 10810å•è¯å„è‡ªå¯¹åº”çš„æ ‡ç­¾ => è®­ç»ƒç»“æœæ¥è‡ªword_label_probsä¸­çš„æœ€å¤§çš„æ¦‚ç‡å€¼
        
    
    def classify(self, sentence): # sentence: 1D np.array von Wortindizes
        sequence = self.assignments[sentence] #å®å‚æ—¶evaluateå‡½æ•°ä¸­çš„wordsï¼šä¹Ÿå°±æ˜¯æ¯ä¸€ä¸ªå¥å­ä¸­çš„å•è¯çš„ç´¢å¼• => å¾—åˆ°æ ‡ç­¾çš„ç´¢å¼•
        return sequence

evaluate(SimpleTagger(word_label_probs)) #(10810, 46)
'''output
[0 3 7 ... 1 5 6]
Accuracy: 88.6%
'''
```



# Decoder: Viterbi|æµ‹é‡åºåˆ—Wort => çŠ¶æ€ä¼°è®¡Tag

- EOSç”¨ä½œèµ·å§‹ã€ç»“æŸçŠ¶æ€
  - tag_start_probsä¹Ÿå°±æ˜¯åˆå§‹çŠ¶æ€çš„åˆ†å¸ƒæ¦‚ç‡(åŸå§‹æ•°æ®ä¸­ï¼šé¦–å•è¯tagçš„æ¦‚ç‡åˆ†å¸ƒ)

```python
transition_probs = np.copy(tag_bigram_probs) #Zustand: Tagçš„ä¼ é€’æ¦‚ç‡ (46, 46)
transition_probs[0] = tag_start_probs

emission_probs = tag_label_probs #Zustand: Tagçš„æ¦‚ç‡
```

æ³¨æ„ï¼štransitionæ˜¯æœ‰æ¬¡åºçš„ï¼Œæ‰€ä»¥å¹¶ä¸å¯¹ç§°

```python
transition_probs[2][1] == transition_probs[1][2] #False è¯´æ˜ä¸å¯¹ç§°
```



æ³¨æ„ï¼š$v_t$æ¦‚ç‡ç›¸ä¹˜ï¼Œ20æ¬¡è®¡ç®—åï¼Œç²¾è¯»å°äºdoubleçš„åˆ†è¾¨ç‡ => ä½¿ç”¨$log(v_t)$

```python
np.log2(transition_probs[0]) 
'''
array([        -inf,  -4.42622864,  -2.88221875,  -1.80952352,
        -2.28119181,  -4.45129238,  -4.47038013, -14.28576343,
       -14.28576343,  -6.45921494,  -4.28294841, -10.4784085 ,
        -8.4784085 ,  -4.75828642,  -8.3084835 ,  -9.15648041,
        -7.64190724,  -5.77796879,  -6.24136931, -10.37887283,
       -10.96383533, -14.28576343,  -8.33156712, -11.96383533,
        -4.2564762 , -14.28576343,  -7.54429644, -10.19830058,
        -9.33156712,  -8.50440371,  -8.37887283,  -7.41539871,
        -9.24136931,  -9.11583842, -14.28576343, -14.28576343,
        -8.37887283,  -8.19830058, -10.82633181, -10.4784085 ,
       -11.70080092, -14.28576343, -14.28576343, -11.28576343,
        -9.42778243,  -9.37887283])
'''
```



## ä»£ç å®ç°

> - å•å¼•å·æ³¨é‡Šçš„æ˜¯åŸç‰ˆï¼šå‚è€ƒæ€è·¯
> - printæ³¨é‡Šä¸»è¦ç”¨æ¥è°ƒè¯•

```python
class HMM(POSTagger):
    def __init__(self, transition, emission):
        self.transition = transition
        self.emission = emission
        self.num_emissions, self.num_states = emission.shape
        assert self.transition.shape == (self.num_states, self.num_states)
        assert self.emission.shape == (self.num_emissions, self.num_states)
        self.log_transition = np.log2(transition)   #(46, 46)
        self.log_emission = np.log2(emission)       #(10810, 46)
        #print(self.log_transition)
        #print(self.log_emission)

    def classify(self, observation):
        '''æ³¨æ„ï¼šæ‰€æœ‰çš„æ¦‚ç‡ç›¸ä¹˜éƒ½æ”¹ä¸ºlog_probç›¸åŠ 
        INPUT:
            observation: => sentence: 1D np.array von Wortindizes
        OUTPUT:
            sequence: Gleich langes 1D array mit Tagindizes
        '''
        T = len(observation) #tè¡¨ç¤ºåºåˆ—é¡ºåº => ä¹Ÿå°±æ˜¯æ—¶åˆ»
        N = self.num_states #46ä¸ªçŠ¶æ€ï¼šç¬¬ä¸€ä¸ªæ˜¯åˆå§‹EOSçš„è½¬ç§»æ¦‚ç‡åˆ†å¸ƒ
        

        v_log_probs = np.zeros([T, N])
        best_states = np.zeros([T, N], dtype=int) #æ•°å­—ç´¢å¼•æ˜¯æ•´å‹æ•°æ®
        '''
        v_log_probs = [[0]*N for _ in range(T)] 
        best_states = [[0]*N for _ in range(T)]
        '''
        
            
        #step1: init 
        v_log_probs[0] = self.log_transition[0] + self.log_emission[observation[0]]
        best_states[0,:] = 0
        '''
        for i in range(N):
            v_log_probs[0][i] = self.log_transition[0][i] + self.log_emission[observation[0]][i] #???ç»´åº¦åŒ¹é…
            best_states[0][i] = 0
        '''
        #print(v_log_probs)
        #print(best_states)
        
        
        #step2: iter
        for t in range(1,T):
            for i in range(N):
                temp,maxindex = float('-inf'),0
                for j in range(N):
                    res = v_log_probs[t-1][j] + self.log_transition[j][i]
                    #print(v_log_probs[t-1][j]) #, self.log_transition[j][i]
                    #print(-999, res, temp, "\n")
                    if res > temp:
                        temp = res
                        maxindex = j

                v_log_probs[t][i] = temp + self.log_emission[observation[t]][i]
                best_states[t][i] = maxindex

        #step3: end => æ”¹|ç›´æ¥æ±‚ä¸‹æ ‡
        best_state_end = np.argmax(v_log_probs[-1])
        #print("Log-Wahrscheinlichkeit des Viterbi-Pfads ist: ", v_log_probs[-1, best_state_end])
        #print(-1, type(best_state_end))
        #print(999, best_states)
        '''
        p = max(v_log_probs[-1])
        for i in range(N):
            if v_log_probs[-1][i] == p:
                best_state_end = i
        print(-2, best_state_end)
        '''
        
        #step4ï¼šbacktrack
        sequence = np.zeros(T, dtype=int)
        state = best_state_end
        for t in reversed(range(T-1)):
            #print(0,state)
            state = best_states[t+1][state]
            sequence[t] = state
        sequence[-1] = best_state_end

        return sequence
        
    
    def classify_score(self, observation):
        """vertib-pathçš„æ¦‚ç‡
        """
        # Ihr code hier
        T = len(observation)
        seq = self.classify(observation)

        v_log_probs = self.log_transition[0][seq[0]] + self.log_emission[observation[0]][seq[0]]
        
        for i in range(T-1):
            v_log_probs += self.log_transition[seq[i]][seq[i+1]] + self.log_emission[observation[i+1]][seq[i+1]]
        return v_log_probs
```



æµ‹è¯•ï¼š

```python
# Wir haben Nullwahrscheinlichkeiten, die beim Logarithmus Warnungen produzieren
import warnings
warnings.filterwarnings("ignore", "divide by zero") #å¦‚æœæ²¡æœ‰è¿™ä¸ªï¼šæ˜¾ç¤ºRuntimeWarning: divide by zero encountered in log2 => ä¸ªåˆ«æ•°æ®æ˜¯0(æ¯”å¦‚transition[0, 0])

hmm = HMM(transition_probs, #(46, 46)
          emission_probs)   #(10810, 46)

words = np.array([vocabulary_lookup.get(word, 1) for word in corpus[1000]] + [0])

np.testing.assert_equal(hmm.classify(words),
                           np.array([ 3,  3,  7,  9,  6,  5,  7, 11,  5,  1,  2,  4,  1,  2,  6, 13,  5,
                                      6,  2,  9, 14,  9,  8, 17, 20, 12,  2,  4,  1,  8,  0]))
# Log-Wahrscheinlichkeit des Viterbi-Pfads ist -273.094
print(f"Log-Wahrscheinlichkeit des Viterbi-Pfads ist: {hmm.score(words)}")	#-246.27664177066708
```



è¯„ä»·æ¨¡å‹çš„è®­ç»ƒç»“æœï¼š

- æ¨¡å‹è®­ç»ƒçš„ä½œç”¨ï¼šä¸åŒçš„å•è¯å¯¹åº”ä¸åŒçš„tag => æé«˜å¯¹åº”tagçš„å‡†ç¡®ç‡

```python
%%time 
evaluate(hmm)#or %time evaluate(hmm) 
'''
Accuracy: 91.7%
Wall time: 8min 22s
'''
```



## å‚è€ƒç­”æ¡ˆ

- innerçŸ©é˜µæ˜¯æ ¸å¿ƒ => è¿ç®—å‘é‡åŒ–â­
  - åŸºäºnp.max(, axis=)å’Œnp.argmax()è¿›è¡Œå‘é‡çš„è¿ç®—ï¼Œè€Œä¸æ˜¯å•ä¸ªå…ƒç´ æ¯”è¾ƒ

```python
	def classify(self, observation):
        length = len(observation)
        # çŸ©é˜µï¼šè¡¨ç¤ºè·¯å¾„ => è®°å½•æŸä¸ªæ—¶åˆ»çš„çŠ¶æ€çš„æ¦‚ç‡ => ä»è€Œé€‰å–æœ€ä¼˜çš„çŠ¶æ€ => åŠ¨æ€è§„åˆ’çš„dpè¡¨æ ¼
        psi = np.zeros((length, self.num_states), dtype=np.int)
        
        # è¡¨ç¤ºæŸä¸ªæ—¶åˆ»tçš„æ‰€æœ‰çŠ¶æ€çš„æ¦‚ç‡
        delta = np.full((self.num_states,), float("-inf"))
        delta[0] = 0
        
        for t in range(length): #éšç€æ—¶åˆ»t: ä¸€æ­¥æ­¥è¿›è¡Œå˜åŒ–
            #==# innerçŸ©é˜µæ˜¯æ ¸å¿ƒ
            inner = delta[:, np.newaxis] + self.log_transition #inneræ˜¯ä¸€ä¸ªçŸ©é˜µï¼šè¡¨ç¤ºt-1æ—¶åˆ»çŠ¶æ€åˆ°tæ—¶åˆ»æ‰€æœ‰çŠ¶æ€çš„å‰å‘æ¦‚ç‡å€¼ => åœ¨ç»´åº¦0ä¸Šå–maxï¼šä¹Ÿå°±æ˜¯åˆå¹¶è¡Œï¼Œä¿ç•™åˆ—çš„max(ä¿ç•™å˜æ¢åçŠ¶æ€çš„maxæ¦‚ç‡å€¼)
            delta = np.max(inner, 0) + self.log_emission[observation[t]]
            psi[t] = np.argmax(inner, 0)
        
        best_path = np.empty((length,), dtype=np.int)
        best_path[length-1] = np.argmax(delta)
        for t in reversed(range(1, length)):
            best_path[t-1] = psi[t, best_path[t]]
        return best_path
```





# è¡¥å……|Language Modeling

## è¯„ä»·å‡½æ•°|ç†µ => PPå€¼

- .score()å¾—åˆ°å•ä¸ªå¥å­çš„log(p) => æ•´åˆå¾—åˆ°æ•´ä¸ªè¯­æ–™çš„ç†µ =>å¾—åˆ°Perplexity

```python
class LanguageModel:
    def score(self, sentence):
        # sentence: 1D np.array von Wortindizes
        # return: Skalar. log-Wahrscheinlichkeit dieses Satzes
        raise NotImplementedError

def evaluate_lm(lm):
    entropy, total = 0, 0
    
    for sentence in test_corpus:
        words = np.array([vocabulary_lookup.get(word, 1) for word in sentence] + [0])

        lprob = lm.score(words)
        if lprob == float("-inf"):
            print(words)

        total += len(words)
        entropy -= lprob

    print(f"Perplexity: {2 ** (entropy / total):.1f}")
```



## ä¸åŒæ¨¡å‹çš„æ¯”è¾ƒ

### 1 å‚æ•°

| è¯­è¨€æ¨¡å‹        | å‚æ•°ä¸ªæ•°               |
| --------------- | ---------------------- |
| Unigramm-Modell | Vï¼šè¡¨ç¤ºçŠ¶æ€ä¸ªæ•°        |
| HMM             | $T + T^2 +  V \cdot T$ |
| Bigramm-Modell  | $V^2$                  |

### 2 æŒ‡æ ‡Perplexity

- $\hat{H}(W)$æ˜¯log(p)çš„å€¼ï¼Œæ±‚å’Œå–å¹³å‡
- me|å‡å¦‚é¢„æµ‹å®Œå…¨æ­£ç¡®ï¼Œé‚£ä¹ˆæ¦‚ç‡å°±æ˜¯1ï¼Œ$\hat{H}(W)$ä¹Ÿå°±æ˜¯0ï¼Œæœ€åçš„PP(W)å°±æ˜¯1
  - å¦åˆ™ï¼šlog(p)å°äº0ï¼Œ$\hat{H}(W)$ä¹Ÿå°±å¤§äº0ï¼Œé‚£ä¹ˆPP(W)ä¹Ÿå°±å˜å¤§

![image-20210605151559745](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210605151559.png)

```python
class UnigramLanguageModel(LanguageModel):
    def __init__(self, unigram_probs):
        self.unigram_probs = np.log2(unigram_probs)	#æ¦‚ç‡å€¼æ˜¯log
    
    def score(self, sentence):	#é’ˆå¯¹å•ä¸ªå¥å­
        return self.unigram_probs[sentence].sum()

evaluate_lm(	#é’ˆå¯¹æ‰€æœ‰è¯­æ–™
    UnigramLanguageModel(
        word_unigram_counts / word_unigram_counts.sum())
)
'''output
Perplexity: 731.2
'''
```



```python
class BigramLanguageModel(LanguageModel):
    def __init__(self, start_probs, bigram_probs):	#æ¦‚ç‡å€¼æ˜¯log
        self.start_probs = np.log2(start_probs)
        self.bigram_probs = np.log2(bigram_probs)
    
    def score(self, sentence):
        lprob = self.start_probs[sentence[0]]	#åˆå§‹å•è¯
        lprob += self.bigram_probs[sentence[:-1], sentence[1:]].sum()
        return lprob

evaluate_lm(
    BigramLanguageModel(
        word_start_probs,
        word_bigram_probs)
)
'''output
Perplexity: 83.2
'''
```



```python
%%time 
evaluate_lm(hmm)	
'''output
Perplexity: 522.4
Wall time: 4.18 s
'''
```



# Evaluation: Forward-Algorithmus mit Skalierung

1 é—®é¢˜èƒŒæ™¯

> 1 ä½¿ç”¨log-æ¦‚ç‡ï¼šæ¦‚ç‡å€¼å¤šæ¬¡ç›¸ä¹˜åå¾ˆå°ï¼Œå¯èƒ½å°äºè®¡ç®—æœºdoubleçš„åˆ†è¾¨ç‡ğŸ‘Œ
>
> 2 Viterbiå¯ä»¥ä½¿ç”¨log-æ¦‚ç‡ VS. å‰å‘ç®—æ³•ä¸å¯ä»¥(ä¸ºä»€ä¹ˆï¼šå‰å‘ç®—æ³•æ¶‰åŠå…ƒç´ ç›¸åŠ æ±‚å’Œï¼Œä¸€æ—¦è½¬åŒ–ä¸ºlogï¼Œæ— æ³•å®Œæˆæ­¤è®¡ç®— => è¢«è¿«è½¬æ¢å›åŸå§‹å°ºåº¦)
>
> - me|å°ç»“ï¼šlogé€‚åˆç”¨æ¥è®¡ç®—ä¹˜æ³•ï¼Œä½†æ˜¯å½“å…¶ä¸­æ··å«åŠ æ³•æ—¶ï¼Œæ— æ³•æœ‰æ•ˆè®¡ç®—
>
> 3 ä¸èƒ½ä½¿ç”¨log-æ¦‚ç‡çš„æŒ½æ•‘åŠæ³•ï¼šå½’ä¸€åŒ–skalierung

```python
    def score(self, observation):
        T = len(observation)
        N = self.num_states
        '''log-pçš„ç‰ˆæœ¬
        '''
        log_alpha_t_s = np.log2(np.zeros([T, N]))
        log_alpha_t_s[0][0] = 0 #EOSæ¦‚ç‡ä¸º1 => Logæ˜¯0

        for t in range(T-1):
            for i in range(N):
                for j in range(N):
                    #é—®å·å³è¾¹æ˜¯ä¸€ä¸ªæ¦‚ç‡å€¼(ä»çŠ¶æ€jå˜æ¢åˆ°çŠ¶æ€i) => éœ€è¦éå†çŠ¶æ€jæ±‚å’Œ  => åŸºäºlogï¼Œæ²¡æœ‰ç›¸åº”çš„è¿ç®—
                	log_alpha_t_s[t+1][i] é—®å· log_alpha_t_s[t][j] \
                    						+ self.log_transition[j][i] \
                                            + self.log_emission[observation[i], j]
                
        end_probs = 2**(log_alpha_t_s[-1])

        return end_probs.sum()
```

åŸç†ç»†èŠ‚ï¼š
$$
\alpha_t[i] = \sum_j \alpha_{t-1}[j] \cdot transition[j][i]
$$




2 å½’ä¸€åŒ–skalierungâ­

![image-20210604121405850](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210604121406.png)

```python
    def score(self, observation):
        T = len(observation)
        
        alpha_t_s = np.zeros([T+1, self.num_states])
        alpha_t_s[0][0] = 1 #EOSæ¦‚ç‡ä¸º1
        c = np.zeros([T+1,])
        c[0] = 1

        for i in range(T-1):
            alpha_t_s[i+1] = np.dot(alpha_t_s[i], self.transition) * self.emission[observation[i]]
            c[i+1] = alpha_t_s[i+1].sum()
            alpha_t_s[i+1] = alpha_t_s[i+1] / c[i+1]
            
        log_prob = np.log2(alpha_t_s[-1][0]) + np.log2(c).sum()

        return 2**log_prob
```



## å‚è€ƒç­”æ¡ˆ

```python
    def score(self, observation):
        length = len(observation)
        alpha = np.zeros((self.num_states,))
        alpha[0] = 1
        
        c = np.zeros((length,))
        for t in range(length):
            alpha = np.dot(alpha, self.transition) * self.emission[observation[t]]
            c[t] = alpha.sum()
            alpha /= c[t]
        return np.log2(c).sum() + np.log2(alpha[0])
    
    def _score(self, observation):
        length = len(observation)
        alpha = np.zeros((self.num_states,))
        alpha[0] = 1
        for t in range(length):
            alpha = np.dot(alpha, self.transition) * self.emission[observation[t]]
        return np.log2(alpha.sum())
```



# æ€»ç»“

> å…³é”®åœ¨äºHMMæ¨¡å‹ï¼Œç®—æ³•çš„ç†è§£å’Œåœ¨ä»€ä¹ˆæƒ…å¢ƒä¸‹ä½¿ç”¨

## çŸ¥è¯†ç‚¹

1 NLP 

- [è¯æ€§åˆ—è¡¨](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)



## é€šç”¨æŠ€å·§â­

1 å†…å­˜ä¸è¶³ 

1ï¼‰ä½¿ç”¨å°‘é‡æºæ•°æ®ï¼š95%é¢‘æ¬¡ => 90%

- è®­ç»ƒæ•°æ®ï¼šæ¯”å¦‚è¯­æ–™ä½¿ç”¨ä¸€éƒ¨åˆ†å¥å­

2ï¼‰ä½¿ç”¨Google Colab

3ï¼‰ä½¿ç”¨JetBrains Datalore



2 å‡å¦‚æœ‰åˆå§‹çŠ¶æ€ï¼Œæ¶‰åŠçŠ¶æ€é—´çš„è½¬ç§»ï¼Œé‚£ä¹ˆ==ä¸‹æ ‡ç»Ÿä¸€== => å¤šå‡ºä¸€ä¸ª0çš„ä½ç½®

```python
    def score(self, observation):
        T = len(observation)
        
        alpha_t_s = np.zeros([T+1, self.num_states])
        alpha_t_s[0][0] = 1 #EOSæ¦‚ç‡ä¸º1
        c = np.zeros([T+1,])
        c[0] = 1

        for i in range(T-1):
            alpha_t_s[i+1] = np.dot(alpha_t_s[i], self.transition) * self.emission[observation[i]]
            c[i+1] = alpha_t_s[i+1].sum()
            alpha_t_s[i+1] = alpha_t_s[i+1] / c[i+1]
            
        log_prob = np.log2(alpha_t_s[-1][0]) + np.log2(c).sum()

        return 2**log_prob
```

![image-20210605153435446](https://raw.githubusercontent.com/DaiDuncan/PicUploader/main/img3/20210605153436.png)



## å¸¸è§åº”ç”¨æƒ…å¢ƒ

1 zip()

ä¸¤ä¸ªç›¸å…³è”çš„å¯¹è±¡ç»„åˆåœ¨ä¸€èµ·

- `word_bigram_counts`, `tag_bigram_counts`, `label_counts`

```python
    word_pairs = zip(words[:-1], words[1:])
    for first, second in word_pairs:
        word_bigram_counts[first, second] += 1

    tag_pairs = zip(tags[:-1], tags[1:])
    for first, second in tag_pairs:
        tag_bigram_counts[first, second] += 1    
    
    for word, tag in zip(words, tags):
        label_counts[word, tag] += 1
```



2 numpyåˆ›å»ºæ•°ç»„

- `np.zeros(, dtype)`
- `np.full(, float("-inf"))`
- 

```python
def classify(self, observation):
    length = len(observation)
    psi = np.zeros((length, self.num_states), dtype=np.int)
    delta = np.full((self.num_states,), float("-inf"))
    delta[0] = 0

    for t in range(length):
        inner = delta[:, np.newaxis] + self.log_transition
        delta = np.max(inner, 0) + self.log_emission[observation[t]]
        psi[t] = np.argmax(inner, 0)

    best_path = np.empty((length,), dtype=np.int)
    best_path[length-1] = np.argmax(delta)
    for t in reversed(range(1, length)):
        best_path[t-1] = psi[t, best_path[t]]
    return best_path
```

