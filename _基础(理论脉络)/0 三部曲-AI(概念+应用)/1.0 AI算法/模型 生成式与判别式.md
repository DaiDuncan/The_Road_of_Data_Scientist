# [判别|Discriminative model](https://easyai.tech/ai-definition/discriminative-model/)

在机器学习领域判别模型是一种对未知数据 y 与已知数据 x 之间关系进行建模的方法。

判别模型是一种基于概率理论的方法。已知输入变量 x ，判别模型通过==构建条件概率分布 P(y|x) 预测 y== 。



判别模型，也称为条件模型，是一类用于统计分类的模型，尤其是在有监督的 机器学习中。

判别分类器试图通过仅依赖于观察到的数据进行建模，同时学习如何从给定的统计数据进行分类。

监督学习中使用的方法可以分为判别模型或生成模型。

**与生成模型相比，判别模型对分布的假设较少**，但在很大程度上取决于数据的质量。



例如，给定一组狗和兔子的标记图片，**辨别模型**将新的未标记图片与最相似的标记图片匹配，然后给出标签类，狗或兔子。

然而，生成将开发一个模型，应该能够从他们所做的假设输出类标签到未标记的图片，就像所有兔子都有红眼。



典型的判别学习方法包括：

- 逻辑回归（LR），支持向量机（[SVM](https://easyai.tech/ai-definition/svm/)） ，条件随机场（CRF）（在无向图上指定）等。



典型的生成模型方法包含：

- 朴素贝叶斯，高斯混合模型等。





# [生成|Generative model](https://easyai.tech/ai-definition/generative-model/)

在概率统计理论中， 生成模型是指能够**随机生成观测数据的模型**，尤其是在给定**某些隐含参数的条件下**。

它给观测值和标注数据序列指定一个==联合概率分布==。

在机器学习中，生成模型：

- 可以用来直接对数据建模（例如根据**某个变量的概率密度函**数进行数据采样）
- 也可以用来建立变量间的**条件概率分布**。
  - 条件概率分布可以由生成模型根据贝叶斯定理形成@Informationsfusion：贝叶斯网络





在统计分类中，包括机器学习，两种主要方法被称为**生成**方法和**判别**方法。

这些计算分类器采用不同的方法，统计建模的程度不同。

术语不一致，但可以区分三种主要类型，遵循Jebara（2004）：

- 给定一个可观察到的变量X和目标变量Y，一个**生成模型**是一个统计模型的的联合概率分布上

![{\displaystyle P(X,Y)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a6fd347c3c83e69250fe5a6c3648fed056ff8f68)

- **判别模型**是:

![{\ displaystyle P（Y | X = x）}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d84be69ab64442b5c15c7f749f715b8f3d75acc1)

- 不使用概率模型计算的分类器也被宽泛地称为“判别型”

最后两个类之间的区别并不一致; 

- Jebara（2004）将这三个类别称为生成学习*，*条件学习和歧视性学习，
- 但Ng＆Jordan（2002）仅区分两个类别，称为**生成分类器**（联合分布）和**判别分类器**（条件分布或没有分配），没有区分后两类。



---

[CS229 2014.11.01](https://www.cnblogs.com/fanyabo/p/4067295.html)

# **一、引言**

本材料参考Andrew Ng大神的机器学习课程 http://cs229.stanford.edu

在上一篇有监督学习回归模型中，我们利用训练集直接对**条件概率**$p(y|x;θ)$建模，例如logistic回归就利用$h_θ(x) = g(θ^Tx)$对$p(y|x;θ)$建模（其中g(z)是sigmoid函数）。

- 假设现在有一个分类问题，要根据一些动物的特征来区分大象(y = 1)和狗(y = 0)。

- 给定这样的一种数据集，回归模型比如logistic回归会试图找到一条直线也就是决策边界，来区分大象与狗这两类，

- 然后对于新来的样本，回归模型会**根据这个新样本的特征**计算这个样本会落在**决策边界**的哪一边，从而得到相应的分类结果。

=> 根据**样本特征**，基于**决策边界**划分分类



　　现在我们考虑另外一种建模方式：

- 首先，根据训练集中的大象样本，我们可以建立大象模型，根据训练集中的狗样本，我们可以建立狗模型。

- 然后，对于新来的动物样本，我们可以让它**与大象模型匹配看概率有多少**，与狗模型匹配看概率有多少，哪一个概率大就是那个分类。

=> 与**模型匹配的概率**





判别式模型（Discriminative Model）是直接对**条件概率**$p(y|x;θ)$建模。

- 常见的判别式模型有 线性回归模型、LDA(线性判别分析)、支持向量机SVM、神经网络等。



生成式模型（Generative Model）则会对x和y的**联合分布**$p(x,y)$建模，然后通过贝叶斯公式来求得$p(y_i|x)$，然后选取使得$p(y_i|x)$最大的$y_i$，即：

![img](https://images0.cnblogs.com/blog/392228/201411/011342115036933.jpg)

- 常见的生成式模型有 隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA(Latent Dirichlet Allocation)等。

 

# **二、高斯判别分析 Gaussian Discriminant Analysis**

高斯判别分析GDA是一种生成式模型，在GDA中，假设$p(x|y)$满足多值正态分布。



多值正态分布介绍如下：

## **2.1 多值正态分布 multivariate normal distribution**

一个**n维**的多值正态分布可以表示为**多变量**高斯分布，其参数为均值向量![img](https://images0.cnblogs.com/blog/392228/201411/011351298477290.jpg)，协方差矩阵![img](https://images0.cnblogs.com/blog/392228/201411/011352142223900.jpg)，其概率密度表示为：

![img](https://images0.cnblogs.com/blog/392228/201411/011352592848665.jpg)

当均值向量为2维时概率密度的直观表示：

![img](https://images0.cnblogs.com/blog/392228/201411/011356240974128.jpg)

左边的图表示均值为0，协方差矩阵∑ = I；

中间的图表示均值为0，协方差矩阵∑ = 0.6I；

右边的图表示均值为0，协方差矩阵∑ = 2I。

可以观察到，

- 协方差矩阵越大，概率分布越扁平；
- 协方差矩阵越小，概率分布越高尖。(更加集中)

 

## **2.2 高斯判别分析模型**

如果有一个分类问题，其训练集的输入特征x是**随机的连续值**，就可以利用高斯判别分析。

可以假设p(x|y)满足多值正态分布，即：

![img](https://images0.cnblogs.com/blog/392228/201411/011407235191838.jpg)

该模型的概率分布公式为：

![img](https://images0.cnblogs.com/blog/392228/201411/011408060341049.jpg)

模型中的参数为Φ，Σ，μ0和μ1

- Φ是y = 1的概率，
- Σ是协方差矩阵，
- μ0是y = 0对应的特征向量x的均值 ， 
- μ1是y = 1对应的特征向量x的均值



其中，于是得到它们的计算公式如下：

![img](https://images0.cnblogs.com/blog/392228/201411/011415130198003.jpg)



于是==似然函数（x和y的联合分布）==为：

![img](https://images0.cnblogs.com/blog/392228/201411/011411223478974.jpg)



于是这样就可以对p(x,y)建模，从而**得到概率p(y = 0|x)与p(y = 1|x)**，从而得到分类标签。

其结果如下图所示：

- 在边界线附近的值：即可以属于y=0类，也可以属于y=1类(属于两个类的概率接近)

![img](https://images0.cnblogs.com/blog/392228/201411/011417484877473.jpg)

 

# **三、朴素贝叶斯模型**

在==高斯判别分析GDA中，特征向量x是连续实数值==

如果特征向量x是==离散值==，可以利用朴素贝叶斯模型。

## **3.1 垃圾邮件分类**

假设我们有一个已被标记为是否是垃圾邮件的数据集，要建立一个垃圾邮件分类器。

用一种简单的方式来描述邮件的特征，有一本词典，如果邮件包含词典中的第i个词，则设xi = 1，如果没有这个词，则设xi = 0

最后会形成这样的特征向量x：

![img](https://images0.cnblogs.com/blog/392228/201411/011522146757097.jpg)

这个特征向量表示邮件包含单词"a"和单词"buy"，但是不包含单词"aardvark,"aardwolf","zygmurgy"。

特征向量x的维数等于字典的大小。

假设字典中有5000个单词，那么特征向量x就为5000维的包含0/1的向量

> 如果我们建立多项式分布模型，那么有25000中输出结果，这就意味着有接近25000个参数，这么多的参数，要建模很困难。



因此为了建模p(x|y)，必须做出强约束假设，这里假设对于给定的y，特征x是条件独立的，这个假设条件称为朴素贝叶斯假设，得到的模型称为朴素贝叶斯模型。

比如，如果y= 1表示垃圾邮件，其中包含单词200 "buy"，以及单词300 "price"，那么我们假设此时单词200 "buy" x200、单词300"price"x300 是条件独立的，可以表示为p(x200|y) = p(x200|y,x300)。

注意，这个假设与x200与x300独立是不同的，x200与x300独立可以写作：p(x200) = p(x200|x300)；

这个假设是==对于给定的y==，x200与x300是**条件独立**的。



因此，利用上述假设，根据链式法则得到：

![img](https://images0.cnblogs.com/blog/392228/201411/011541588005974.jpg)

该模型有3个参数：

- $\Phi_{i|y=1}=p(x_i=1|y=1)$
- $\Phi_{i|y=0}=p(x_i=1|y=0)$
- $\Phi_{y}=p(y=1)$



根据生成式模型的规则，我们**要使联合概率最大**：

![img](https://images0.cnblogs.com/blog/392228/201411/011549386903276.jpg)

根据这3个参数意义，可以得到它们各自的计算公式：

![img](https://images0.cnblogs.com/blog/392228/201411/011550340655991.jpg)



这样就得到了朴素贝叶斯模型的完整模型。

==对于新来的邮件特征向量x==，可以计算：

![img](https://images0.cnblogs.com/blog/392228/201411/011552478472220.jpg)

**实际上只要比较分子就行了**，分母对于y = 0和y = 1是一样的，这时只要比较p(y = 0|x)与p(y = 1|x)哪个大就可以确定邮件是否是垃圾邮件。

 

## **3.2 拉普拉斯平滑|针对数据稀疏问题**

朴素贝叶斯模型可以在大部分情况下工作良好。但是该模型有一个缺点：==对数据稀疏问题敏感==。



比如在邮件分类中，对于低年级的研究生，单词NIPS显得太过于高大上，邮件中可能没有出现过，现在新来了一个邮件"NIPS call for papers"，假设NIPS这个词在词典中的位置为35000，然而NIPS这个词从来没有在训练数据中出现过，这是第一次出现NIPS，于是算概率时：

![img](https://images0.cnblogs.com/blog/392228/201411/011603061129503.jpg)

由于NIPS从未在垃圾邮件和正常邮件中出现过，所以结果只能是0了。

于是最后的后验概率：

![img](https://images0.cnblogs.com/blog/392228/201411/011604188624478.jpg)

　　对于这样的情况，我们可以采用拉普拉斯平滑，==对于未出现的特征，我们赋予一个小的值而不是0==。



具体平滑方法为：

假设离散随机变量取值为{1,2,···,k}，原来的估计公式为：

![img](https://images0.cnblogs.com/blog/392228/201411/011607473477351.jpg)

使用拉普拉斯平滑后，新的估计公式为：

![img](https://images0.cnblogs.com/blog/392228/201411/011608375812970.jpg)

即**每个k值出现次数加1**，分母总的加k，类似于NLP中的平滑，具体参考宗成庆老师的《统计自然语言处理》一书。

　　对于上述的朴素贝叶斯模型，参数计算公式改为：

![img](https://images0.cnblogs.com/blog/392228/201411/011611590507520.jpg)

