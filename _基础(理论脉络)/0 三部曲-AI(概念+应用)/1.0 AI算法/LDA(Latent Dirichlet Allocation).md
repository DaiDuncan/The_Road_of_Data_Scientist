[LDA](https://easyai.tech/ai-definition/latent-dirichlet-allocationlda/)

- [网络转载版本](https://easyai.tech/ai-definition/latent-dirichlet-allocationlda/#zhuan)
- [百度百科版本](https://easyai.tech/ai-definition/latent-dirichlet-allocationlda/#baidu)
- [维基百科版本](https://easyai.tech/ai-definition/latent-dirichlet-allocationlda/#wiki)

## 网络转载版本

要描述 LDA 模型，就要说一下 ==LDA 模型所属的**产生式模型**==的背景。

产生式模型是相对于**判别式模型**而说的。

这里，我们假设需要建模的数据有特征信息，也就是通常说的 X，以及标签信息，也就是通常所说的 Y。



判别式模型常常**直接对 Y 的产生过程（Generative Process) 进行描述，而对特征信息本身不予建模**。

这使得判别式模型**天生就成为构建分类器或者回归分析的有利工具**。

而产生式模型则要同时对 X 和 Y 建模，这使得产生式模型**更适合做无标签的数据分析**，比如聚类。

当然，因为产生式模型要对比较多的信息进行建模，所以一般认为对于同一个数据而言，==产生式模型要比判别式模型更难以学习==。



一般来说，产生式模型希望通过一个产生过程来帮助读者理解一个模型。

注意，这个产生过程本质是描述一个**联合概率分布**（Joint Distribution）的分解过程。

也就是说，这个过程是一个虚拟过程，真实的数据往往并不是这样产生的。

这样的产生过程是模型的一个假设，一种描述。

==任何一个产生过程都可以在数学上**完全等价一个联合概率分布**。==



LDA 的产生过程描述了文档以及文档中文字的生成过程。

在原始的 LDA 论文中，作者们描述了对于每一个文档而言有这么一种生成过程：

1. 首先，从一个**全局的泊松**（Poisson）参数为β的分布中生成一个**文档的长度** N；
2. 从一个**全局的狄利克雷**（Dirichlet）参数为α的分布中生成一个**当前文档的θ**；
3. 然后对于当前文档长度 N 的每一个字执行以下两步，
   - 一是从以θ为参数的多项（Multinomial）分布中生成一个**主题**（Topic）的下标（Index）z_n；
   - 二是从以φ和 z 共同为参数的多项分布中产生一个**字**（Word）w_n。

![img](https://easy-ai.oss-cn-shanghai.aliyuncs.com/img/006tNc79gy1fzn9chzuxlj30gh08nt91.jpg)

从这个描述我们可以马上得到这些重要的模型信息。

第一，我们有一个维度是 K 乘以 V 的主题矩阵（Topic [Matrix](https://easyai.tech/ai-definition/matrix/)）。

- 其中每一行都是一个φ，也就是某一个生成字的多项分布。
- 当然，这个主题矩阵我们在事先并不知道，是需要学习得到的。

另外，对每一个文档而言，θ是一个长度为 K 的向量，用于描述当前文档在 K 个主题上的分布。

产生过程告诉我们，我们对于文档中的每一个字，都先从这个**θ向量中产生一个下标**，用于告诉我们现在要==从主题矩阵中的哪一行去生成当前的字==。





## 百度百科版本

LDA（Latent Dirichlet Allocation）是一种==文档主题生成模型==，也称为一个三层贝叶斯概率模型，包含**词、主题和文档**三层结构。

所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到。

==文档到主题服从多项式分布，主题到词服从多项式分布==。



LDA是一种**非监督**机器学习技术，可以用来识别大规模文档集（document collection）或语料库（corpus）中潜藏的主题信息。

它采用了词袋（bag of words）的方法，这种方法将每一篇文档视为一个词频向量，从而将文本信息转化为了易于建模的数字信息。

但是词袋方法**没有考虑词与词之间的顺序**，这简化了问题的复杂性，同时也为模型的改进提供了契机。

- 每一篇文档代表了一些主题所构成的一个概率分布，
- 而每一个主题又代表了很多单词所构成的一个概率分布。

[查看详情](https://baike.baidu.com/item/LDA/13489644)

 

## 维基百科版本

在自然语言处理中，潜在Dirichlet分配（LDA）是一种生成统计模型，它允许未观察到的组解释观察集，解释为什么数据的某些部分是相似的。

例如，如果观察是收集到文档中的单词，则假定**每个文档是少量主题的混合**，并且每个单词的存在可归因于文档的主题之一。

LDA是主题模型的示例。

[查看详情](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)