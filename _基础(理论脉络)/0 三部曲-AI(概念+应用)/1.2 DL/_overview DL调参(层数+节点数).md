[2018.07.31](http://www.atyun.com/25874.html)



人工神经网络有两个重要的超参数，用于控制网络的体系结构或拓扑：层数和每个隐藏层中的节点数。

配置网络时，必须指定这些参数的值。

为你的特定预测建模问题配置这些超参数的最可靠方法是通过**强大的测试工具系统实验**。



对于初学者来说，寻找一种分析方法来计算最佳层数和节点数，或者遵循简单的经验法则，可能是一个很难接受的机器学习领域。

### **多层感知器**

节点，也称为神经元或感知器，是具有一个或多个权重输入连接的计算单元，它以某种方式连接输入的转移函数，并且连接输出。

然后将节点组织成层以构成网络。



单层人工神经网络，也简称为单层，顾名思义，具有单层节点。

> 单层网络只有一层活动的单元。输入通过单层权重直接连接到输出。
>
> 输出不相互影响，因此具有N个输出的网络可被视为N个分离的单输出网络。



单层网络可以扩展到多层网络，也曾为称为多层感知器。多层感知器(MLP)是具有不止一层的人工神经网络。

它有一个连接到输入变量的输入层，一个或多个隐藏层，以及一个产生输出变量的输出层。

> 标准多层感知器（MLP）是单层感知器的连接在一起。存在一层输入节点，一层输出节点和一个或多个中间层。中间层也被称为“隐藏层”，因为它们不能直接从系统输入和输出中观察到。

我们可以总结MLP中层的类型如下：

- **输入层**：输入变量，有时称为可见层。
- **隐藏层**：输入和输出层之间的节点层。这些层可能存在一个或多个。
- **输出层**：生成输出变量的节点层。



最后，以下是用于描述神经网络形状和能力的一些术语：

- **尺寸**：模型中的节点数。
- **宽度**：特定层中的节点数。
- **深度**：神经网络中的层数。
- **能力**：==可以通过网络配置学到的函数的类型或结构。有时被称为“ 表征能力 ”==。
- **架构**：网络中层和节点的具体排列。



### **如何计算层？**

过去，对于如何计算层数存在一些分歧。

分歧的核心在于输入层是否被计算在内。

有一种观点认为不应该计算它，因为输入并不活动，它们只作输入变量。我们将使用这个惯例; 这也是《Neural Smithing》 一书中推荐的惯例。

因此，具有输入层，一个隐藏层和一个输出层的MLP是2层MLP！



这种方便的表示法表述了每层的层数和节点数。每个层中的节点数被指定为一个整数，从输入层到输出层，每个层的尺寸由一个正斜线字符(/)分隔。

例如，输入层中具有**两个变量**的网络，有一个具有**八个节点**的隐藏层和具有**一个节点**的输出层使用符号来描述为：2/8/1。

我建议在描述多层感知器神经网络的层及其尺寸时使用此表示法。



### **为什么要有多个层？**

在我们查看要指定的层数之前，有必要先思考为什么我们希望拥有多个层。

单层神经网络只能用于表示线性可分离的函数。

也就是说非常简单的问题，例如，分类问题中可以被一行整齐地分隔开的两个类。如果你的问题相对简单，那么单层网络就足够了。

然而，我们有兴趣解决的大多数问题都不是线性可分的。



多层感知器可用于表示凸区域。

这意味着，实际上，他们可以学习在一些高维空间中围绕实例绘制形状，以对它们进行分类，从而克服线性可分性的限制。

实际上，Lippmann在1987年的论文“An introduction to computing with neural nets ”中有一个理论发现，它表明具有两个隐藏层的MLP足以创建任何所需形状的分类区域。

这很有启发性，但应该注意的是，没有给出每层中使用多少节点或如何学习权重的指示。

==进一步的理论发现和证明已经显示MLP是万能逼近器==。

有了一个隐藏层，MLP就可以逼近我们需要的任何函数。

> 万能逼近定理表明：只要有**足够的隐藏节点**，具有**线性输出层**和**至少一个具有任何“压缩”激活函数（如，logistic sigmoid）的隐藏层**的前馈网络可以从一个有限维空间到另一个有限维空间**有任意的非零误差逼近**任何波莱尔可测函数。

– 第198页，Deep Learning，2016年。



这是一个经常被引用的理论发现，关于它的文献很多。

在实践中，我们同样不知道在给定问题的单个隐藏层中要使用多少节点，也不知道如何有效地学习或设置其权重。

此外，已经出现了许多反例，有些函数不能通过单个隐藏层的MLP直接学习或者需要无限数量的节点。



即使对于那些可以通过足够大的单隐藏层MLP学习的函数，使用两个（或更多）隐藏层来学习它也会更有效。

> 既然一个足够大的隐藏层足以近似大多数函数，为什么还有人会使用更多呢？
>
> 其中一个原因在于“足够大”这个词。
>
> 虽然单个隐藏层对于某些函数是最佳的，但是与有更多层的解决方案相比，单隐藏层解决方案的效率非常低。

– 第38页，Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks，1999。



### **要使用多少层和节点？**⭐

有了前面的铺垫，让我们来处理你真正的问题。

应该在多层感知器中使用多少层，每层有多少个节点？



在本节中，我们将列举解决此问题的五种方法。

#### **1）实验**

一般来说，当我被问到用于MLP的层数和节点数时，我经常回复：

> 我不知道，你**要使用系统的实验**来发现对特定数据集最有效的方法。

我仍然坚持这个答案。



通常，你无法分析计算人工神经网络中每层使用的层数或节点数，以解决特定的实际预测建模问题。

每层中的层数和节点数是必须指定的模型超参数。

你可能是第一个尝试使用神经网络解决自己的特定问题的人。

在你之前没有人解决过它。因此，没有人能告诉你如何配置网络的正确答案。



你必须使用**强大的测试工具**和**受控实验**来发现答案。推荐文章：

- https://machinelearningmastery.com/evaluate-skill-deep-learning-models/

无论你遇到什么样的启发式方法，所有答案都会回到需要仔细的实验来查看哪种方法最适合你的特定数据集。



#### **2）直觉**

网络可以通过直觉进行配置。

例如，你可能有直觉认为需要深层网络来解决特定的预测建模问题。

**深度模型提供了层次结构**，这种层次构建了从输入变量空间到输出变量的不断增加的抽象级别。

如果对问题域有了一定的了解，我们可能认为需要深层次模型来充分解决预测问题。

在这种情况下，我们可以选择具有多层深度的网络配置。

> 选择深度模型编码了一个非常常见的信念，即我们想要学习的函数应该包含几个更简单函数的组合。
>
> 这可以从表示学习的角度解释为我们认为学习问题由发现发现的一组**潜在的变异因素**构成，这些因素又可以用其他**更简单的变异潜在因素**来描述。

– 第201页，Deep Learning，2016年。



这种直觉可以来自领域的经验，神经网络建模问题的经验，或者两者都有。

根据我的经验，直觉常常被实验证明是无效的。



#### **3）去深度**

在他们重要的深度学习教科书中，Goodfellow，Bengio和Courville强调，在经验上，对于感兴趣的问题，深度神经网络似乎表现得更好。

具体而言，他们表示在深度可能直观有益的情况下，选择使用深度神经网络作为统计论据。

> 从经验上看，深度越大，对各种任务的归纳就越好。[…]这表明使用深层架构确实在模型学习的函数空间上表达了一个有用的先验。

– 第201页，Deep Learning，2016年。



我们可以使用这个论点来建议使用深层网络，具有多层的网络，可能是一种配置网络的启发式方法，以应对具有挑战性的预测建模问题。

这与从随机森林和随机梯度提升开始对预测建模问题的建议类似，利用表格数据，在测试其他方法之前==快速地**了解模型技能的上限**==。



#### **4）借用思想**

一种简单但可能很耗时的方法是==利用文献中报道的研究结果==。

查找研究论文，描述在预测问题的实例上使用MLP，以类似方式解决你的问题。

请注意这些文章中使用的网络配置，并将它们作为测试自己问题的配置起点。

模型超参数的可转移性导致从一个问题到另一个问题的巧妙的模型，这是一个具有挑战性的开放问题，并且这就是模型超参数配置比艺术更具艺术性的原因。

然而，在相关问题上使用的网络层和节点数量是测试想法的良好起点。



#### 5）搜索

设计自动搜索以测试不同的网络配置。你可以用**文学和直觉的想法**来进行搜索。

一些流行的搜索策略包括：

- **随机**：尝试每层的层和节点的随机配置。
- **网格**：尝试系统地搜索每个层的层数和节点数。
- **启发式**：==尝试在配置上进行定向搜索，如遗传算法或贝叶斯优化==。
- **穷举**：尝试所有层的组合和节点的数量；它可能适用于**小型网络和数据集**。



对于大型模型，大型数据集以及或者都大，这可能具有挑战性。

减少或管理计算负担的一些想法包括：

- 在训练数据集的**较小子集上拟合模型以加速搜索**。
- 严格限制搜索空间的大小。
- **跨多个服务器实例**==并行化搜索==（例如，使用Amazon EC2服务）。

如果时间和资源允许，我建议系统化的搜索。



# 拓展阅读

如果希望深入了解，本节将提供有关该主题的更多资源。

#### **论文**

- https://ieeexplore.ieee.org/abstract/document/1165576/
- https://www.tandfonline.com/doi/abs/10.1080/01431160802549278

#### **书**

- https://amzn.to/2vhyW8j
- https://amzn.to/2IXzUIY

#### **文章**

- https://en.wikipedia.org/wiki/Artificial_neural_network
- https://en.wikipedia.org/wiki/Universal_approximation_theorem
- http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-9.html

# 总结

在这篇文章中，你将了解层和节点的作用，以及如何着手为你的预测建模问题配置多层感知器神经网络。

阅读这篇文章后，你会知道：

- 单层和多层感知器网络之间的区别。
- 在网络中拥有一个和多个隐藏层的价值。
- 配置网络中的层数和节点数的五种方法。