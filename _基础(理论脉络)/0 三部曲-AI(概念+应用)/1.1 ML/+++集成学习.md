- [什么是集成学习？](https://easyai.tech/ai-definition/ensemble-learning/#what)
- [Bagging](https://easyai.tech/ai-definition/ensemble-learning/#bagging)
- [Boosting](https://easyai.tech/ai-definition/ensemble-learning/#boosting)
- [Bagging 和 Boosting 的4 点差别](https://easyai.tech/ai-definition/ensemble-learning/#qubie)
- [百度百科和维基百科](https://easyai.tech/ai-definition/ensemble-learning/#wiki)
- [扩展阅读](https://easyai.tech/ai-definition/ensemble-learning/#links)



> 在机器学习中，我们讲了很多不同的算法。那些算法都是单打独斗的英雄。而集成学习就是将这些英雄组成团队。实现“3 个臭皮匠顶个诸葛亮”的效果。
>
> 本文将介绍集成学习的 2 种主要思路：bagging、boosting。

 

## 什么是集成学习？

**集成学习归属于机器学习，他是一种「训练思路」，并不是某种具体的方法或者算法。**

现实生活中，大家都知道「人多力量大」，「3 个臭皮匠顶个诸葛亮」。

而集成学习的核心思路就是「人多力量大」，它并没有创造出新的算法，而是把已有的算法进行结合，从而得到更好的效果。

![img](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-13-weizhi.png)

集成学习会挑选一些简单的基础模型进行组装，组装这些基础模型的思路主要有 2 种方法：

1. bagging（bootstrap aggregating的缩写，也称作“套袋法”）
2. boosting

 

## Bagging

**Bagging 的核心思路是——民主。**

Bagging 的思路是所有基础模型都一致对待，每个基础模型手里都只有一票。然后使用民主投票的方式得到最终的结果。

大部分情况下，**经过 bagging 得到的结果方差（variance）更小**。

![bagging的具体过程](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-10-17-bagging.png)

**具体过程：**

1. 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
2. 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
3. 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

**举例：**

在 bagging 的方法中，最广为熟知的就是随机森林了：bagging + 决策树 = 随机森林

《[一文看懂决策树（3个步骤+3种典型算法+10个优缺点）](https://easyai.tech/ai-definition/decision-tree/)》

《[一文看懂随机森林（4个步骤+4种方式评测+10个优缺点）](https://easyai.tech/ai-definition/random-forest/)》

 

## Boosting

**Boosting 的核心思路是——挑选精英。**

Boosting 和 bagging 最本质的差别在于他对基础模型不是一致对待的，而是经过不停的考验和筛选来挑选出「精英」，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。

大部分情况下，**经过 boosting 得到的结果偏差（bias）更小**。

![boosting的具体过程](https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-10-17-boosting.png)

**具体过程：**

1. 通过加法模型将基础模型进行线性的组合。
2. 每一轮训练都提升那些错误率小的基础模型权重，同时减小错误率高的模型权重。
3. 在每一轮改变训练数据的权值或概率分布，通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

**举例：**

在 boosting 的方法中，比较主流的有 [Adaboost](https://easyai.tech/ai-definition/adaboost/) 和 Gradient boosting 。

《[一文看懂 Adaboost 以及它的优缺点](https://easyai.tech/ai-definition/adaboost/)》

 

## Bagging 和 Boosting 的4 点差别

### 1 **样本选择上：**

Bagging：训练集是在原始集中**有放回选取的**，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中**每个样例在分类器中的权重发生变化**。而权值是根据上一轮的分类结果进行调整。



### 2 **样例权重：**

Bagging：使用均匀取样，每个样例的权重相等

Boosting：**根据错误率不断调整样例的权值**，错误率越大则权重越大。



### 3 **预测函数：**

Bagging：所有预测函数的权重相等。

Boosting：每个**弱分类器都有相应的权重**，对于分类误差小的分类器会有更大的权重。



### 4 **并行计算：**

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

差别部分内容转自《[Bagging和Boosting 概念及区别](https://www.cnblogs.com/liuwu265/p/4690486.html)》



# [5个集成学习最常用的方法](https://easyai.tech/blog/5-way-ensemble-learning/)

在自然界中，一群树组成一片森林。

假设训练一组树形判定分类器，根据训练数据集的不同子集进行个体预测。

后来考虑到大多数人对相同情况的预测，你预测到了预期的观察类。这意味着，你基本上是在使用树形判定分类法的知识集合，它通常被称为随机森林。



本文将介绍最流行的集成方法，包括bagging、boosting、stacking等。在深入讨论之前，请谨记：

> 当预测器尽可能相互独立时，集成方法的效果最好。
>
> 获得不同分类器的方法之一是使用完全不同的算法来训练他们。这增加了其产生不同类型误差的机会，从而提高了集成的准确性。
>
> ——摘自《使用Scikit-Learn&TensorFlow进行机器学习》第7章

## **简单集成技术**

### 1 **Hard Voting 分类器**

这是该技术最简单的例子，你可能已经熟练掌握。

投票分类器通常用于分类问题。假设你已训练并将一些分类器(逻辑回归分类器，[SVM](https://easyai.tech/ai-definition/svm/)分类器，随机森林分类器等)与训练数据集进行了匹配。

创建一个更好的分类器的简单方法是**将每个分类器所做的预测集合起来，将选择的多数作为最终预测**。

基本上，我们可以将其视为检索所有预测器的模式。

![img](https://easy-ai.oss-cn-shanghai.aliyuncs.com/img/006tNc79gy1fzpmmx8wkyj30fe095aam.jpg)

### 2 **平均**

上述第一个例子主要用于分类问题。

现在，我们来看一种用于回归问题的技术——平均。

与Hard Voting类似，我们采用不同的算法进行多次预测，取其平均值进行最终预测。



### 3 **加权平均**

这种方法是指在求平均数时，根据模型对最终预测的重要性，给模型分配不同的权重。



## **高级集成技术**

### **Stacking**

这种技术也被称为**堆栈泛化**，它基于训练模型的思想，将执行我们之前看到的常规集合。

![img](https://easy-ai.oss-cn-shanghai.aliyuncs.com/img/006tNc79gy1fzpmnj4nwrj30fe04zaaa.jpg)

我们有N个预测器，每个都进行预测并返回一个最终值。

之后，元学习者或 Blender ==将这些预测作为输入并做出最终预测==。

让我们看看它是如何工作的。

![img](https://easy-ai.oss-cn-shanghai.aliyuncs.com/img/006tNc79gy1fzpmnrmb74j30fe09ngm8.jpg)

![img](https://easy-ai.oss-cn-shanghai.aliyuncs.com/img/006tNc79gy1fzpmnwgx3yj30fe0f7q42.jpg)

训练元学习者的**常用方法是留出集**。

首先，将训练集分成两个子集。

- 第一个数据子集用来训练预测器。

- 之后，用训练第一个子集的预测器来**对第二个子集（即留出集）进行预测**。这样可以确保预测的洁净度，因为这些算法从未进行过数据处理。
  - 通过这些新的预测，我们可以生成一个新的训练集，将其作为输入特征(**使新的训练集三维化**)并保持目标值。
  - 最后在这个新的数据集上训练**元学习者**Blender，并在**考虑到第一次预测给出的值**的情况下预测目标值。



此外，你可以用这种方法**训练好几个元学习者**(例如，一个使用线性回归，另一个使用随机森林回归，等等)。

要使用多个元学习者，必须将**训练集分为三个或以上的子集**：

- 第一个子集用于训练第一层预测器，
- 第二个用于**对未知数据进行预测**并**创建新的数据集**，
- 第三个用于训练元学习者并对目标值进行预测。



### **Bagging 与 Pasting**

另一种方法是对每个预测器（如树形判定分类法）采用相同的算法，然而，训练集的不同随机子集可以得到更全面的结果。

![img](https://easy-ai.oss-cn-shanghai.aliyuncs.com/img/006tNc79gy1fzpmo5i9utj30fe07ut98.jpg)

​																				摘自文章《利用适应性随机森林减少降水的空间尺度》



至于子集的创建，可以进行替换 ，也可以不替换。

如果假设存在替换，那么有些观测结果可能出现在多个子集中，这就是我们为什么将这个方法称为bagging（ bootstrap aggregating的缩写）的原因。

而在不进行替换的情况下进行抽样时，我们确保每个子集中的所有观察值都是唯一的，从而保证每个子集中不会出现重复的观察值。



一旦所有的预测器训练完毕，集合就可以通过聚合所有训练过的预测器的预测值来对一个新实例进行预测，就像我们在hard voting分类器中看到的那样。虽然每个个体预测器的偏差都高于在原始数据集上训练的预测器，但是聚合可以减少偏差和方差。

使用pasting则可能使这些模型得到相同的结果，因为它们的输入相同。

与pasting相比，bootstraping(有放回)的每个子集更具多样性，因此用bagging所得到的偏差更大，这意味着预测器之间的相关性更小，从而降低了集成的方差。

总之， bagging 模型通常能提供更好的结果，这就解释了为什么一般而言bagging比pasting更常见。



### **Boosting**

如果第一个模型和下一个模型(可能是所有模型)对一个数据点的预测都不正确，那么其结果的组合会提供更好的预测吗？这就是Boosting的作用所在。

Boosting也被称为Hypothesis Boosting，是指任何可以将学习能力弱者组合成学习能力强者的集成方法。

这是一个连续的过程，**每个模型都试图修正上一个模型的错误**。

![img](https://easy-ai.oss-cn-shanghai.aliyuncs.com/img/006tNc79gy1fzpmotp4hfj30fe055jro.jpg)

每个模型不会在整个数据集上都表现良好。

然而，在数据集的某些部分它们确实表现得很好。因此，通过Boosting，我们可以期待每个模型都能有助于实际提高整个集成的性能。



## **基于Bagging和Boosting的算法**

最常用的集成学习技术是Bagging和Boosting。

下面是这些技术中一些最常见的算法。

**Bagging 算法:**

· 随机森林（https://medium.com/diogo-menezes-borges/random-forests-8ae226855565）

**Boosting 算法:**

· [AdaBoost](https://easyai.tech/ai-definition/adaboost/)（https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81）

· Gradient Boosting Machine (GBM)（https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81）

· XGBoost（https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81）

· Light GBM（https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81）





# #参考文献

- [粒子群算法（Particle swarm optimization | PSO）](https://easyai.tech/ai-definition/particle-swarm-optimization/)
- [模糊神经网络（Neuro-fuzzy | FNN）](https://easyai.tech/ai-definition/fnn/) 1
- [人工神经网络 - Artificial Neural Network | ANN](https://easyai.tech/ai-definition/ann/)
- [迁移学习（Transfer learning）](https://easyai.tech/ai-definition/transfer-learning/) 1
- [遗传算法（Genetic algorithm | GA）](https://easyai.tech/ai-definition/genetic-algorithm/)



【实践】[5个集成学习最常用的方法](https://easyai.tech/blog/5-way-ensemble-learning/)