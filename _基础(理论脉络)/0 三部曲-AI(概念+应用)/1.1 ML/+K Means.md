## K均值聚类的优缺点

**优点**

- 算法简单，容易实现 ；
- 算法速度很快；
- 对==处理大数据集，该算法是相对可伸缩的和高效率的==，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目, t是迭代的次数。
  - 通常k<<n。这个算法**通常局部收敛。**
- 算法尝试找出使平方误差函数值最小的k个划分。
  - 当簇是==密集的、球状或团状的，且簇与簇之间区别明显时==，聚类效果较好。



**缺点**

- 对数据类型要求较高，适合**数值型数据**；
- 可能收敛到局部最小值，在**大规模数据上收敛较慢**
- 分组的数目k是一个输入参数，**不合适的k可能返回较差的结果**。
- 对**初值的簇心值敏感**，对于不同的初始值，可能会导致不同的聚类结果；
- ==不适合于发现非凸面形状的簇==，或者大小差别很大的簇。
- **对于”噪声”和孤立点数据敏感**，少量的该类数据能够对平均值产生极大影响。



## 百度百科版本

K均值聚类算法是先随机选取K个对象作为初始的聚类中心。

然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。

聚类中心以及分配给它们的对象就代表一个聚类。

一旦全部对象都被分配了，每个聚类的聚类中心会根据聚类中现有的对象被重新计算。

这个过程将不断重复直到满足某个终止条件。

终止条件可以是

- 没有（或最小数目）对象被重新分配给不同的聚类，
- 没有（或最小数目）聚类中心再发生变化，
- 误差平方和局部最小。

[查看详情](https://baike.baidu.com/item/K均值聚类算法)

 

## 维基百科版本

ķ -means聚类是的方法的矢量量化，**最初来自信号处理**，即流行聚类分析在数据挖掘。

ķ -means聚类的目的是划分 Ñ观测到 ķ其中每个观测属于簇群集与最近的平均值，作为原型群集的。这导致数据空间划分为 Voronoi单元。

问题在计算上很困难（NP难）; 



然而，**有效的启发式算法**快速收敛到局部最优。

这些通常是类似于最大期望算法为混合物的高斯分布经由通过两个采用的迭代细化方法k-均值和高斯混合模型。

他们都使用集群中心来建模数据; 

然而，k -means聚类倾向于找到具有可比空间范围的聚类，而期望最大化机制**允许聚类具有不同的形状**。



该算法与k最近邻分类器有松散的关系，这是一种流行的分类机器学习技术，由于名称的原因，它经常与k -means 混淆。

应用1最近邻分类器，通过k -means 获得的聚类中心将新数据分类到现有聚类中。这被称为最近的质心分类器或Rocchio算法。

[查看详情](https://en.wikipedia.org/wiki/K-means_clustering)





# #参考文献

- [朴素贝叶斯 - Naive Bayes classifier | NBC](https://easyai.tech/ai-definition/naive-bayes-classifier/)
- [模糊神经网络（Neuro-fuzzy | FNN）](https://easyai.tech/ai-definition/fnn/) 1
- [前馈神经网络（Feedforward neural network）](https://easyai.tech/ai-definition/feedforward-neural-network/)
- [长短期记忆网络 - Long short-term memory | LSTM](https://easyai.tech/ai-definition/lstm/) 8
- [启发式算法 - Heuristic](https://easyai.tech/ai-definition/heuristic/)