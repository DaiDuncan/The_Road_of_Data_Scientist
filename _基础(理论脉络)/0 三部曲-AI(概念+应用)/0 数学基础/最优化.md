# [约束优化](https://easyai.tech/ai-definition/constrained-optimization/)

约束优化(Constrained Optimization)，即约束优化问题，是优化问题的分支。

它是在一系列约束条件下，**寻找一组参数值，使某个或某一组函数的目标值(成本函数或能量函数)达到最优**。

其中约束条件既可以是等式约束也可以是不等式约束。

寻找这一组参数值的关键可是：满足约束条件和目标值要达到最优。

求解约束问题的方法可分为传统方法和进化算法。





# [梯度下降法](https://easyai.tech/ai-definition/gradient-descent/)

Gradient descent

## 什么是梯度下降法？

![梯度下降法](https://easyai.tech/wp-content/uploads/2019/01/tiduxiajiang-1.png)

梯度下降算法的公式非常简单，”**沿着梯度的反方向（坡度最陡）**“是我们日常经验得到的，其本质的原因到底是什么呢？

为什么局部下降最快的方向就是梯度的负方向呢？也许很多朋友还不太清楚。



**我们以爬上山顶为例**

假设我们位于一座山的山腰处，没有地图，并不知道如何到达山顶。

于是决定走一步算一步，也就是每次沿着当前位置最陡峭最易上山的方向前进一步，然后继续沿下一个位置最陡方向前进一小步。

这样一步一步走下去，一直走到觉得我们已经到了山顶。

这里通过最陡峭的路径上山的方向就是梯度。

 

## 百度百科

梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 

要使用梯度下降法找到一个函数的局部极小值，必须向函数上==当前点对应梯度（或者是近似梯度）的反方向==的规定步长距离点进行迭代搜索。

如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。

[查看详情](https://baike.baidu.com/item/梯度下降法)

 

## 维基百科

梯度下降是用于找到函数最小值的一阶 迭代 优化 算法。为了使用梯度下降找到函数的局部最小值，需要采用与当前点处函数的梯度（或近似梯度）的负值成比例的步长。

相反，如果采用与梯度的正值成比例的步长，则接近该函数的局部最大值 ; 然后将该过程称为梯度上升。

梯度下降也称为最陡下降。但是，梯度下降不应与最速下降的最速下降方法相混淆。

[查看详情](https://en.wikipedia.org/wiki/Gradient_descent)



# [随机梯度下降](https://easyai.tech/ai-definition/stochastic-gradient-descent/)

Stochastic gradient descent | SGD

随机梯度下降（通常缩短为SGD），也称为**增量**梯度下降，是用于优化可微分目标函数的迭代方法，梯度下降优化的随机近似。

2018年的一篇文章暗示Her[bert](https://easyai.tech/ai-definition/bert/) Robbins和Sutton Monro在其1951年题为“随机近似方法”的文章中发展SGD。

它被称为随机的 

- 因为==样本是随机选择==（或混洗）而不是作为单个组（如标准梯度下降）或按训练集中出现的顺序选择的。

