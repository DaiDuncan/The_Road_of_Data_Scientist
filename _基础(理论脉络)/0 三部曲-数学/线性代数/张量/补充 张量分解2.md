奇异值分解作为矩阵分解中最基本的方法，通常都是作用于完整的矩阵（即矩阵中不存在元素缺失的现象），即要求矩阵必须是稠密矩阵

在实际应用中，我们能够得到的很多矩阵都是稀疏的或者说是部分位置上的元素出现缺失，那么，如何对稀疏矩阵进行奇异值分解呢？



# 导读|矩阵分解(涉及正交矩阵)

在[数值分析](https://zh.wikipedia.org/wiki/数值分析)，矩阵分解常常用来实现一些矩阵运算的快速[算法](https://zh.wikipedia.org/wiki/算法)。

对称阵特征向量分解的基础是[谱分析](https://zh.wikipedia.org/w/index.php?title=谱分析&action=edit&redlink=1)，而奇异值分解则是谱分析理论在==任意矩阵==上的推广



- [LU分解](https://zh.wikipedia.org/wiki/LU分解)：
  - L：下三角矩阵
  - U：上三角矩阵
  - 正定矩阵 => [Cholesky分解](https://zh.wikipedia.org/wiki/Cholesky分解) $A=LL^*$

- QR分解：M = QR, Q正交，R上三角
- 特征值分解/谱分解：$S = QΛQ^T$, S对称，Q正交，Λ对角。
  - [谱定理](https://zh.wikipedia.org/wiki/%E8%B0%B1%E5%AE%9A%E7%90%86)：当M是一个正规矩阵（因而必须是方阵）根据谱定理，M可以被一组特征向量酉对角化
  - 相比于SVD：Q不需要是酉矩阵，$\Lambda$也不需要是半正定的(奇异值非负特征值的平方根)
- ==奇异值SVD分解==：$M = UΣV^T$, U和V都是酉矩阵，Σ**非负**对角。
- 极分解：M = QS, ==Q正交，S对称半正定==。







## 奇异值SVD分解(非方阵)

不要求矩阵是实的，也不要求矩阵是方阵，并且拥有比满秩分解更好的特性



奇异值：

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210223204015.png" alt="image-20210223204015555" style="zoom:80%;" />



奇异值分解：$$A_{m \times n}=U \Sigma V^H$$

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224095400.png" alt="image-20210224095400446" style="zoom: 67%;" />

矩阵Σ的对角线上的元素等于M的奇异值，==U和V的列==分别是奇异值中的左、右奇异向量

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224100207.png" alt="image-20210224100207737" style="zoom: 67%;" />

- 1 酉矩阵$U_{m \times m}$：正交矩阵推广到复数域
  - U的列（columns）组成一套对M的正交"输出"的基向量。这些向量是$MM^*$的特征向量

- 2 $\Sigma_{m \times n}$：非负实数对角矩阵
  - 对角线上的元素是奇异值，视为是在输入与输出间进行的标量的"膨胀控制"
  - 这些是$MM^*$及的$M^*M$的特征值的==非负平方根==，并与U和V的行向量相对应
- 3 酉矩阵$V_{n \times n}^H$
  - $VV^H=V^HV=E$
  - V的列（columns）组成一套对M的正交"输入"或"分析"的基向量。这些向量是$M^*M$的特征向量

备注：基于酉矩阵$$U^HU=E$$ => 奇异值分解也可表示为$$\Sigma = U^HAV$$



### 理论推导过程：

注意：关键公式|$A^TA=V\Sigma^2V^T$

- 将$V^T$看作一个整体
- 由于V是酉矩阵(此处是实数：正交矩阵) 
  - $V=(V^T)^T =(V^T)^{-1} $
- => 上述公式==可看作相似变换==：$M=P^{-1}\Lambda P$
- 相对应的：$A^TA$的特征向量组成了矩阵$V^T$ => 这也是SVD公式中用的是$V^T$

![image-20210224113436596](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224113436.png)

1 $A^TA$是一个$n \times n$方阵：可以进行特征分解 => 特征向量span成矩阵V(这些特征向量：A的右奇异向量)

2 $AA^T$是一个$ m \times m$方阵 =>特征向量span成矩阵U(这些特征向量：A的左奇异向量)

3 Σ除了对角线上是奇异值其他位置都是0：只需要求出每个奇异值σ就可以了

![image-20210224112954802](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224112955.png)





### 示例|计算

![image-20210224114310566](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224114310.png)









### 性质定理

1 定理：若矩阵A的奇异值分解为$$A=U \Sigma V^H$$，那么$$A^H=V \Sigma U^H$$是$$A^H$$的奇异值分解

2 





### 应用

SVD作为一个很基本的算法，在很多机器学习算法中都有它的身影，特别是在现在的大数据时代，由于SVD可以实现并行化，因此更是大展身手。SVD的原理不难，只要有基本的线性代数知识就可以理解，实现也很简单因此值得仔细的研究。当然，SVD的缺点是分解出的矩阵解释性往往不强，有点黑盒子的味道，不过这不影响它的使用。



1 求广义逆阵（伪逆）

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224100948.png" alt="image-20210224100948790" style="zoom:67%;" />



2 列空间、零空间和秩

奇异值分解的另一个应用是给出矩阵的[列空间](https://zh.wikipedia.org/wiki/行空間與列空間)、[零空间](https://zh.wikipedia.org/wiki/零空間)和[秩](https://zh.wikipedia.org/wiki/秩_(线性代数))的表示。

- 对角矩阵$\Sigma$的非零对角元素的个数对应于矩阵M的秩
- 与非零奇异值对应的左奇异向量则生成矩阵M的列空间
- 与零奇异值对应的右奇异向量生成矩阵M的零空间

在线性代数数值计算中奇异值分解一般用于确定矩阵的有效秩，这是因为，由于舍入误差，秩亏矩阵的零奇异值可能会表现为很接近零的非零值。



3 矩阵近似值PCA

数据集的特征值（在SVD中用奇异值表征）按照重要性排列，降维的过程就是舍弃不重要的特征向量的过程，而剩下的特征向量张成空间为降维后的空间。

- 在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例



由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。

也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。

同时也可以用于NLP中的算法，比如潜在语义索引（LSI）



```matlab
[b c d]=svd(x)
```

```c++
void cvSVD( CvArr* A, CvArr* W, CvArr* U=NULL, CvArr* V=NULL, int flags=0 )
```

```python
U,s,Vh = scipy.linalg.svd(A)
```

```python
S=svd(x)
```





## SVD的局限 => truncated SVD

奇异值分解得到的三组件并没有体现出降维过程（dimensionality reduction），如果给定一个大小为$20000 \times 10000$的矩阵（已经很大了），我们会发现通过奇异值分解得到三个矩阵Q，$\Sigma$，P大小依次是：$20000 \times 20000$，$20000 \times 10000$，$10000 \times 10000$，此时，分解出来的三组件更大了，并且加重了计算机的存储负担，我们不禁会问：奇异值分解的降维处理体现在哪里呢？

事实上，奇异值分解的降维处理主要体现在其低秩逼近问题（low-rank approximation problem）上，在这里，奇异值分解的低秩逼近也被称为截断奇异值分解（truncated SVD），只选取前$k \le min(m,n)$个最大的奇异值和其对应的特征向量，低秩逼近问题将奇异值分解表达为$A \approx U_k \Sigma_k V_k^T$

这样便可大大节约了计算机的存储开销。

其中，MATLAB提供了奇异值分解和其低秩逼近的函数，分别是svd和svds





## 应用|SVD进行matrix completion

对稀疏矩阵进行奇异值分解有很多方法，Charu C. Aggarwal在其著作《Recommender systems》第115页提到了一种简单的迭代式分解方法

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224133905.png" alt="image-20210224133905282" style="zoom:80%;" />

值得一提的是，随着感知技术的发展，智能交通系统能够通过大量固定感知器（如地感线圈、高清卡口等）和移动感知器（如浮动车）采集交通数据，但在数据采集过程中，往往会发生由于检测器故障或通讯中断而导致的数据缺失，因此，上述“？”位置上==缺失数据的填补==也就成了相应的核心问题。

另外，交通流量与时间和空间高度相关，利用稀疏矩阵的奇异值分解可以用于提取交通特征。



稀疏矩阵的奇异值分解将分为两部分进行讲解：

- 缺失元素的初始化填补
- 迭代的奇异值分解



### 1 缺失元素的初始化填补

如果将缺失元素直接看作0，虽然也可以进行奇异值分解，但是各行或者各列缺失程度不同会导致分解出来的结果并不是我们所期望的



因此，这就往往需要先对缺失元素做一个“粗糙的”填补，最简单地，我们可以将缺失元素用所在行或者列的均值进行填补，即所有元素缺失的位置都用均值（即$\mu =401$）填补。

填补后的RMSE(the root mean squared error)，结果为266.16：

- 真实值是上述的$A_{real}$

![image-20210224160855019](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224160855.png)



#### 方法1）基于行、列的偏置

- $b_i^{(1)}$表示第i行相对于均值$\mu$产生的偏置(bias)
- $b_j^{(2)}$表示第j列相对于均值$\mu$产生的偏置(bias)

![image-20210224161019649](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224161019.png)



构造的优化问题：$min \ L_1 = \frac{1}{2}\sum_{(i, j) \in S}(a_{ij}-\hat{a}_{ij})^2$

- 将偏置项![[公式]](https://www.zhihu.com/equation?tex=b_i%5E%7B%5Cleft%28+1+%5Cright%29+%7D)和![[公式]](https://www.zhihu.com/equation?tex=b_j%5E%7B%5Cleft%28+2+%5Cright%29+%7D)视为无约束优化的优化问题的决策变量

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224161358.png" alt="image-20210224161358644" style="zoom:80%;" />

```matlab
A_sparse=[0,90,449,517;0,0,412,0;192,0,697,687;185,0,699,657;164,58,0,0];
A_real=[208,90,449,517;104,43,412,411;192,77,697,687;185,115,699,657;...
    164,58,696,599];
    
pos1=find(A_sparse==0);pos2=find(A_sparse~=0);[m,n]=size(A_real);
mu=mean(A_sparse(pos2));b1=0.1*rand(1,m);b2=0.1*rand(1,n);error=zeros(m,n);

alpha1=0.01;
for iter=1:1000
    for i=1:m
        for j=1:n
            A_hat(i,j)=round(mu+b1(1,i)+b2(1,j));
            if A_sparse(i,j)~=0
                error(i,j)=A_sparse(i,j)-A_hat(i,j);
            end
        end
    end
    b1=b1+alpha1*sum(error.T);b2=b2+alpha1*sum(error);	%更新参数：一个是行，一个是列
    L1(1,iter)=0.5*sum(sum(error.^2));	%每次迭代后的目标函数结果
end


RMSE=sqrt(sum((A_real(pos1)-A_hat(pos1)).^2)/length(pos1))
```

多次迭代后的结果：

![image-20210224161443243](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224161443.png)

相应的RMSE为110.65，从原来“？”位置的填补情况来看，这个结果并不理想（因为交通流量都是非负的）



#### 方法2）基于评分矩阵的分解$R=UV^T$

![image-20210224163230008](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224163230.png)

参数的更新公式：其中q=1,... ,r

![image-20210224163306074](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224163306.png)

```matlab
A_sparse=[0,90,449,517;0,0,412,0;192,0,697,687;185,0,699,657;164,58,0,0];
A_real=[208,90,449,517;104,43,412,411;192,77,697,687;185,115,699,657;...
    164,58,696,599];

pos=find(A_sparse==0);[m,n]=size(A_real);r=3;
U=0.1*rand(m,r);V=0.1*rand(n,r);error=zeros(m,n);
alpha2=0.0001;

for iter=1:1500
    A_hat=round(U*V');error=A_sparse-A_hat;error(pos)=0;
    U_plus=U+alpha2*error*V;
    V_plus=V+alpha2*error'*U;
    U=U_plus;V=V_plus;
    L2(1,iter)=0.5*sum(sum(error.^2));
end

RMSE=sqrt(sum((A_real(pos)-A_hat(pos)).^2)/length(pos))
```

其中，程序中的更新公式是：

- $U \leftarrow U + \alpha_2EV$
- $V \leftarrow V + \alpha_2E^TU$
- E是由$e_{ij}, i=1,...,5, j=1,...,4$构成的残差矩阵



多次迭代后的结果：

![image-20210224163415033](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224163415.png)

相应的RMSE为47.21



### 2 迭代的奇异值分解

Charu C. Aggarwal在其著作《Recommender systems》第115页给出的算法非常简单，将缺失元素初始化填补阶段得到的$\hat{A}$记为$A_f$

1. 对$A_f$进行奇异值分解，只选取前$k \le min(m,n)$个最大的奇异值和其对应的特征向量，得到$U_k\Sigma_k V_k^T$

2. 用$U_k\Sigma_k V_k^T$更新矩阵A(注意是最初的A，而不是$A_f$)==缺失位置的元素== => 记为$\hat{A}$
3. 迭代更新|若没有收敛，令$A_f=\hat{A}$，返回第一步



说明：

- 这个方法对初始化的$A_f$要求较高
- 另外，由于$U_k\Sigma_k V_k^T$在缺失位置的元素上会不同于$A_f$，最终收敛时会输出最终的$\hat{A}$



Matlab程序：加载所有元素缺失的位置都用均值（即$\mu=401$）填补的矩阵$\hat{A}$

- $U_k\Sigma_k V_k^T$在程序中表示为$Q_k\Sigma_k P_k^T$

```matlab
%% 基于均值 初始化缺失元素 => 直接得到的A_hat

A_sparse=[0,90,449,517;0,0,412,0;192,0,697,687;185,0,699,657;164,58,0,0];
A_real=[208,90,449,517;104,43,412,411;192,77,697,687;185,115,699,657;...
    164,58,696,599];
A_hat=[401,90,449,517;401,401,412,401;192,401,697,687;185,401,699,657;...
    164,58,401,401];
    
pos=find(A_sparse==0);[m,n]=size(A_real);k=2;
A_f=A_hat;

for iter=1:8000
    [Q,Sigma,P]=svds(A_f,k);	%截断SVD
    A_hat=Q*Sigma*P';
    A_f(pos)=A_hat(pos);		%仅更新最初缺失位置的元素
    convergence(1,iter)=sum(A_hat(pos).^2);	%仅计算缺失位置元素的收敛性
end

RMSE=sqrt(sum((A_real(pos)-A_f(pos)).^2)/length(pos))
```

相应的RMSE为161.85



```matlab
%% 基于行、列的偏置更新 => 计算得到的A_hat
A_sparse=[0,90,449,517;0,0,412,0;192,0,697,687;185,0,699,657;164,58,0,0];
A_real=[208,90,449,517;104,43,412,411;192,77,697,687;185,115,699,657;...
    164,58,696,599];
A_hat=[48,41,505,510;-44,-51,412,418;219,212,676,681;207,200,664,670;...
    115,107,571,577];
    
pos=find(A_sparse==0);[m,n]=size(A_real);k=2;
A_f=A_hat;

for iter=1:100
    [Q,Sigma,P]=svds(A_f,k);
    A_hat=Q*Sigma*P';
    A_f(pos)=A_hat(pos);
    convergence(1,iter)=sum(A_hat(pos).^2);
end

RMSE=sqrt(sum((A_real(pos)-A_f(pos)).^2)/length(pos))
```

相应的RMSE为110.15





```matlab
%% 基于奇异值更新 => 计算得到的A_hat
A_sparse=[0,90,449,517;0,0,412,0;192,0,697,687;185,0,699,657;164,58,0,0];
A_real=[208,90,449,517;104,43,412,411;192,77,697,687;185,115,699,657;...
    164,58,696,599];
A_hat=[139,89,449,517;114,46,412,408;192,75,697,687;186,59,699,657;...
    163,59,599,579];
    
pos=find(A_sparse==0);[m,n]=size(A_real);k=2;
A_f=A_hat;

for iter=1:500
    [Q,Sigma,P]=svds(A_f,k);
    A_hat=Q*Sigma*P';
    A_f(pos)=A_hat(pos);
    convergence(1,iter)=sum(A_hat(pos).^2);
end

RMSE=sqrt(sum((A_real(pos)-A_f(pos)).^2)/length(pos))
```

相应的RMSE为47.03



### 小结

通过迭代奇异值分解，缺失位置的元素会随着迭代过程而趋于收敛

另外，迭代奇异值分解过程有助于我们==提取矩阵行和列的特征==，并且左奇异向量间的正交特性和右奇异向量的正交特性能够==帮助我们找到主要的模式==。









# #参考文献

[Link: 张量分解3|如何对稀疏矩阵进行奇异值分解？](https://zhuanlan.zhihu.com/p/25512080)

主要参考了Charu C. Aggarwal于2016年出版的著作《Recommender systems》（链接：[Recommender Systems](https://link.zhihu.com/?target=http%3A//link.springer.com/book/10.1007/978-3-319-29659-3)）



[Link: 博客|SVD](https://www.cnblogs.com/pinard/p/6251584.html)