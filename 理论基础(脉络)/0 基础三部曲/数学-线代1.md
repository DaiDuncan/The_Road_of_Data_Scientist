# 向量

==波(声波、光波)都可以看做向量==

- 特征波作为基底
- 波由特征波叠加组成：即基底向量的线性组合
  - 幅值 <=> 数乘
  - $\vec{0}$表示直流
- 波的叠加
  - 干涉
    - 当两个音调结合在一起时，它们会发出拍子
    - 当光波在油膜中混合在一起时，它们会干涉并形成彩虹图案



实际意义：用来==高度抽象、概括==某些研究对象的本质特征，这样可以利用相关的性质和结论



## 向量空间

遇到问题：先要考虑是不是向量空间 [Link: 向量空间的基本性质⭐](https://brilliant.org/practice/waves-abstract-vectors/?p=3)

- 符号表示 |u>

定义：满足向量空间的几条性质

- 结合律，分配律
- 对加法、数乘封闭
- 零元，单位元
- ……



Ax = 0

- 对加法封闭：$x_1+x_2$
- 对数乘封闭：$c \cdot x_1$



### 子空间subspace

子空间V‘中所有的元素都属于空间V



子空间的运算

| 符号描述 |                                    |
| -------- | ---------------------------------- |
| V ∩ W    | 交集：向量组同时属于这两个向量空间 |
| V ⊕ W    | 当v∈V and w∈W：v + w ∈ V ⊕ W       |
| V ∪ W    |                                    |

假如$V_1, V_2$都是V的子空间，那么前两种运算后的空间也都是V的子空间(并集不一定)







### 4个向量空间

| 向量空间       |                          |
| -------------- | ------------------------ |
| column space   | 矩阵A的列生成的空间      |
| row space      | 矩阵A的行生成的空间      |
| nullspace      | $Av=0$                   |
| left nullspace | $A^Tv=0$，或者说$v^TA=0$ |



示例|矩阵A：

![image-20210213185206781](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213185206.png)

A的列向量生成的空间：

![image-20210213185221175](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213185221.png)

维度：注意后面两个列向量是前面两个列向量的线性组合(2个列向量冗余)，所以最终维度是2



基于上述关系：定义rank

1 列向量空间的维度/秩等于行向量空间的维度/秩 $rank(A) = rank(A^T)$

2 nullspace的维度

- ker(A)表示nullspace => kernel of A
- 具体计算过程：基于Av=0，代入(v1, v2, ... )，求解vi的自由度 [Link: Brilliant](https://brilliant.org/practice/four-fundamental-subspaces/?p=5)

![image-20210213201732115](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213201732.png)

![image-20210213202318650](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213202318.png)

#### rank-nullity theorem

nullspace的维度也被称为nullity：rank(A) + nullity(A) = 矩阵的==列数==

证明过程：

- 首先使用高斯消除将矩阵转换为其高斯-约旦形式(只涉及行初等变换，不改变rank) => Gauss-Jordan matrix：最简行形式



## 汇总|矩阵$A_{m \times n}$

| Subspace                                                     | Subspace of    | Symbol            | Dimension                                    |
| ------------------------------------------------------------ | -------------- | ----------------- | -------------------------------------------- |
| [column space](https://brilliant.org/wiki/row-and-column-spaces/) | $\mathbb{R}^m$ | $\text{im}(A)$    | r = [rank](https://brilliant.org/wiki/rank/) |
| [nullspace (kernel)](https://brilliant.org/wiki/kernel/)     | $\mathbb{R}^n$ | $\text{ker}(A)$   | n - r                                        |
| [row space](https://brilliant.org/wiki/row-and-column-spaces/) | $\mathbb{R}^n$ | $\text{im}(A^T)$  | r                                            |
| [left nullspace (kernel)](https://brilliant.org/wiki/kernel/) | $\mathbb{R}^m$ | $\text{ker}(A^T)$ | m - r                                        |

![image-20210213203225087](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213203225.png)





# 矩阵

## 矩阵的初等变换|高斯消元法

行初等变换：每一行是一个方程



### #pivot

优先行/列，比如：

![image-20210213190243489](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213190243.png)

## 矩阵的特性

结合律

不满足交换律

矩阵转置

矩阵的迹(只适用于方阵) => @最优化的指标



## 矩阵的运算

### 向量的运算@MSV

点积dot product：针对实数向量(不包括矩阵)

- 意义：度量向量的“接近”程度 => 刻画相似性@信号三部曲，@机器学习-LLS最小二乘回归(尽可能地接近/误差小)

范数

柯西施瓦茨不等式

![image-20210213195028966](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213195029.png)

内积inner product：点积的广义拓展 <v, w>

- 内积的运算性质@SVNT，MSV

- 拓广到非数值的向量空间 => 连续函数

  ![image-20210213195516872](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213195516.png)

- 拓广到随机变量：$<X, Y> = E[XY]$



正交向量orthogonal => 构造基底

- 对于实数向量：正交就等价于垂直

- 正交化方法：Gram-Schmidt正交化



## 矩阵的高级运算

求导：向量，矩阵，转置等相互关系



## 内积的应用|最小二乘法

$min \ ||Ax-b||$@上述矩阵的高级运算





# 基本概念⭐

| 基本概念                    |                                  |
| --------------------------- | -------------------------------- |
| vector space                |                                  |
| span                        | minimal spanning sets            |
| degrees of freedom          | 基于线性方程：讨论自由变量的个数 |
| Basis/minimal spanning sets | standard basis                   |
| dimension: size of basis    |                                  |

## span：

向量空间的==线性组合== => “张成”的空间



## minimal spanning sets

最小生成集 => 线性独立，没有冗余redundant

- 线性独立：线性方程只有在系数全为0的时候才成立





## degrees of freedom

- `x+y+z=6`的自由度为2

- 下式(A|b)秩为2，独立度为1

![image-20210213191329315](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213191329.png)

在系数矩阵中添加一个方程，那么自由度一般就会减少1

- 如果该方程是冗余的(线性相关)，那么自由度不变



## Basis

向量空间V的任一最小生成集minimal spanning set

- standard basis：基底范数为1



## dimension

意义：可以用来分析子空间、间接定义的向量空间



向量空间基底basis的维度

- 理论依据1：每一个向量空间都有基底Basis
- 理论依据2：向量空间所有有限的基底维度相同

拓展：向量空间-3*3矩阵的维度是9

![image-20210213192037804](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213192038.png)

![image-20210213192044913](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213192045.png)

- 3*3对角阵的维度是6

- 多项式$x^3+x$也是向量空间：
  - 基底是$\{x^3, x^2, x, 1\}$
  - 维度是4(基底的size是4)



### 示例|[Link: Brilliant](https://brilliant.org/practice/basis-and-dimension/?p=8)

矩阵A：

- A diagonal：$A^T=A$
- A skew symmetric：$A^T=-A$ => 对角线必定为0

V是对称阵向量空间，W是反对称阵向量空间：

- dim(V) = 6
- dim(W) = 3
- dim(V + W) =9
- dim(V ∩ W)=0



分析过程：

1 dim(V + W) =9

![image-20210213194017083](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213194017.png)

![image-20210213194023198](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213194023.png)

$(A^T)^T=((A_+ + A_-)^T)^T=...=A_+^T-A_-^T=A_+ + A_-=A$

- 说明任意任意一个普通的矩阵A都可以写成v+w的形式(v∈V and w∈W)

- 两者等价：维度相同，都是9



2 dim(V ∩ W)=0

从定义出发：

![image-20210213194634315](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213194634.png)

零向量的维度是0



注意到：

![image-20210213194659307](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213194659.png)





# 图论-邻接矩阵

邻接矩阵adjacency matrix：表明从某一个点转变到另一个点



1 从节点i到节点j所有路径的长度：$\sum_k A_{ik}\cdot A_{kj}$

2 n步：$A^n$

2.1 三角形：从自身出发，3步后回到自身 => 即对角线元素 => 步数和为$tr(A^3)$

- 三角形边数是奇数：适用 => 四边形可以在相邻的节点来回走两遍
- 三个顶点各自顺时针、逆时针 => 一个三角形被重复算了6次

![image-20210213205148333](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213205148.png)

2.2 complete graph(每一对顶点都有边相连接) => 5个顶点

![image-20210213205443830](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213205444.png)

问：路径长度为9回到自身的路径有多少？

观察模式：上面两个矩阵都是奇数步，对角线元素比其他元素少1 => 归纳规律：5个顶点等价

对单个顶点而言，9步之后又路径$4^9$条 => 由于每个顶点都是等价/完全对称的 => 所以整除5

- 奇数步：对角线元素比其他元素少1
- 偶数步：对角线元素比其他元素多1 => 计算$A^2$可得





@补充：

两个人之间最短链 连接次数是3.5





# 线性变换|矩阵描述

特征：线性变换可以用矩阵来描述



加法 + 数乘

下图从左到右：

- 尺度变换
- 旋转
- 水平方向的分量拉伸

![image-20210213214644112](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213214644.png)



- 微分：符合线性运算的性质



## 变换的象与原象

线性变换T(V)=W|P⊂V，S⊂W：

- P是S的原象preimage
- ==线性变换/矩阵的rank就是象S的维度==：rank(T)=dim im(T)
  - 比如T(x)=2x，线性变换的秩为1
- kernel/零空间T(x)=0：求解kernel就等于求解齐次的线性方程组(可以用高斯消去法)
  - 比如T(x)=2x，线性变换的kernel为0
  - nullity(T)=dim ker(T)



示例|T(x,y)=(x+y,2(x+y))

- 象(右边的结果)可以写成：c(1, 2)，所以维度为1
  - 正式来说：(1, 2)是象的基底basis
  - 因此：线性变换的rank=1
- kernel：也就是T(x, y)=0 => x+y=0
  - 对应结果是(x, -x)，基底是(1, -1)
  - 因此：nullity =1 



理解：为什么成为kernel？

- 因为T(x)=0，代表了线性变换的本质/结构属性



### injective transformation 单射

本质：不存在多射一



也就是T(x)=b，有唯一的结果b

- 此时，对于kernel T(x)=0
  - 只有T(x)={0}，变换T(x)才是单射的
  - 反证法：如果T(x)在多种情况下都为0，那么对应的，在这些情况下，T(x)=0+b均为0，违背了单射的原则
  - [Link: 具体证明](https://brilliant.org/practice/properties-of-linear-transformations/?p=5) 在T(x)={0}的前提下进行推导



从Ax=b这个角度来看：单射说明Ax=0只有零解(也就是变换的kernel T(x)=0)



### surjectivity 满射

象包含整个空间 => 线性变换/矩阵满秩



### bijectivity 一一映射 <=> isomorphism同构

重要结论：

> If \{v\} is a ==linearly independent set== of vectors in a vector space V
>
> and T is an injective transformation单射 from V to W
>
> then T(\{v\}) = \{w\} is a ==linearly independent set== of vectors in the space W.



结论的反面：

> If \{w\} is a ==linearly independent set== of vectors in a vector space W
>
> and T is an injective transformation单射 from V to W
>
> then T(\{v\}) = \{w\} is a ==linearly independent set== of vectors in the space V.

证明思路：

![image-20210213222206909](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210213222207.png)



## #小结

1 线性变换 T: V(domain) -> W(image) | T(v)=w

- T、W可以在同一个域：T(x)=2x
- 也可以不在同一个域：比如多项式 => 矩阵



2 秩与维度

- rank(M) = dim(image)
- nullity(M) = dim(kernel)



3 映射关系

- injective：不同的V转换为不同的W

- surjective满射：每一个象都有原象
- bijective：一一映射

 injective transformations保持线性独立性



0 重点：线性变换可以看做与矩阵相乘





# 特征值与矩阵对角化

特征向量：在变换过程中不改变方向的向量

- 图像压缩
- 协同预测：用户相似性
- PageRank 



验证是否为特征向量|正向验证：代入v => 找到标量$\lambda$使得向量v只发生尺度变换(下式中从(1, 1)到(3, 3))

![image-20210214084614297](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214084614.png)

- 注意：如果是复数域，$\lambda$可以是复数



结论：

> $n \times n$矩阵有n个复数的特征向量
>
> - 如果矩阵有两个特征值都是1，那么就计算为2个特征值

这与代数基本定理很类似：n阶多项式有n个复数的根





意义|对角化

- 求特征值很方便
- 求矩阵的幂更加方便
  - $D=P^{-1}AP$ => $A=PDP^{-1}$
  - ![image-20210214092312531](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214092312.png)
  - 最终结果是$A^k=PD^kP^{-1}$





性质：[Link: 多项式的根之和、积公式](https://brilliant.org/wiki/vietas-formula/#vietas-formula-higher-degrees)

![image-20210214090007870](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214090008.png)

- 特征值的和：有一项的系数是特征值的和($x^{n-1}$)@待定系数法 => $\sum r_i=-\frac{a_{n-1}}{a_n}$
- 特征值的积：令x为0 => $\Pi r_i=(-1)^n \frac{a_0}{a_n}$

应用|基于特征多项式：求特征值的和或积

![image-20210214090633979](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214090634.png)

![image-20210214090752759](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214090753.png)

注意观察：$x^{n-1}$项只能由对角线元素组成 => 而且恰巧是对角线元素的和 => 也就是迹的意义

- $tr(A)=\sum \lambda_i$



## 对角化

除非另有说明，否则本测验中的所有矩阵均为实数



对角化定义|存在P，是的$P^{-1}AP$是对角阵

性质：对角化 <=> 相似

- 相似矩阵具有相同的特征多项式
- 相似矩阵具有相同的特征值
- 相似矩阵具有相同的行列式

![image-20210214091255881](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214091256.png)

![image-20210214091310296](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214091310.png)



### 如何对角化？

思想：$D=P^{-1}AP$ => $PD=AP$

- D是对角阵 => 把P拆成列向量(c1, c2, ..., cn)来看待

- 即$(\lambda_1 c_1, \lambda_2 c_2, ..., \lambda_n c_n) = (Ac_1, Ac_2,..., Ac_n)$ 

- => ==惊奇发现==：$c_i$就是矩阵A的特征向量



由此：可对角化的条件 => A有n个线性独立的特征向量



1 特征值不同，那么一定能对角化

2 即使特征值相同，也可能能够对角化





## 应用

斐波那契数列

![image-20210214094027192](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214094027.png)

![image-20210214094038255](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214094038.png)



# 实例|PageRank

背景：搜索网页，加载数十亿的结果，并针对查询进行检查，找到相关项

- 细节：如何排名？ => 如何保证页面P的重要性？
  - 许多链接指向P：页面P可能对该主题具有权威性
  - 链接到它的站点已经很重要，比如wikipedia



建立评分过程：

初始状态都均等

![image-20210214094658359](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214094658.png)

迭代得到稳态：

- 根据链接数量，平均分配初始值的“给予”
  - 比如A给B、C每一个0.1
  - D给E是0.2
  - E给A也是0.2

- 一次更新过后A=0.3,B=0.1,C=0.2,D=0.1,E=0.3.

- 多次更新...(计算量巨大)





## 分析|状态转移矩阵

![image-20210214095558248](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214095558.png)

令M表示转移矩阵：$M_{ij}$表示从节点j到i，所以第一行是从其他节点到节点A

- $M^n=PD^{n}P^{-1}$
  - $D^n=\{\lambda_i^n\}$
- $M^nv$|v可以看做初始状态
  - ![image-20210214100916829](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214100917.png)



## 随机矩阵

定义：方阵，其元素为==非负实数==，且行和或列和为1

- 如果行和为1，则称为行随机矩阵
- 如果列和为1，则称为列随机矩阵
- 如果行和和列和都为1，则称为双随机矩阵

[谷歌矩阵](http://bitjoy.net/2016/08/04/googles-pagerank-and-beyond/)和[HMM中的转移矩阵](http://bitjoy.net/2016/08/20/introduction-to-hmm-1/)都属于随机矩阵，所以随机矩阵也称为概率矩阵、转移矩阵、或马尔可夫矩阵。



### 最大特征值为1|稳态

随机矩阵有一个性质，就是其所有特征值的绝对值==小于等于1，且其最大特征值为1==。



首先，随机矩阵A肯定有特征值1：当v是向量$(\frac{1}{n}, \frac{1}{n},\frac{1}{n}, ...)^T$时，由于$Av=1v$，所以至少有一个特征值是1@VES

其次：

- 证明方法|反证法
  - ![image-20210214143426626](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214143426.png)
- 证明方法|常规证法
  - $Ax=\lambda x$  => $x^TAx=\lambda x^Tx$
  - ![image-20210214143527823](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214143528.png)



由于最大特征值为1，而$M^nv_i=\lambda^n v_i, v=\alpha_1 v_1+ \alpha_2 v_2+...$

- 除了特征值为1的情况外，其余情况由于$\lambda<1$，n很大的时候变为0
- 所以$M^nv=M^n(\alpha_1 v_1+ \alpha_2 v_2+...) \approx  \alpha_i v_i$



最大特征值对应的特征向量==也就是稳态==

- 上述PageRank图形的稳态是(0.3077, 0.1538, 0.2308, 0.1154, 0.1923)
- 即A > C > E > B > D






	

### 第二大特征值$\lambda(A)$

矩阵A的谱间隔（spectral gap）$1- \lambda(A)$表示：最大特征值和第二大特征值之间的差值

$\lambda(A)$在马尔可夫随机游走领域有重要作用

![image-20210214143732111](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210214143732.png)









# #参考文献

[Link: 随机矩阵及其特征值](https://bitjoy.net/2016/08/23/the-eigenvalue-of-stochastic-matrix/)

[Link: wiki|凸几何-凸组合](https://zh.wikipedia.org/wiki/%E5%87%B8%E7%BB%84%E5%90%88)

- 点的线性组合，要求==所有系数都非负且和为 1==。此处的“点”可以是仿射空间中的任何点，包括向量和标量。
- 任意两个点的凸组合都在它们之间的线段上
- 三个点以上的凸组合：在几何体内部 => 这个几何体被包含在最长边围成的球体当中
- 点集的凸包等价于该点集的所有凸组合

