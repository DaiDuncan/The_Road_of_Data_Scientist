# 张量分解导论 [Link](https://zhuanlan.zhihu.com/p/24798389)

先简单地了解一下张量是什么，然后再考虑张量分解有什么用途，并如何像稀疏矩阵分解（matrix decomposition/factorization）一样来对稀疏张量进行分解

实际上，标量是第0阶张量，向量是第1阶张量，矩阵是第2阶张量，第3阶或阶数更高的张量被称为高阶张量（higher-order tensor），一般提到的张量都是特指高阶张量

- 张量可以被视为多维数组



张量分解从本质上来说是矩阵分解的高阶泛化。

对矩阵分解有所了解的读者可能知道，矩阵分解有三个很明显的用途

- 降维处理
- 缺失数据填补（或者说成“稀疏数据填补”）
- 隐性关系挖掘

其实张量分解也能够很好地满足这些用途。



## 推荐系统中常用的矩阵分解

在我们常见的推荐系统（如商品推荐系统、电影推荐系统等）中，给定一个大小为$m \times n$的==评分矩阵R==

1 当矩阵R的秩为$k=rank(R) << min(m, n)$，并且能够写成$R=UV^T$时：

- U是大小为$m \times k$的矩阵(用户因子矩阵，user-factor matrix)
- V是大小为$n \times k$的矩阵(项因子矩阵，item-factor matrix)



2 当$k<rank(R)$时，将矩阵分解的过程看作是一个低秩逼近问题（low-rank approximation problem），原来的分解过程则变成$R \approx UV^T$

在这个低秩逼近问题中，可以很明显得看出整体误差为残差矩阵中所有元素的平方和，即$||R-UV^T||^2$

如果简单地使总的误差最小，那么，可以将矩阵分解的逼近问题转化为一个无约束的优化问题

- $min \ J = \frac{1}{2}||R-UV^T||^2$

- 梯度下降/随机梯度下降



在实际应用中，这里的==评分矩阵R往往是一个稀疏矩阵==，即很多位置上的元素是空缺的，或者说根本不存在。

试想一下，如果有10000个用户，同时存在10000部电影，如果我们需要构造一个评分矩阵，难道每个用户都要把每部电影都看一遍才知道用户的偏好吗？

其实不是，我们只需要知道每个用户仅有的一些评分就可以利用矩阵分解来估计用户的偏好，并最终推荐用户可能喜欢的电影。

- 需要注意的是，推荐系统中的矩阵分解对原矩阵R有一定的要求，即矩阵的每行和每列至少有一个元素



### 评分估计值⭐

任意位置(i, j)所对应的评分估计值为：

- 评分矩阵分解：$R=UV^T$
  - U表示user-Factor
  - V表示item-Factor
  - => 最终的R表示user-item(比如用户-电影)

$\hat{r}_{ij}=(UV^T)_{ij}=\sum_{q=1}^k u_{iq}\cdot v_{jq}$

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224162923.png" alt="image-20210224162923364" style="zoom:80%;" />







## 张量|隐性因子模型

上面的矩阵分解在推荐系统中常常被称为==隐性因子模型（latent factor model, LFM）==，其实张量分解与上述过程非常相似



Charu C. Aggarwal在其著作《Recommender systems》中给出了一个特殊的张量分解结构，即大小为$m \times n \times d$的三阶评分张量R分解后会得到三个矩阵，这三个矩阵分别是：

- 大小为$m \times k$的用户因子矩阵U(user-factor matrix)
- 大小为$n \times k$的项因子矩阵V(item-factor matrix)
- 大小为$d \times k$的==环境因子==矩阵W(context-factor matrix)

这种分解结构是隐性因子模型的一种高阶泛化

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222203410.png" alt="image-20210222203410801" style="zoom:80%;" />

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224163041.png" alt="image-20210224163041356" style="zoom:80%;" />![image-20210224163116077](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224163116.png)





部分读者会疑问，这里介绍的张量分解过程为何与我们常在==大量文献中==见到的Tucker张量分解和CP张量分解（可认为是Tucker张量分解的特例）不一样呢？接下来我们来看看采用Tucker分解的情况是怎样的。



## **稀疏张量的Tucker分解**

一般来说，张量分解有各种各样的分解结构，而且对于每种结构其求解的算法也不唯一

常见的做法是采用ALS（alternating least squares）框架

并且在实际应用中，我们需要用到的张量分解通常是用来解决稀疏张量的填补问题



由于Tucker分解为张量分解提供了一个优美的分解结构，下面我们来介绍如何用梯度下降方法实现稀疏张量的Tucker分解

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222203757.png" alt="image-20210222203757433" style="zoom:80%;" />



![image-20210224163116077](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210224163116.png)

## 补充|Tucker分解与CP分解的比较

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222203845.png" alt="image-20210222203845761"  />

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222204007.png" alt="image-20210222204007180"  />



# 张量分解的数学基础 [Link](https://zhuanlan.zhihu.com/p/24824550)

## Kronecker积

Kronecker积在张量计算中非常常见，是衔接矩阵计算和张量计算的桥梁

- 矩阵A：$m_1 \times m_2$
- 矩阵B：$n_1 \times n_2$
- $A \otimes B$：$(m_1 n_1) \times (m_2 n_2)$

![image-20210222200133838](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222200134.png)



例子：

![image-20210222200321980](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222200322.png)





## Khatri-Rao积

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222200403.png" alt="image-20210222200403667" style="zoom:80%;" />



注意：该符号有时候表示矩阵的元素相乘



## 向量外积 **vector outer product**

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222200630.png" alt="image-20210222200630432" style="zoom:80%;" />



在大量的文献中，Kronecker积的符号$\otimes$，有时也用来表示向量的外积。





## 内积**inner product**

众所周知，向量的内积是一个标量

- $<\vec{a}, \vec{b}> = \vec{a}^T \vec{b}$

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222200826.png" alt="image-20210222200826507" style="zoom:80%;" />





## F-范数Frobenius norm

很多涉及到矩阵分解或张量分解的优化问题中常常会出现：**残差矩阵**的平方和最小化或是残差张量的平方和最小化

- 目标函数也多以相应的残差矩阵或残差张量的F-范数的平方形式进行书写

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222201007.png" alt="image-20210222201007595" style="zoom:80%;" />





## 张量的展开unfolding

在实际应用中，由于高阶张量比向量、矩阵都抽象，最简单地，向量和矩阵可以很轻松地书写出来并进行运算，而高阶张量则不那么直观，如何将高阶张量转换成二维空间的矩阵呢？

这就是张量的展开，有时，也将张量的展开称为张量的矩阵化（Matricization: transforming a tensor into a matrix）

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222201304.png" alt="image-20210222201304188" style="zoom:80%;" />

可惜的是，张量的展开虽然有一定的规则，但并没有很强的物理意义，对高阶张量进行展开会方便使用相应的矩阵化运算。

除此之外，高阶张量可以展开自然也就可以还原（即将展开后的矩阵还原成高阶张量，这个过程称为folding）。



## 张量与矩阵相乘 modal product, 模态积

张量与矩阵相乘（又称为模态积）相比矩阵与矩阵之间的相乘更为抽象，如何理解呢？



定义：

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222201544.png" alt="image-20210222201543878" style="zoom:80%;" />





示例：

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222201805.png" alt="image-20210222201805506" style="zoom:80%;" />

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222201853.png" alt="image-20210222201853627" style="zoom:80%;" />





## 张量积 => "膨胀"开来

### 张量积的意义？

关于几何意义：

- 低阶张量：具有几何意义@数组，矩阵

- 高阶张量：有了计算机处理张量，几何意义也就无关紧要了



关于物理意义：

- 用数学量来表示物理量
  - 可是标量加上向量，都不足以表达所有的物理量，所以就需要扩大数学量的概念，张量就出现了

- 量子力学：
  - 单个粒子的状态是由向量空间（实际上是希尔伯特空间）中的一个单位向量给出，该单位向量编码了该粒子的相关信息
  - 粒子系统的状态可以用多重张量积上的密度矩阵来描述

- 张量积编码了基本事物彼此间相互作用的所有方式

数学上，一个张量是一个多重线性函数。这里f乘以g是一个二重线性函数，因此是一个张量。



![image-20210222213946180](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222213946.png)

#### 向量的理解

张量：用基向量与分量的组合来表示物理量(由于基向量可以有丰富的组合，所以可以表示非常丰富的物理量)





向量可以表示什么？

- 比如，我们可以用==一个平面的法向量==代表这个平面；

- 物理上可以用向量代表力等。

看来，向量可以表示很多东西，不过仔细想想向量也只表示了幅度（magnitude）与方向（direction）两个要素而已。



一个向量有很多种表示方式，我们可以用[0, 1]表示一个二维向量，也可以用平面、三维或更高维空间中的一条带箭头的线表示一个向量

我们都是知道（0， 0） —> （1， 1）可表示一个从（0， 0）到（1， 1）的有向线段（向量），那么，为什么可以用[0, 1]表示一个向量呢？



一个向量就是空间中的一条有向线段，可以用一组坐标系的基和向量相应分量的==乘积组合==来表示。

由于坐标系有很多种定义方式，基也就有很多种，对应的分量也会有很多种

但如果大家默认使用==同一套基向量，那么基向量都不需要了==，此时，想要表示一个向量，只要给定这三个分量即可

- 比如用0, 1表示一个向量，如果加上两个括号，这就是我们在书上经常看到的向量的列表示（0， 1）
- 三维的有（1， 2， 1）







#### 定义 [Link](https://www.jianshu.com/p/2a0f7f7735ad)

张量的严格定义是利用线性映射来描述的。

与矢量相类似，定义由若干坐标系改变时满足==一定坐标转化关系==的有序数组成的集合为张量。   

从几何角度讲， 它是一个真正的几何量，也就是说，它是一个==不随参照系的坐标变换==（其实就是基向量变化）而变化的东西。

最后结果就是基向量与对应基向量上的分量的组合（也就是张量）保持不变，比如一阶张量（向量）**a**可表示为**a** = x***i** + y***j**。由于基向量可以有丰富的组合，张量可以表示非常丰富的物理量。



一个(p,q)型张量，就是一个映射：其中V是矢量空间，V*是对应的对偶空间

![image-20210222214205376](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222214205.png)

- 一个物理量，在物体的某个位置上只是一个单值，那么就是普通的标量，比如密度
- 在同一个位置、从不同的方向上看，有不同的值，而且这个数恰好可以用矩阵乘观察方向来算出来，就是张量

> 张量的理解：**张量是有大小和多个方向的量。**
> 这里的方向就是指==张量的阶数==。
> 空间维度n：一般我们使用3维空间，也可以是4维及以上维度。
> 张量阶数m：在固定的3维度空间再谈张量的阶数，==阶数小于等于维数==，即m<=n。
>
> 
>
> 下面区分：
>
> - **张量的阶数（张量的方向数）**
> - 所在空间的维数（所在空间的方向数）
>
>  在二维空间里，二维二阶张量（平面应力张量）的每个方向都可以用二维空间的两个方向表示。（区分2阶张量的2个方向，和二维空间的两个方向x，y）所以共有2^2=4个方向。
>
>  在三维空间里，三维二阶张量（空间应力张量）的每个方向都可以用三维空间三个方向表示。（区分2阶张量的2个方向，和三维空间的三个方向x，y、z）所以共有3^2=9个方向。



#### 张量积

| 不同的乘积       |                                   |
| ---------------- | --------------------------------- |
| 矩阵乘积         | $(AB)_{ik} = A_{ij}B_{jk}$        |
| 向量的内积       | $\vec{a} \cdot \vec{b} = a_j b_j$ |
| 矩阵和向量的乘法 | $(A\vec{b})_i = A_{ij} b_j$       |

共同点：有一个相同指标在经过求和之后就看不见了。

如果你只是把两个量放在一起，不求和，只是构造多重线性的话，你就发现了张量积。

比如向量  $(\vec{a} \otimes \vec{b})_{ij} = a_j b_j$

比如矩阵(二阶张量)  $(AB)_{ijkl} = A_{ij}B_{jk}$ => 构造了一个四阶张量



如果拿来跟矩阵乘积比较的话，我觉得比较好的说法是，张量积是一种万有乘积，而矩阵乘法是一种具体化。

> 现在手里有很多矩阵，然后希望把两个矩阵乘起来。一开始肯定想不到怎么乘，但是可以猜一些乘积的最基本的性质，比如说要和数乘是匹配的，也要和加法匹配也就是分配律。不管这个乘积是什么，都应当有这些基本的性质。那么这个时候张量积就出现了，他代表了最广的乘积，也是最弱的乘积，就仅仅满足上面说的那些基本性质。正因为是最弱的，所以一切具体的乘积都可以看成是从张量积的结果具体化得到的，也就是可以看成是万有乘积，或者是一个包络的乘积。



在[数学](https://zh.wikipedia.org/wiki/数学)中，**张量积**，记为$\otimes$

可以应用于不同的上下文中：如[向量](https://zh.wikipedia.org/wiki/向量)、[矩阵](https://zh.wikipedia.org/wiki/矩阵)、[张量](https://zh.wikipedia.org/wiki/张量)、[向量空间](https://zh.wikipedia.org/wiki/向量空间)、[代数](https://zh.wikipedia.org/wiki/代数)、[拓扑向量空间](https://zh.wikipedia.org/wiki/拓扑向量空间)和[模](https://zh.wikipedia.org/wiki/模)。

在各种情况下这个符号的意义是同样的：最一般的[双线性运算](https://zh.wikipedia.org/wiki/双线性算子)。在某些上下文中也叫做[外积](https://zh.wikipedia.org/wiki/外积)。

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222182442.png" alt="image-20210222182442618" style="zoom:80%;" />



有两个(或更多)**张量积**的分量的一般公式。

例如，如果 *U* 和 *V* 是秩分别为 *n* 和 *m* 的两个[协变](https://zh.wikipedia.org/wiki/协变)张量，则它们的==张量积的分量==给出为：

![image-20210222220013791](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222220013.png)



两个张量的张量积的分量(等式左边)，是每个张量的分量的普通积

---

### [Link: 区分外积 Kronecker积 张量积](https://zhuanlan.zhihu.com/p/26774182)

- 满足数乘结合律
- 满足分配律

![image-20210222212742105](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222212742.png)



近年来，张量分解技术在数据挖掘领域得到了很好的应用

当然，就线性代数和多重线性代数而言，主流的观点将涉及到张量计算的内容归为“多重线性代数”



定义：

给定两个有限维的向量空间（finite dimensional vector space）V和W

- $\vec{v_1}, ..., \vec{v_m}$是向量空间V的基底basis
- $\vec{w_1}, ..., \vec{w_n}$是向量空间W的基底basis

则张量积是两个向量空间基底==“某种运算”==的线性组合

![image-20210222212141774](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222212142.png)

同时，双线性映射（bilinear map）被定义为：$B: V \times W \rightarrow V \otimes W$

![image-20210222212251201](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222212251.png)

- 说明：$V \times W$也被称为两个向量空间的Cartesian积





例子|计算：

- 理解：二维空间和三维空间的张量积组成了六维空间，在六个基向量的值就是张量积结果的元素值

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222212529.png" alt="image-20210222212529199" style="zoom:80%;" />

```python
import numpy as np
x = np.array([[1], [1]])
y = np.array([[1], [-2], [1]])
np.kron(x, y.T)	#张量积
np.outer(x, y)	#外积
```

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222212642.png" alt="image-20210222212642450" style="zoom:80%;" />





```python
import numpy as np
a = np.array([[1], [2]])
b = np.array([[3], [4]])
np.kron(a, b)
np.outer(a, b)
np.kron(a, b.T)
```

Kronecker积与外积并不相同

- 严格意义上的Kronecker积的计算结果和外积的不同，但这种不同仅仅体现在每个元素的摆放位置不同 => 依然可以认为外积是Kronecker积的特例

对于向量而言，张量积和外积是等价的





# 应用|奇异值分解

矩阵的奇异值分解（singular value decomposition，简称SVD）是线性代数中很重要的内容，通常，给定一个大小为$m \times n$的矩阵A，奇异值分解的形式为 $A = U \Sigma V^T$

- 三个矩阵的维度分别是$m \times m, m \times n, n \times n$

- U：由左奇异向量（left singular vector）构成
- V：由右奇异向量（right singular vector）构成
- 矩阵$\Sigma$对角线上的元素称为奇异值（singular value）





## 高阶奇异值分解

higher-order singular value decomposition, 简称HOSVD



就高阶奇异值分解而言，著名学者Tucker于1966年给出了计算Tucker分解的三种方法，第一种方法就是我们这里要提到的高阶奇异值分解，其整个分解过程也是由矩阵的奇异值分解泛化得到的。

![image-20210222210802883](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222210803.png)

![image-20210222210950001](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210222210950.png)







# #参考文献

[Link: 直观理解张量|原文](http://mrw.so/4orWbY)

- [Link: 直观理解张量|翻译](https://www.sohu.com/a/359296954_652527)

[Link: 知乎专栏|张量分解1 如何简单地进行张量分解？](https://zhuanlan.zhihu.com/p/24798389)

[Link: 知乎专栏|张量分解2 张量分解的数学基础](https://zhuanlan.zhihu.com/p/24824550)

- 主要包括常见的Kronecker积、Khatri-Rao积、向量的外积、内积、F-范数、模态积的运算规则以及高阶奇异值分解



Gene H. Golub和Charles F. Van Loan合著的经典著作《Matrix computations (4th edition)》

Tamara G. Kolda和Brett W. Bader于2009年发表的一篇经典综述论文《Tensor decompositions and Applications》 [Link: pdf](https://public.ca.sandia.gov/~tgkolda/pubs/pubfiles/TensorReview.pdf)