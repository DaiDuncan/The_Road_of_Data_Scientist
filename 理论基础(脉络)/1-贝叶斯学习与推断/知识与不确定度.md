# 导论

信息时代：获取价值信息



## 查找罪犯

- 一个一个询问
- 二分法 => 极大地减少不确定度



最大的不确定性 => 信息最丰富

> The **most informative** questions are the ones whose answer we are **most uncertain** about.

加权平均数：分成两组，A组是9个，B组是8个

- 那么A组对象的每一个概率是9/16，B组是7/16
- 加权平均后：

![image-20210208213537329](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210208213537.png)

计算期望EV⭐

## 示例|称重

假货币有可能轻，也有可能重(4枚硬币中，最多只有一枚假货币)

常见分类：天平两边各2枚

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210208222052.png" alt="image-20210208222052109" style="zoom:67%;" />



最佳分类：最大信息量

- 用与真货币等重的东西代替货币4

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210208222437.png" alt="image-20210208222437148" style="zoom:67%;" />





## 示例|语言

表达的信息公式：`(Information per second)=(Syllables per second)×(Information per syllable)`

- 英语有6949种发音

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210208223534.png" alt="image-20210208223534485" style="zoom:67%;" />



前提假设：`"Information per second" is actually roughly the same in all languages.`

根据上述的信息公式：

- syllable rate：Syllables per second



如果每种语言的每秒信息量相等，我们希望人们能在大约同一时间结束讲话

信息传递的速率：大概是39$\pm$5 bits/s



# 一 信息论

信息：解决我们的不确定性归结为找到这三个问题(二分类型)的答案，因此使用所需的问题数量作为我们不确定性的度量似乎很自然

- 提问：maximally informative questions

条件概率



比特：$log_2(n)$ bit



可能性：$P=2^n$

信息：$log_2 P$ bits

要点：

- 通过计算解决知识不足(犯罪嫌疑人)所需的最大信息量(提问)，可以==量化不确定性==



一般均匀分布的前提假设很少能够被满足

- 完全不知道信息时，假设为均匀分布，熵最大

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210209221614.png" alt="image-20210209221614272" style="zoom:67%;" />



信息量最大：概率最平均 => ==类似均匀分布，熵最大==

- A maximally informative question is one where the answer is equally likely to be "yes" or "no,"

1 对应的问题时：Mimi是罪犯吗？ => 一半的概率

2 接下来的问题：(只剩下下面一半)

- Is the criminal blue and unstriped?
- Is the criminal a square and unstriped?



概率为0：不可能事件

概率为1：确定事件，信息量$log_2(\frac{1}{P})=0$ => P越小，bit数量越大

## 定义

1 We defined the amount of uncertainty as the number of maximally informative questions we need to resolve our uncertainty.

![image-20210209222714138](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210209222714.png)



2 Another way we can look at this is as the amount of information we are missing.

![image-20210209222723465](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210209222723.png)

单位是：bits





证据：增加信息量，但有时会使事情变得更繁琐 => 比如均匀分布，只需要3个问题，而非均匀分布虽然信息量增加，但对概率小的怀疑对象，需要检测的手续更多

![image-20210209223149850](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210209223150.png)



## 定义|平均的信息量

单个比特数：$-log_2(p_i)$

平均比特数(期望的计算公式)：$-\sum_1^{N}p_i \cdot log_2(p_i)$



## 熵

1940 香农

- 在Shannon之前，没人会想到，信息可以被完全衡量，就好像它是某种物理量一样

均匀分布熵最大



### 示例|提问题的平均数

不公平的硬币：3/4是头head，1/4是花tail

![image-20210210094148011](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210094148.png)

$1 \times 0.56 + 2 \times 0.19 +3 \times 0.19 + 3 \times 0.06\approx 1.69 $

- HH需要一个问题：概率是0.56
- HT需要两个问题
- TH和TT需要三个问题来区分

=> 平均需要1.69个问题：比均匀分布，最大熵的2个问题要少



但是两枚硬币的信息是1.62 bits(每一枚是0.81bit) => 多次重复：会更加接近

- 公平硬币每一枚的信息量是1bit

一枚硬币：至少需要一个问题(这就是为什么要多枚硬币：更加接近信息量的数据)



熵的计算公式：所需maximally informative questions的数量 => 但并不是总是能够问出这样的问题



## 压缩

如何在不丢失任何信息的情况下压缩图像

- 无损压缩：这意味着即使图像尺寸较小，图片本身也将与原始图片完全相同

bit => byte => KB



==压缩的思路==：“询问”maximally informative questions

以颜色为例：在颜色条带上的分布

![image-20210210095628456](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210095628.png)

使用8bit来表示：

- 问题1：左半边，还是右半边(1表示右半边)
- ……

结果是11100110

三种颜色就是24bits



### 示例|压缩图像

方法0

图像更小化：比如2 x 2像素，基于平均值，转化为一个像素



方法1：与相邻像素的差异

![image-20210210095949209](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210095949.png)

![image-20210210100149867](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210100150.png)

用101来编码表示相邻的差异：即从倒数第一到倒数第三个相同为0，不同为1

- 第一个像素点是1110 0110
- 而相邻的是101

=> 当附近的像素彼此相似时，这可以明显节省存储空间



但是当元素相差大时，并没有节省空间，比如(00000000,10101010,00000000)



小结：需要的前提是颜色是渐进变化的(即相邻元素相似性高)

以下为两种情境的对比

![image-20210210100628146](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210100628.png)

![image-20210210100635116](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210100635.png)







方法2：相同颜色聚类



## trit

到目前为止，我们已经看到，不确定性远不是模糊的感觉，而是可以像长度或重量一样测量的物理量。我们选择使用的单位是bit

- 一个问题2种可能的答案 => binary	$-log_2(1/2)$
- 一个问题3种可能的答案 => trinary   $-log_2(1/3) > 1bit$

![image-20210210101148105](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210101148.png)

每进行一次称重(提问)，筛选之后只剩下1/3的未知



分析思路：多少种基本情况

13枚硬币(其中一枚可能重，轻或者真实) => 13*2+1=27种基本情况 => 3次称重即可

@第一次称重

![image-20210210103852174](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210103852.png)

![image-20210210103845162](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210103845.png)

### trinary vs binary

总的信息量相同 $H = 2^B = 3^T$ => $B = T \times log_2(3) \approx 1.6T$

即一个trinary包含的信息量是binary的1.6倍 => 推广开来：假如量子计算机 量子比特位为16，那么一个量子比特的信息量是比特的4倍



## life tip

所有涉及某一项虚假的问题都可以用这种思路maximally informative questions来解决





# 二 贝叶斯推断

必须考虑支持每种可能性的证据。测试结果是一条可能会影响重量平衡的新信息

学习新证据如何影响我们已经知道的内容



## 案例|疾病检测

需要考虑baseline：疾病的稀有性

阳性检测并不能增加患有疾病的belief



所以：现实中，鼓励超过40岁以上的妇女检测乳房

- 乳腺癌的机率非常低，以至于阳性的检测结果很可能是错误的警报，可能会导致不必要的放射或手术。



患病中(10人)，检测结果，阳性：阴性 = 9：1

![image-20210210110906148](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210110906.png)

不患病中(990人)，检测结果：阳性：阴性 = 99 ：801

=> 所以：阳性检测中(9+99)，患病人数是9 => 最后概率为9/(9 + 99)





## 贝叶斯定理

- 给belief一个概率
  - P(obj) 

- 基于证据：更新概率的权重 => 更新概率时需要考虑哪些相关因素？
  - 推理过程：P(evidence|obj)
  - 目标：P(obj|evidence)



下图中：注意分母p(evidence)都是一样的(客观存在的概率)

![image-20210210112759512](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210112759.png)

关键思路：如何基于evidence更新概率/信念

注意：下图中，基于evidence更新概率后，两者相加就是p(evidence)，==与p(evidence)相除的结果为1==@朴素贝叶斯算法

![image-20210210113929454](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210113929.png)



@贝叶斯定理

- 基于evidence是后验
- 从对象h出发：是先验
- 基于h的evidence则是最大似然

![image-20210210114123839](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210114124.png)



## 悖论|monty hall

贝叶斯定理：基于新的证据evidence，调整我们的信念belief



背景|游戏：三个门 = 两头羊 + 一辆车

在选手做出选择后

1 主持人先打开一个门：是山羊

2 问对方要不要改变意见？

只考虑一种情况(因为是对称的问题)：选手选择A，主持人打开门C，问是否换到B门？





$h_a$表示车在A门后面

$h_b$表示车在B门后面

A和C对称：在C门后面等价于在A门后面

e表示打开门C

如果车在门B后面，那么$p(e|h_b)=1$

如果车在门A后面，那么$p(e|h_a)=1/2$



关键点：主持人需要利用自己对汽车在哪里的知识来决定打开哪扇门 => evidence



### 1 注意前提⭐

前提1 选手选择了A

前提2 主持人打开了C，而且是山羊

但是按照我的思路：before

| 门A                                          | 门B  | 门C  |
| -------------------------------------------- | ---- | ---- |
| $h_a$                                        |      |      |
| Y                                            | N1   | N2   |
| Y                                            | N2   | N1   |
|                                              |      |      |
| $h_b$                                        |      |      |
| N1                                           | Y    | N2   |
| N2                                           | Y    | N1   |
|                                              |      |      |
| $h_c$ 基于前提(主持人从C中打开了山羊 不考虑) |      |      |
| N1                                           | N2   | Y    |
| N2                                           | N1   | Y    |



### 2 切入角度|多次重复试验⭐

注意：概率的表现 => 稳定性：多次重复试验接近概率@德州扑克赔率：多少次能赢(而不是只关注于一局)

- 当然前提是：同等条件下，大量重复的试验/情境
- 而现实中：很难有上述机会 => 但多去实践(目标明确的实践)，可以提高一次击中的机会



假设选了四次：

- $h_a$情况下，两次选B，两次选C
- $h_b$情况下，次次都是选C(A已经被选手选定)

<img src="https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210125405.png" alt="image-20210210125405715" style="zoom:67%;" />

### 3 贝叶斯定理|证据更新概率

已知：$p(h_a) : p(h_b) = 1:1$

证据|基于朴素贝叶斯：$p(e|h_a)p(h_a):p(e|h_b)p(h_b)= 2:4$

- 证据更支持$h_b$：也就是主持人知道车在门B之后，比知道车在门A之后，更倾向于打开门C
- 而基于这个证据的结果：就是我们==增加了车在门B后的信念==

更新概率：$p(h_a|e):p(h_b|e) = 2:4$



## p值|有效性

定义：即使药物实际上不起作用，医生也将测量的概率。



实例|药物测试得到的p=0.04

- The chance of ==getting this data== if the drug doesn’t actually work is 4%

- me|理解：基于$H_0$假设，对$H_1$假设进行验证(药物不起作用)时得到测试样本数据的概率

注意：p值不是药物起作用的概率(与实验对象有效性无关)，而是针对实验样本的有效性

通常将0.05作为p的阈值：

- <0.05 拒绝$H_1$
- \>0.05 不拒绝$H_1$

| 概念             |                   |
| ---------------- | ----------------- |
| $p(results|H_1)$ | 显著性 => p-value |
| $p(results|H_0)$ | Power             |
|                  |                   |



### RCT|large randomized control trial

大型随机对照实验

- 一半是实验组
- 一半是空白对照组(安慰剂)

随机分配：受试者不知道服用的是药物，还是安慰剂



基于下述结果的分析

![image-20210210204707348](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210204707.png)

1 药物X没有作用，但是服用过后比一般人更健康

2 药物X有作用，但效果不是很强

3 药物X有大作用，但复用安慰剂的对照组比环境一般人更健康



在理想情况下，这些药物将由许多==独立==的科学家小组进行==重复测试==。



## 贝叶斯定理的缘由

定位：决定新证据evidence如何改变信念belief

cause：因果

effect：效应



### 示例|肿瘤L+良性肿瘤B

L：Lumps

B：Benign Bumps

+：表示检测结果为阳性

![image-20210210210607412](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210210607.png)



表格分析：

- 肿块和良性肿块都是测试结果呈阳性的原因



测试结果：

![image-20210210211129970](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210211130.png)

检测为阳性的结果：

![image-20210210211415270](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210211415.png)



贝叶斯定理的直观意义：从因果到结果 => 人类非常善于因果推理reasoning from cause to effect@关注结果



$p(effect|cause)$ + 贝叶斯定理 => 后验$p(cause|effect)$

- 此次实验中：cause表示实际病情(从内到外)，effect表示测试结果test(从外到内)

容易得到的数据：上述表格，比如$p(T=+|L=n, B=y)=1/2$

难得到的数据：比如$p(L=y|T=+)$





小结：为什么阳性检测有时候也无效？

虽然患病者被检测阳性的概率更高(更敏感)，但是良性肿块的数量居多，而这些良性肿块也会导致检测阳性。





## @启发 => 运用这些想法来应对生活中的不确定性

看到阳性检测(不好的证据)，不要直接认定就这样了。

这不是最终的结果，需要综合考虑其他的因素/条件@抑郁症：只身沉浸在“片面”的绝望空间





# 三 因果贝叶斯网络

![image-20210210210712606](https://cdn.jsdelivr.net/gh/DaiDuncan/PicUploader/img/20210210210712.png)

没有箭头：表示没有彼此的因果性

- 彼此是独立的



# 参考文献

KIT：

- 信息融合IF

